[
  {
    "timestamp": "2025-11-21T10:26:46.401776",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T10:26:46.402355",
    "type": "ä»£ç ç”Ÿæˆ",
    "content": "å¼€å§‹ç”Ÿæˆæ–¹æ¡ˆå®ç°: æµ‹è¯•æ–¹æ¡ˆ"
  },
  {
    "timestamp": "2025-11-21T10:26:46.412377",
    "type": "ä»£ç ç”Ÿæˆ",
    "content": "æ–¹æ¡ˆ æµ‹è¯•æ–¹æ¡ˆ ä»£ç ç”Ÿæˆå®Œæˆ"
  },
  {
    "timestamp": "2025-11-21T10:26:46.422845",
    "type": "å®¡æ ¸æäº¤",
    "content": "æäº¤ä»£ç å®¡æ ¸è¯·æ±‚: approval_20251121_102646 - æµ‹è¯•æ–¹æ¡ˆ"
  },
  {
    "timestamp": "2025-11-21T10:36:46.682583",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T10:36:46.683451",
    "type": "ä»£ç ç”Ÿæˆ",
    "content": "å¼€å§‹ç”Ÿæˆæ–¹æ¡ˆå®ç°: æ™ºèƒ½ä½“ååŒæµ‹è¯•æ–¹æ¡ˆ"
  },
  {
    "timestamp": "2025-11-21T10:36:46.694984",
    "type": "ä»£ç ç”Ÿæˆ",
    "content": "æ–¹æ¡ˆ æ™ºèƒ½ä½“ååŒæµ‹è¯•æ–¹æ¡ˆ ä»£ç ç”Ÿæˆå®Œæˆ"
  },
  {
    "timestamp": "2025-11-21T10:37:20.865800",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T10:37:20.878581",
    "type": "ä»£ç ç”Ÿæˆ",
    "content": "å¼€å§‹ç”Ÿæˆæ–¹æ¡ˆå®ç°: æ™ºèƒ½ä½“ååŒæµ‹è¯•æ–¹æ¡ˆ"
  },
  {
    "timestamp": "2025-11-21T10:37:20.889537",
    "type": "ä»£ç ç”Ÿæˆ",
    "content": "æ–¹æ¡ˆ æ™ºèƒ½ä½“ååŒæµ‹è¯•æ–¹æ¡ˆ ä»£ç ç”Ÿæˆå®Œæˆ"
  },
  {
    "timestamp": "2025-11-21T10:37:20.900274",
    "type": "å®¡æ ¸æäº¤",
    "content": "æäº¤ä»£ç å®¡æ ¸è¯·æ±‚: approval_20251121_103720 - æ™ºèƒ½ä½“ååŒæµ‹è¯•æ–¹æ¡ˆ"
  },
  {
    "timestamp": "2025-11-21T11:34:30.476053",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T11:34:47.827811",
    "type": "å¯¹è¯å“åº”",
    "content": "ç”¨æˆ·æ¶ˆæ¯ï¼šè¯·å®ç°è¿™ä¸ªRAGç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œå“åº”ï¼šä½œä¸ºä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“ï¼Œæˆ‘å°†åŸºäºRAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„å…¸å‹æ¶æ„ï¼Œä¸ºæ‚¨æä¾›æ ¸å¿ƒæ¨¡å—çš„ä»£ç å®ç°å»ºè®®ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„ä¸“ä¸šå®ç°æ–¹æ¡ˆï¼š\n\n## RAGç³»ç»Ÿæ ¸å¿ƒæ¨¡å—ä»£ç å®ç°\n\n### 1. æ–‡æ¡£å¤„ç†æ¨¡å—\n```python\nimport os\nimport pickle\nfrom typing import List, Dict\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader, TextLoader\n\nclass DocumentProcessor:\n    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap\n        )\n    \n    def load_documents(self, file_path: str) -> List[Dict]:\n        \"\"\"åŠ è½½æ–‡æ¡£å¹¶åˆ†å‰²æˆå—\"\"\"\n        if file_path.endswith('.pdf'):\n            loader = PyPDFLoader(file_path)\n        elif file_path.endswith('.txt'):\n            loader = TextLoader(file_path)\n        else:\n            raise ValueError(\"Unsupported file format\")\n        \n        documents = loader.load()\n        chunks = self.text_splitter.split_documents(documents)\n        \n        return [\n            {\n                \"content\": chunk.page_content,\n                \"metadata\": chunk.metadata,\n                \"chunk_id\": f\"{os.path.basename(file_path)}_{i}\"\n            }\n            for i, chunk in enumerate(chunks)\n        ]\n```\n\n### 2. å‘é‡å­˜å‚¨æ¨¡å—\n```python\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass VectorStore:\n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        self.model = SentenceTransformer(model_name)\n        self.embeddings = []\n        self.documents = []\n    \n    def add_documents(self, documents: List[Dict]):\n        \"\"\"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨\"\"\"\n        contents = [doc[\"content\"] for doc in documents]\n        embeddings = self.model.encode(contents)\n        \n        self.documents.extend(documents)\n        self.embeddings.extend(embeddings)\n    \n    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"è¯­ä¹‰æœç´¢ç›¸å…³æ–‡æ¡£\"\"\"\n        query_"
  },
  {
    "timestamp": "2025-11-21T11:35:11.247131",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T11:35:28.074127",
    "type": "å¯¹è¯å“åº”",
    "content": "ç”¨æˆ·æ¶ˆæ¯ï¼šè¯·å®ç°è¿™ä¸ªRAGç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œå“åº”ï¼šä½œä¸ºä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“ï¼Œæˆ‘å°†åŸºäºRAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„æŠ€æœ¯æ–¹æ¡ˆï¼Œä¸ºæ‚¨ç”Ÿæˆæ ¸å¿ƒæ¨¡å—çš„ä»£ç å®ç°å»ºè®®ã€‚\n\n## RAGç³»ç»Ÿæ ¸å¿ƒæ¨¡å—ä»£ç å®ç°\n\n### 1. æ–‡æ¡£å¤„ç†æ¨¡å—\n```python\nimport os\nimport pickle\nfrom typing import List, Dict\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader, TextLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nclass DocumentProcessor:\n    def __init__(self, chunk_size=1000, chunk_overlap=200):\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap\n        )\n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-mpnet-base-v2\"\n        )\n    \n    def load_documents(self, file_path: str) -> List[Dict]:\n        \"\"\"åŠ è½½æ–‡æ¡£å¹¶åˆ†å‰²\"\"\"\n        if file_path.endswith('.pdf'):\n            loader = PyPDFLoader(file_path)\n        elif file_path.endswith('.txt'):\n            loader = TextLoader(file_path)\n        else:\n            raise ValueError(\"Unsupported file format\")\n        \n        documents = loader.load()\n        chunks = self.text_splitter.split_documents(documents)\n        return chunks\n    \n    def create_vector_store(self, chunks: List[Dict], save_path: str):\n        \"\"\"åˆ›å»ºå‘é‡å­˜å‚¨\"\"\"\n        vector_store = FAISS.from_documents(chunks, self.embedding_model)\n        vector_store.save_local(save_path)\n        return vector_store\n```\n\n### 2. æ£€ç´¢æ¨¡å—\n```python\nfrom typing import List, Tuple\nimport numpy as np\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nclass Retriever:\n    def __init__(self, vector_store_path: str):\n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-mpnet-base-v2\"\n        )\n        self.vector_store = FAISS.load_local(\n            vector_store_path, \n            self.embedding_model, \n            allow_dangerous_deserialization=True\n        )\n       "
  },
  {
    "timestamp": "2025-11-21T11:36:01.578668",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T11:36:20.041133",
    "type": "å¯¹è¯å“åº”",
    "content": "ç”¨æˆ·æ¶ˆæ¯ï¼šè¯·å®ç°è¿™ä¸ªRAGç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œå“åº”ï¼šä½œä¸ºä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“ï¼Œæˆ‘å°†åŸºäºRAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„å…¸å‹æ¶æ„ï¼Œä¸ºæ‚¨ç”Ÿæˆæ ¸å¿ƒæ¨¡å—çš„ä»£ç å®ç°å»ºè®®ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„å®ç°æ–¹æ¡ˆï¼š\n\n## ğŸ“‹ æ–¹æ¡ˆåˆ†æ\n\n**æ ¸å¿ƒéœ€æ±‚**ï¼š\n- æ–‡æ¡£æ£€ç´¢æ¨¡å—\n- å‘é‡åŒ–å¤„ç†\n- ç”Ÿæˆå¼å›ç­”\n- ä¸Šä¸‹æ–‡ç®¡ç†\n\n**æŠ€æœ¯çº¦æŸ**ï¼š\n- ä½¿ç”¨ç°ä»£Pythonå¼‚æ­¥æ¶æ„\n- æ”¯æŒä¸»æµå‘é‡æ•°æ®åº“\n- æ¨¡å—åŒ–è®¾è®¡ä¾¿äºæ‰©å±•\n\n## ğŸ”§ æ ¸å¿ƒæ¨¡å—ä»£ç å®ç°\n\n### 1. å‘é‡å­˜å‚¨ç®¡ç†æ¨¡å—\n\n```python\n# vector_store.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\nclass VectorStore(ABC):\n    \"\"\"å‘é‡å­˜å‚¨æŠ½è±¡åŸºç±»\"\"\"\n    \n    @abstractmethod\n    async def add_documents(self, documents: List[Dict[str, Any]]) -> bool:\n        pass\n    \n    @abstractmethod\n    async def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        pass\n\nclass ChromaVectorStore(VectorStore):\n    \"\"\"ChromaDBå®ç°\"\"\"\n    \n    def __init__(self, collection_name: str, embedding_model: str = \"all-MiniLM-L6-v2\"):\n        self.embedding_model = SentenceTransformer(embedding_model)\n        self.collection_name = collection_name\n        self._init_chroma_client()\n    \n    def _init_chroma_client(self):\n        try:\n            import chromadb\n            self.client = chromadb.Client()\n            self.collection = self.client.get_or_create_collection(self.collection_name)\n        except ImportError:\n            raise ImportError(\"ChromaDB not installed. Run: pip install chromadb\")\n    \n    async def add_documents(self, documents: List[Dict[str, Any]]) -> bool:\n        \"\"\"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨\"\"\"\n        try:\n            texts = [doc[\"content\"] for doc in documents]\n            embeddings = self.embedding_model.encode(texts).tolist()\n            metadatas = [{\"source\": doc.get(\"source\", \"unknown\"), \n                         \"id\": doc.get(\"id\", str(i))} \n                        for i, doc in enumerate(documents)]\n            \n            self.collection.add(\n                embeddings=embeddings,\n                documents=texts,\n"
  },
  {
    "timestamp": "2025-11-21T11:36:54.122744",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T11:37:16.469795",
    "type": "å¯¹è¯å“åº”",
    "content": "ç”¨æˆ·æ¶ˆæ¯ï¼šè¯·å®ç°è¿™ä¸ªRAGç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œå“åº”ï¼šä½œä¸ºä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“ï¼Œæˆ‘å°†åŸºäºæ‚¨æåˆ°çš„RAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿéœ€æ±‚ï¼Œåˆ†æå…¶æ ¸å¿ƒæ¨¡å—å¹¶æä¾›ä»£ç å®ç°å»ºè®®ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†çš„æŠ€æœ¯æ–¹æ¡ˆå’Œä»£ç é¢„è§ˆï¼š\n\n## RAGç³»ç»Ÿæ ¸å¿ƒæ¨¡å—åˆ†æ\n\n### æ ¸å¿ƒç»„ä»¶éœ€æ±‚ï¼š\n1. **æ–‡æ¡£å¤„ç†æ¨¡å—** - æ–‡æ¡£åŠ è½½ã€åˆ†å—ã€å‘é‡åŒ–\n2. **å‘é‡å­˜å‚¨æ¨¡å—** - å‘é‡ç´¢å¼•æ„å»ºå’Œæ£€ç´¢\n3. **æ£€ç´¢æ¨¡å—** - ç›¸ä¼¼åº¦æœç´¢å’Œç»“æœæ’åº\n4. **ç”Ÿæˆæ¨¡å—** - åŸºäºæ£€ç´¢ç»“æœçš„æ–‡æœ¬ç”Ÿæˆ\n\n## ä»£ç å®ç°é¢„è§ˆ\n\n### 1. æ–‡æ¡£å¤„ç†æ¨¡å—\n\n```python\nimport os\nimport PyPDF2\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nclass DocumentProcessor:\n    def __init__(self, model_name='all-MiniLM-L6-v2', chunk_size=512, chunk_overlap=50):\n        self.embedding_model = SentenceTransformer(model_name)\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap\n        )\n    \n    def load_documents(self, file_paths):\n        \"\"\"åŠ è½½å¤šç§æ ¼å¼çš„æ–‡æ¡£\"\"\"\n        documents = []\n        for file_path in file_paths:\n            if file_path.endswith('.pdf'):\n                documents.extend(self._load_pdf(file_path))\n            elif file_path.endswith('.txt'):\n                documents.extend(self._load_txt(file_path))\n        return documents\n    \n    def _load_pdf(self, file_path):\n        \"\"\"åŠ è½½PDFæ–‡æ¡£\"\"\"\n        with open(file_path, 'rb') as file:\n            pdf_reader = PyPDF2.PdfReader(file)\n            text = \"\"\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n            return [{\"content\": text, \"source\": file_path}]\n    \n    def chunk_documents(self, documents):\n        \"\"\"æ–‡æ¡£åˆ†å—å¤„ç†\"\"\"\n        chunks = []\n        for doc in documents:\n            text_chunks = self.text_splitter.split_text(doc[\"content\"])\n            for i, chunk in enumerate(text_chunks):\n                chunks.append({\n                    \"content\": chunk,\n                    \"source\": doc[\"source\"],\n                    \"chunk_id\": f\"{os.path.basename(doc['source'])}_{i}\",\n                   "
  },
  {
    "timestamp": "2025-11-21T11:37:33.618287",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T11:37:51.904129",
    "type": "å¯¹è¯å“åº”",
    "content": "ç”¨æˆ·æ¶ˆæ¯ï¼šè¯·å®ç°è¿™ä¸ªRAGç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œå“åº”ï¼šä½œä¸ºä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“ï¼Œæˆ‘å°†åŸºäºRAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„å…¸å‹æ¶æ„ï¼Œä¸ºæ‚¨ç”Ÿæˆæ ¸å¿ƒæ¨¡å—çš„ä»£ç å®ç°å»ºè®®ã€‚\n\n## RAGç³»ç»Ÿæ ¸å¿ƒæ¨¡å—ä»£ç å®ç°\n\n### 1. æ–‡æ¡£å¤„ç†æ¨¡å—\n```python\nimport os\nimport pickle\nfrom typing import List, Dict\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader, TextLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\nclass DocumentProcessor:\n    def __init__(self, chunk_size=1000, chunk_overlap=200):\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap\n        )\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-mpnet-base-v2\"\n        )\n    \n    def load_documents(self, file_path: str) -> List[Dict]:\n        \"\"\"åŠ è½½æ–‡æ¡£å¹¶åˆ†å‰²æˆchunks\"\"\"\n        if file_path.endswith('.pdf'):\n            loader = PyPDFLoader(file_path)\n        elif file_path.endswith('.txt'):\n            loader = TextLoader(file_path)\n        else:\n            raise ValueError(\"Unsupported file format\")\n        \n        documents = loader.load()\n        chunks = self.text_splitter.split_documents(documents)\n        return chunks\n    \n    def create_vector_store(self, chunks: List[Dict], save_path: str):\n        \"\"\"åˆ›å»ºå‘é‡å­˜å‚¨\"\"\"\n        vector_store = FAISS.from_documents(chunks, self.embeddings)\n        vector_store.save_local(save_path)\n        return vector_store\n```\n\n### 2. æ£€ç´¢æ¨¡å—\n```python\nfrom typing import List, Tuple\nimport numpy as np\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\nclass Retriever:\n    def __init__(self, vector_store_path: str):\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-mpnet-base-v2\"\n        )\n        self.vector_store = FAISS.load_local(\n            vector_store_path, \n            self.embeddings, \n            allow_dangerous_des"
  },
  {
    "timestamp": "2025-11-21T11:38:27.522016",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T11:38:44.756063",
    "type": "å¯¹è¯å“åº”",
    "content": "ç”¨æˆ·æ¶ˆæ¯ï¼šè¯·å®ç°è¿™ä¸ªRAGç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œå“åº”ï¼šä½œä¸ºä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“ï¼Œæˆ‘å°†åŸºäºRAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„å…¸å‹æ¶æ„ï¼Œä¸ºæ‚¨ç”Ÿæˆæ ¸å¿ƒæ¨¡å—çš„ä»£ç å®ç°å»ºè®®ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†çš„æŠ€æœ¯æ–¹æ¡ˆåˆ†æå’Œä»£ç é¢„è§ˆï¼š\n\n## æŠ€æœ¯æ–¹æ¡ˆåˆ†æ\n\n**æ ¸å¿ƒéœ€æ±‚ï¼š**\n- æ–‡æ¡£æ£€ç´¢æ¨¡å—ï¼šä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯\n- ç”Ÿæˆæ¨¡å—ï¼šåŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”\n- å‘é‡åŒ–å¤„ç†ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º\n- ç›¸ä¼¼åº¦è®¡ç®—ï¼šæ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ\n\n## ä»£ç å®ç°é¢„è§ˆ\n\n### 1. å‘é‡å­˜å‚¨å’Œæ£€ç´¢æ¨¡å—\n\n```python\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom typing import List, Dict, Tuple\n\nclass VectorStore:\n    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n        self.model = SentenceTransformer(model_name)\n        self.index = None\n        self.documents = []\n        \n    def build_index(self, documents: List[str]):\n        \"\"\"æ„å»ºå‘é‡ç´¢å¼•\"\"\"\n        self.documents = documents\n        embeddings = self.model.encode(documents)\n        self.index = faiss.IndexFlatIP(embeddings.shape[1])\n        self.index.add(embeddings.astype('float32'))\n        \n    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n        \"\"\"æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£\"\"\"\n        query_embedding = self.model.encode([query])\n        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n        \n        results = []\n        for score, idx in zip(scores[0], indices[0]):\n            if idx < len(self.documents):\n                results.append((self.documents[idx], float(score)))\n        return results\n```\n\n### 2. RAGæ ¸å¿ƒå¤„ç†æ¨¡å—\n\n```python\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nclass RAGSystem:\n    def __init__(self, vector_store: VectorStore, llm_model: str = \"microsoft/DialoGPT-medium\"):\n        self.vector_store = vector_store\n        self.generator = pipeline(\n            \"text-generation\",\n            model=llm_model,\n            tokenizer=AutoTokenizer.from_pretrained(llm_model),\n            device=0 if torch.cuda.is_available() else -1\n        )\n        \n"
  },
  {
    "timestamp": "2025-11-21T11:39:20.186038",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T11:39:39.496956",
    "type": "å¯¹è¯å“åº”",
    "content": "ç”¨æˆ·æ¶ˆæ¯ï¼šè¯·å®ç°è¿™ä¸ªRAGç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œå“åº”ï¼šä½œä¸ºä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“ï¼Œæˆ‘å°†åŸºäºRAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„æŠ€æœ¯æ–¹æ¡ˆï¼Œä¸ºæ‚¨ç”Ÿæˆæ ¸å¿ƒæ¨¡å—çš„ä»£ç å®ç°å»ºè®®ã€‚\n\n## RAGç³»ç»Ÿæ ¸å¿ƒæ¨¡å—ä»£ç å®ç°\n\n### 1. æ–‡æ¡£å¤„ç†æ¨¡å—\n```python\nimport os\nimport pickle\nfrom typing import List, Dict\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader, TextLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nclass DocumentProcessor:\n    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap\n        )\n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-mpnet-base-v2\"\n        )\n    \n    def load_documents(self, file_path: str) -> List[Dict]:\n        \"\"\"åŠ è½½æ–‡æ¡£å¹¶åˆ†å‰²\"\"\"\n        if file_path.endswith('.pdf'):\n            loader = PyPDFLoader(file_path)\n        elif file_path.endswith('.txt'):\n            loader = TextLoader(file_path)\n        else:\n            raise ValueError(\"Unsupported file format\")\n        \n        documents = loader.load()\n        return self.text_splitter.split_documents(documents)\n    \n    def create_vector_store(self, documents: List[Dict], save_path: str):\n        \"\"\"åˆ›å»ºå‘é‡å­˜å‚¨\"\"\"\n        vector_store = FAISS.from_documents(documents, self.embedding_model)\n        vector_store.save_local(save_path)\n        return vector_store\n```\n\n### 2. æ£€ç´¢æ¨¡å—\n```python\nfrom typing import List, Tuple\nimport numpy as np\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nclass Retriever:\n    def __init__(self, vector_store_path: str):\n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-mpnet-base-v2\"\n        )\n        self.vector_store = FAISS.load_local(\n            vector_store_path, \n            self.embedding_model, \n            allow_dangerous_deserialization=True\n       "
  },
  {
    "timestamp": "2025-11-21T11:40:13.963114",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:26:24.082371",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:26:33.288976",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:27:52.701843",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:28:00.754418",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:28:40.919882",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:31:28.823823",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:31:45.974253",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:32:40.621443",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:32:40.632607",
    "type": "æµ‹è¯•",
    "content": "æµ‹è¯•ä»£ç å®ç°å¸ˆæ—¥è®°åŠŸèƒ½"
  },
  {
    "timestamp": "2025-11-21T14:32:40.642432",
    "type": "ä»£ç ç”Ÿæˆ",
    "content": "å¼€å§‹ç”Ÿæˆæ–¹æ¡ˆå®ç°: æµ‹è¯•æ–¹æ¡ˆ"
  },
  {
    "timestamp": "2025-11-21T14:32:40.653509",
    "type": "ä»£ç ç”Ÿæˆ",
    "content": "æ–¹æ¡ˆ æµ‹è¯•æ–¹æ¡ˆ ä»£ç ç”Ÿæˆå®Œæˆ"
  },
  {
    "timestamp": "2025-11-21T14:33:02.356677",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:33:16.424078",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:33:16.435001",
    "type": "ä»£ç ç”Ÿæˆ",
    "content": "å¼€å§‹ç”Ÿæˆæ–¹æ¡ˆå®ç°: æ™ºèƒ½ä½“ååŒæµ‹è¯•æ–¹æ¡ˆ"
  },
  {
    "timestamp": "2025-11-21T14:33:16.443156",
    "type": "ä»£ç ç”Ÿæˆ",
    "content": "æ–¹æ¡ˆ æ™ºèƒ½ä½“ååŒæµ‹è¯•æ–¹æ¡ˆ ä»£ç ç”Ÿæˆå®Œæˆ"
  },
  {
    "timestamp": "2025-11-21T14:33:16.454383",
    "type": "å®¡æ ¸æäº¤",
    "content": "æäº¤ä»£ç å®¡æ ¸è¯·æ±‚: approval_20251121_143316 - æ™ºèƒ½ä½“ååŒæµ‹è¯•æ–¹æ¡ˆ"
  },
  {
    "timestamp": "2025-11-21T14:34:49.028588",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:34:53.154228",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:35:43.304272",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:35:51.778554",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:35:51.802782",
    "type": "å·²å®Œæˆ",
    "content": "ç”ŸæˆRAGç³»ç»Ÿå®ç°ä»£ç "
  },
  {
    "timestamp": "2025-11-21T14:37:08.734853",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:37:17.246340",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:37:17.269657",
    "type": "å·²å®Œæˆ",
    "content": "ç”ŸæˆRAGç³»ç»Ÿå®ç°ä»£ç "
  },
  {
    "timestamp": "2025-11-21T14:37:17.279672",
    "type": "å®¡æ ¸æäº¤",
    "content": "æäº¤ä»£ç å®¡æ ¸è¯·æ±‚: approval_20251121_143717 - ä¼ä¸šçº§RAGç³»ç»Ÿå®ç°ä»£ç "
  },
  {
    "timestamp": "2025-11-21T14:37:26.813420",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:37:34.226578",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  },
  {
    "timestamp": "2025-11-21T14:37:34.251554",
    "type": "å·²å®Œæˆ",
    "content": "ç”ŸæˆRAGç³»ç»Ÿå®ç°ä»£ç "
  },
  {
    "timestamp": "2025-11-21T14:37:34.262655",
    "type": "å®¡æ ¸æäº¤",
    "content": "æäº¤ä»£ç å®¡æ ¸è¯·æ±‚: approval_20251121_143734 - ä¼ä¸šçº§RAGç³»ç»Ÿå®ç°ä»£ç "
  },
  {
    "timestamp": "2025-11-21T15:07:53.578106",
    "type": "ç³»ç»Ÿå¯åŠ¨",
    "content": "ä»£ç å®ç°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šä»†äººï¼Œæƒé™ï¼šä¸»äººç¡®è®¤åæ‰§è¡Œ"
  }
]