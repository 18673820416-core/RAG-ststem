#!/usr/bin/env python3
# @self-expose: {"id": "data_crawler", "name": "Data Crawler", "type": "component", "version": "1.0.0", "needs": {"deps": [], "resources": []}, "provides": {"capabilities": ["Data CrawleråŠŸèƒ½"]}}
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹æ•°æ®çˆ¬å–å·¥å…·
- é€šè¿‡å‘½ä»¤æ¿€æ´»æ‰å¯åŠ¨çˆ¬å–
- æ”¯æŒå¢é‡æ›´æ–°å’Œå…¨é‡çˆ¬å–æ¨¡å¼
- çˆ¬å–å®Œæˆåæ•°æ®æŒä¹…åŒ–å­˜å‚¨
- é¿å…ä¸»ç¨‹åºå¯åŠ¨æ—¶é‡å¤çˆ¬å–
"""

import argparse
import logging
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from src.enhanced_data_crawler import EnhancedDataCrawler
from src.data_collector import DataCollector
from config.system_config import DATA_DIR

logger = logging.getLogger(__name__)

class DataCrawlerTool:
    """ç‹¬ç«‹æ•°æ®çˆ¬å–å·¥å…·"""
    
    def __init__(self):
        self.crawler = EnhancedDataCrawler()
        self.collector = DataCollector()
        self.data_file = DATA_DIR / "crawled_data.json"
        self.metadata_file = DATA_DIR / "crawl_metadata.json"
    
    def check_existing_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²çˆ¬å–çš„æ•°æ®"""
        return self.data_file.exists() and self.metadata_file.exists()
    
    def load_existing_data(self) -> Dict[str, Any]:
        """åŠ è½½å·²çˆ¬å–çš„æ•°æ®"""
        if not self.check_existing_data():
            return {"data": [], "metadata": {}}
        
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            return {"data": data, "metadata": metadata}
        except Exception as e:
            logger.error(f"åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {e}")
            return {"data": [], "metadata": {}}
    
    def save_data(self, data: List[Dict[str, Any]], metadata: Dict[str, Any]):
        """ä¿å­˜çˆ¬å–çš„æ•°æ®å’Œå…ƒæ•°æ®"""
        try:
            # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
            DATA_DIR.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ•°æ®
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜å…ƒæ•°æ®
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {self.data_file}")
            logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {self.metadata_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def crawl_full(self) -> List[Dict[str, Any]]:
        """å…¨é‡çˆ¬å–æ‰€æœ‰æ•°æ®"""
        logger.info("å¼€å§‹å…¨é‡æ•°æ®çˆ¬å–...")
        
        # çˆ¬å–æ‰€æœ‰äº¤äº’æ•°æ®
        all_data = self.crawler.crawl_all_sources()
        
        # æ„å»ºå…ƒæ•°æ®
        metadata = {
            "crawl_type": "full",
            "crawl_time": datetime.now().isoformat(),
            "total_records": len(all_data),
            "sources": {}
        }
        
        # ç»Ÿè®¡æ¥æºä¿¡æ¯
        for item in all_data:
            source = item.get('source', 'unknown')
            metadata['sources'][source] = metadata['sources'].get(source, 0) + 1
        
        # ä¿å­˜æ•°æ®
        self.save_data(all_data, metadata)
        
        logger.info(f"å…¨é‡çˆ¬å–å®Œæˆï¼å…±è·å¾— {len(all_data)} æ¡æ•°æ®")
        return all_data
    
    def crawl_incremental(self) -> List[Dict[str, Any]]:
        """å¢é‡çˆ¬å–æ–°æ•°æ®"""
        logger.info("å¼€å§‹å¢é‡æ•°æ®çˆ¬å–...")
        
        # åŠ è½½ç°æœ‰æ•°æ®
        existing_data = self.load_existing_data()
        old_data = existing_data.get("data", [])
        old_metadata = existing_data.get("metadata", {})
        
        # è·å–ä¸Šæ¬¡çˆ¬å–æ—¶é—´
        last_crawl_time = old_metadata.get("crawl_time")
        
        # å¢é‡çˆ¬å–é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„å¢é‡æ£€æµ‹ï¼‰
        # è¿™é‡Œæš‚æ—¶ä½¿ç”¨å…¨é‡çˆ¬å–ï¼Œå®é™…åº”è¯¥å®ç°å¢é‡æ£€æµ‹
        new_data = self.crawl_full()
        
        # åˆå¹¶æ•°æ®ï¼ˆå»é‡ï¼‰
        combined_data = self._merge_data(old_data, new_data)
        
        # æ›´æ–°å…ƒæ•°æ®
        metadata = {
            "crawl_type": "incremental",
            "crawl_time": datetime.now().isoformat(),
            "previous_crawl": last_crawl_time,
            "total_records": len(combined_data),
            "new_records": len(new_data) - len(old_data),
            "sources": {}
        }
        
        # ç»Ÿè®¡æ¥æºä¿¡æ¯
        for item in combined_data:
            source = item.get('source', 'unknown')
            metadata['sources'][source] = metadata['sources'].get(source, 0) + 1
        
        # ä¿å­˜æ•°æ®
        self.save_data(combined_data, metadata)
        
        logger.info(f"å¢é‡çˆ¬å–å®Œæˆï¼æ–°å¢ {metadata['new_records']} æ¡æ•°æ®ï¼Œæ€»è®¡ {len(combined_data)} æ¡")
        return combined_data
    
    def _merge_data(self, old_data: List[Dict], new_data: List[Dict]) -> List[Dict]:
        """åˆå¹¶æ–°æ—§æ•°æ®ï¼ˆç®€å•å»é‡ï¼‰"""
        # åŸºäºå†…å®¹å“ˆå¸Œå»é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        seen = set()
        merged = []
        
        for item in old_data + new_data:
            # ç”Ÿæˆç®€å•çš„å“ˆå¸Œæ ‡è¯†
            content_hash = hash(item.get('content', '') + item.get('source', ''))
            
            if content_hash not in seen:
                seen.add(content_hash)
                merged.append(item)
        
        return merged
    
    def show_status(self):
        """æ˜¾ç¤ºæ•°æ®çˆ¬å–çŠ¶æ€"""
        if not self.check_existing_data():
            print("âŒ æœªå‘ç°å·²çˆ¬å–çš„æ•°æ®")
            print("   è¯·å…ˆè¿è¡Œçˆ¬å–å‘½ä»¤: python tools/data_crawler.py --crawl")
            return
        
        # åŠ è½½å…ƒæ•°æ®
        with open(self.metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        print("ğŸ“Š æ•°æ®çˆ¬å–çŠ¶æ€:")
        print("=" * 50)
        print(f"çˆ¬å–ç±»å‹: {metadata.get('crawl_type', 'unknown')}")
        print(f"çˆ¬å–æ—¶é—´: {metadata.get('crawl_time', 'unknown')}")
        print(f"æ•°æ®æ€»é‡: {metadata.get('total_records', 0)} æ¡")
        
        if 'sources' in metadata:
            print("\næ•°æ®æ¥æºç»Ÿè®¡:")
            for source, count in metadata['sources'].items():
                print(f"  {source}: {count} æ¡")
        
        if metadata.get('crawl_type') == 'incremental':
            print(f"æ–°å¢æ•°æ®: {metadata.get('new_records', 0)} æ¡")
            print(f"ä¸Šæ¬¡çˆ¬å–: {metadata.get('previous_crawl', 'unknown')}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç‹¬ç«‹æ•°æ®çˆ¬å–å·¥å…·')
    parser.add_argument('--crawl', action='store_true', help='å…¨é‡çˆ¬å–æ‰€æœ‰æ•°æ®')
    parser.add_argument('--incremental', action='store_true', help='å¢é‡çˆ¬å–æ–°æ•°æ®')
    parser.add_argument('--status', action='store_true', help='æ˜¾ç¤ºçˆ¬å–çŠ¶æ€')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°çˆ¬å–ï¼ˆå¿½ç•¥ç°æœ‰æ•°æ®ï¼‰')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    tool = DataCrawlerTool()
    
    if args.status:
        tool.show_status()
    
    elif args.crawl:
        if tool.check_existing_data() and not args.force:
            print("âš ï¸  å·²å­˜åœ¨çˆ¬å–æ•°æ®ï¼Œä½¿ç”¨ --force å‚æ•°å¼ºåˆ¶é‡æ–°çˆ¬å–")
            print("   æˆ–ä½¿ç”¨ --incremental è¿›è¡Œå¢é‡çˆ¬å–")
            return
        
        tool.crawl_full()
    
    elif args.incremental:
        if not tool.check_existing_data():
            print("â„¹ï¸  æœªå‘ç°ç°æœ‰æ•°æ®ï¼Œå°†è¿›è¡Œå…¨é‡çˆ¬å–")
            tool.crawl_full()
        else:
            tool.crawl_incremental()
    
    else:
        print("ç‹¬ç«‹æ•°æ®çˆ¬å–å·¥å…·")
        print("=" * 30)
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python tools/data_crawler.py --crawl        # å…¨é‡çˆ¬å–")
        print("  python tools/data_crawler.py --incremental  # å¢é‡çˆ¬å–")
        print("  python tools/data_crawler.py --status      # æ˜¾ç¤ºçŠ¶æ€")
        print("  python tools/data_crawler.py --crawl --force # å¼ºåˆ¶é‡æ–°çˆ¬å–")

if __name__ == "__main__":
    main()