#!/usr/bin/env python3
# @self-expose: {"id": "memory_slicer_tool", "name": "å¤šå±‚æ¬¡è‡ªé€‚åº”åˆ†ç‰‡å·¥å…·", "type": "tool", "version": "2.3.0", "needs": {"deps": ["src.memory_bubble_manager", "src.cognitive_engines.memory_reconstruction_engine"], "resources": []}, "provides": {"capabilities": ["å¤šå±‚æ¬¡è‡ªé€‚åº”åˆ†ç‰‡ç­–ç•¥", "ä¿¡æ¯ç†µé€’å½’åˆ†ç‰‡", "LLMç²¾ç‚¼æ”¹å†™", "å›°æƒ‘åº¦å¤åˆåˆ†ç‰‡", "å±‚çº§ç¼–ç ç®¡ç†", "åˆ†ç‰‡å¤±è´¥æ³¡æ³¡è®°å½•", "æ™ºèƒ½é˜ˆå€¼èŒƒå›´ä¼˜åŒ–", "è‡ªé€‚åº”é€’å½’æ·±åº¦è°ƒæ•´", "LLMé‡æ„æœ‰æ•ˆæ€§æ ¡éªŒ"], "methods": ["slice_text", "parse_slice_id", "get_slice_hierarchy", "hierarchical_retrieval"]}}
# -*- coding: utf-8 -*-
"""
è®°å¿†åˆ‡ç‰‡ç®¡ç†å·¥å…· - å¤šå±‚æ¬¡è‡ªé€‚åº”åˆ†ç‰‡ç­–ç•¥

å¼€å‘æç¤ºè¯æ¥æºï¼šç”¨æˆ·æå‡ºçš„"é€»è¾‘åˆ‡ç‰‡å·¥å…·å’Œäº‹ä»¶äºŒæ¬¡åˆ‡ç‰‡å·¥å…·ç»Ÿä¸€æ‰“åŒ…æˆä¸€ä¸ªåˆ‡ç‰‡å·¥å…·ï¼Œ
æ³¨å†Œåœ¨å·¥å…·ç®±é‡Œï¼Œä¸“é—¨è´Ÿè´£è®°å¿†çš„åˆ‡ç‰‡ç®¡ç†"

é‡è¦åŸç†è®°å½•ï¼šå¤šå±‚æ¬¡è‡ªé€‚åº”åˆ†ç‰‡ç­–ç•¥ï¼ˆ2025å¹´12æœˆ3æ—¥ä¼˜åŒ–ï¼‰
æ ¸å¿ƒç†å¿µï¼šæˆæœ¬ä¸è´¨é‡çš„æ¢¯åº¦å¹³è¡¡ï¼Œä»ä½æˆæœ¬çš„çº¯ç®—æ³•åˆ†ç‰‡åˆ°é«˜æˆæœ¬çš„LLMè¾…åŠ©åˆ†ç‰‡

åˆ†ç‰‡æµç¨‹ï¼ˆå››å±‚æ¢¯åº¦ï¼‰ï¼š
ã€ç¬¬ä¸€å±‚ã€‘ä¿¡æ¯ç†µé€’å½’åˆ†ç‰‡ï¼ˆæ— LLMè°ƒç”¨ï¼‰
    â†“ æˆåŠŸ â†’ å®Œæˆ
    â†“ å¤±è´¥ï¼ˆè¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦ï¼‰
ã€ç¬¬äºŒå±‚ã€‘æ–‡æœ¬é¢„å¤„ç† + LLMç²¾ç‚¼æ”¹å†™ + å†æ¬¡é€’å½’åˆ†ç‰‡
    â†“ æˆåŠŸ â†’ å®Œæˆ
    â†“ å¤±è´¥
ã€ç¬¬ä¸‰å±‚ã€‘å›°æƒ‘åº¦è®¡ç®— + å¤åˆåˆ†ç‰‡ï¼ˆéœ€LLMï¼‰
    â†“ æˆåŠŸ â†’ å®Œæˆ
    â†“ å¤±è´¥
ã€ç¬¬å››å±‚ã€‘å¼ºåˆ¶åˆ†ç‰‡ + è®°å½•é—®é¢˜åˆ°æ³¡æ³¡

æŠ€æœ¯åŸç†ï¼š
- ä¿¡æ¯ç†µè®¡ç®—ï¼šH(X) = -âˆ‘ p(x) * logâ‚‚ p(x)ï¼Œç”¨äºæ£€æµ‹é€»è¾‘è¾¹ç•Œ
- å›°æƒ‘åº¦ä¼°ç®—ï¼šåŸºäºn-gramæ¨¡å‹çš„ç®€åŒ–å›°æƒ‘åº¦è®¡ç®—ï¼ˆæ— éœ€å®Œæ•´LLMï¼‰
- å±‚çº§ç¼–ç ï¼šç‚¹åˆ†éš”ç¬¦ï¼ˆ1, 1.1, 1.1.1ï¼‰ä¿æŒé€»è¾‘é“¾ç»“æ„
- æ³¡æ³¡è®°å½•ï¼šåˆ†ç‰‡å¤±è´¥æ—¶è®°å½•æ–‡ä»¶åã€å°è¯•æ–¹æ³•ã€å¤±è´¥åŸå› ã€ä¼˜åŒ–å»ºè®®
"""

import logging
import re
import math
import json
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime
from collections import Counter

logger = logging.getLogger(__name__)

class MemorySlicerTool:
    """è®°å¿†åˆ‡ç‰‡ç®¡ç†å·¥å…· - å¤šå±‚æ¬¡è‡ªé€‚åº”åˆ†ç‰‡ç­–ç•¥"""
    
    def __init__(self, base_path: str = "E:\\RAGç³»ç»Ÿ"):
        self.base_path = Path(base_path)
        
        # é»˜è®¤é…ç½® - å¤šå±‚æ¬¡è‡ªé€‚åº”åˆ†ç‰‡ç­–ç•¥
        self.default_config = {
            # åˆ†å±‚é˜ˆå€¼ï¼šåªæœ‰å¤§åˆ‡ç‰‡æ‰ä¼šè¢«è¿›ä¸€æ­¥åˆ†å‰²
            'size_thresholds': [1000, 700, 500, 300, 200],
            'max_recursion_depth': 10,  # æœ€å¤§é€’å½’æ·±åº¦
            'min_slice_size': 50,      # æœ€å°åˆ‡ç‰‡å¤§å°
            'quality_threshold': 0.7,   # åˆ‡ç‰‡è´¨é‡é˜ˆå€¼
            'enable_entropy_analysis': True,  # å¯ç”¨ä¿¡æ¯ç†µåˆ†æ
            'enable_semantic_evaluation': True,  # å¯ç”¨è¯­ä¹‰è´¨é‡è¯„ä¼°
            'enable_hierarchical_encoding': True,  # å¯ç”¨å±‚çº§ç¼–ç 
            'enable_llm_refinement': True,  # å¯ç”¨LLMç²¾ç‚¼æ”¹å†™
            'enable_perplexity_analysis': True,  # å¯ç”¨å›°æƒ‘åº¦åˆ†æ
            'enable_bubble_logging': True,  # å¯ç”¨æ³¡æ³¡è®°å½•
        }
        
        # åˆå§‹åŒ–è®°å¿†æ³¡æ³¡ç®¡ç†å™¨ï¼ˆç”¨äºè®°å½•åˆ†ç‰‡å¤±è´¥ï¼‰
        self.bubble_manager = None
        try:
            from src.memory_bubble_manager import MemoryBubbleManager
            self.bubble_manager = MemoryBubbleManager(agent_id="memory_slicer_tool")
            logger.info("æ³¡æ³¡ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"æ³¡æ³¡ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†è·³è¿‡æ³¡æ³¡è®°å½•: {e}")
        
        # åˆå§‹åŒ–è®°å¿†é‡æ„å¼•æ“ï¼ˆç”¨äºLLMç²¾ç‚¼æ”¹å†™ï¼‰
        self.reconstruction_engine = None
        try:
            from src.cognitive_engines.memory_reconstruction_engine import MemoryReconstructionEngine
            self.reconstruction_engine = MemoryReconstructionEngine()
            logger.info("è®°å¿†é‡æ„å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"è®°å¿†é‡æ„å¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œå°†è·³è¿‡LLMç²¾ç‚¼: {e}")
        
        logger.info("è®°å¿†åˆ‡ç‰‡ç®¡ç†å·¥å…·åˆå§‹åŒ–å®Œæˆï¼ˆå¤šå±‚æ¬¡è‡ªé€‚åº”åˆ†ç‰‡ç­–ç•¥ï¼‰")
    
    def slice_text(self, text: str, metadata: Dict[str, Any] = None, 
                   config: Dict[str, Any] = None, source_file: str = None) -> List[Dict[str, Any]]:
        """
        å¤šå±‚æ¬¡è‡ªé€‚åº”åˆ†ç‰‡ç­–ç•¥ï¼šä»ä½æˆæœ¬ç®—æ³•åˆ†ç‰‡åˆ°é«˜æˆæœ¬LLMè¾…åŠ©åˆ†ç‰‡
        
        Args:
            text: å¾…åˆ‡ç‰‡çš„æ–‡æœ¬å†…å®¹
            metadata: å…ƒæ•°æ®ä¿¡æ¯
            config: åˆ‡ç‰‡é…ç½®å‚æ•°
            source_file: æºæ–‡ä»¶åï¼ˆç”¨äºæ³¡æ³¡è®°å½•ï¼‰
            
        Returns:
            åˆ‡ç‰‡ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«å†…å®¹å’Œå…ƒä¿¡æ¯ï¼ŒåŒ…å«å±‚çº§ç¼–ç 
        """
        
        if not text or not text.strip():
            logger.warning("åˆ‡ç‰‡æ–‡æœ¬ä¸ºç©º")
            return []
        
        # åˆå¹¶é…ç½®
        merged_config = {**self.default_config, **(config or {})}
        
        # ğŸ”§ ä¼˜åŒ–ï¼šè‡ªé€‚åº”é€’å½’ä¸é˜ˆå€¼è°ƒæ•´ï¼ˆé’ˆå¯¹æ‰€æœ‰æ–‡æœ¬é•¿åº¦ï¼‰
        try:
            if merged_config.get('adaptive_recursion', True):
                length = len(text)
                
                # æ ¹æ®æ–‡æœ¬é•¿åº¦åŠ¨æ€è°ƒæ•´é€’å½’æ·±åº¦
                if length < 1000:
                    # å°æ–‡æœ¬ï¼šä¸éœ€è¦é€’å½’ï¼Œç›´æ¥è¿”å›
                    merged_config['max_recursion_depth'] = 3
                elif length < 5000:
                    # ä¸­ç­‰æ–‡æœ¬ï¼šé€‚åº¦é€’å½’ï¼ˆ5å±‚è¶³å¤Ÿï¼‰
                    merged_config['max_recursion_depth'] = 5
                elif length < 50000:
                    # è¾ƒå¤§æ–‡æœ¬ï¼šéœ€è¦8-10å±‚é€’å½’
                    merged_config['max_recursion_depth'] = max(merged_config.get('max_recursion_depth', 10), 8)
                elif length < 200000:
                    # è¶…å¤§æ–‡æœ¬ï¼šå¢åŠ 3å±‚
                    extra_depth = 3
                    merged_config['max_recursion_depth'] = max(merged_config.get('max_recursion_depth', 10), 10 + extra_depth)
                    if isinstance(merged_config.get('size_thresholds'), list) and merged_config['size_thresholds']:
                        merged_config['size_thresholds'] = [min(t + 300, 3000) for t in merged_config['size_thresholds']]
                else:
                    # å·¨å¤§æ–‡æœ¬ï¼šå¢åŠ 5å±‚
                    extra_depth = 5
                    merged_config['max_recursion_depth'] = max(merged_config.get('max_recursion_depth', 10), 10 + extra_depth)
                    if isinstance(merged_config.get('size_thresholds'), list) and merged_config['size_thresholds']:
                        merged_config['size_thresholds'] = [min(t + 300, 3000) for t in merged_config['size_thresholds']]
                    # å¤§æ–‡æœ¬æ—¶é€‚åº¦æ”¾å®½è´¨é‡é˜ˆå€¼ï¼Œé¿å…å…¨é‡è¢«è¿‡æ»¤
                    if merged_config.get('quality_threshold', 0.7) > 0.6:
                        merged_config['quality_threshold'] = 0.6
        except Exception:
            pass
        
        # åˆ†ç‰‡å°è¯•è®°å½•
        attempt_log = {
            'source_file': source_file or 'unknown',
            'text_length': len(text),
            'attempts': [],
            'success': False,
            'final_method': None
        }
        
        try:
            logger.info(f"å¼€å§‹å¤šå±‚æ¬¡è‡ªé€‚åº”åˆ†ç‰‡ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            
            # ã€ç¬¬ä¸€å±‚ã€‘ä¿¡æ¯ç†µé€’å½’åˆ†ç‰‡ï¼ˆæ— LLMè°ƒç”¨ï¼‰
            logger.info("ã€ç¬¬ä¸€å±‚ã€‘å°è¯•ä¿¡æ¯ç†µé€’å½’åˆ†ç‰‡...")
            attempt_log['attempts'].append('ç¬¬ä¸€å±‚ï¼šä¿¡æ¯ç†µé€’å½’åˆ†ç‰‡')
            
            recursive_slices, success = self._try_entropy_recursive_slice(
                text=text,
                metadata=metadata,
                config=merged_config
            )
            
            if success and recursive_slices:
                logger.info(f"ã€ç¬¬ä¸€å±‚ã€‘ä¿¡æ¯ç†µé€’å½’åˆ†ç‰‡æˆåŠŸï¼Œç”Ÿæˆ {len(recursive_slices)} ä¸ªåˆ‡ç‰‡")
                attempt_log['success'] = True
                attempt_log['final_method'] = 'ç¬¬ä¸€å±‚ï¼šä¿¡æ¯ç†µé€’å½’åˆ†ç‰‡'
                return self._finalize_slices(recursive_slices, merged_config, attempt_log)
            
            logger.warning("ã€ç¬¬ä¸€å±‚ã€‘ä¿¡æ¯ç†µé€’å½’åˆ†ç‰‡å¤±è´¥ï¼Œè¿›å…¥ç¬¬äºŒå±‚")
            
            # ã€ç¬¬äºŒå±‚ã€‘æ–‡æœ¬é¢„å¤„ç† + LLMç²¾ç‚¼æ”¹å†™ + å†æ¬¡é€’å½’åˆ†ç‰‡
            if merged_config['enable_llm_refinement'] and self.reconstruction_engine:
                logger.info("ã€ç¬¬äºŒå±‚ã€‘å°è¯•LLMç²¾ç‚¼æ”¹å†™ + é€’å½’åˆ†ç‰‡...")
                attempt_log['attempts'].append('ç¬¬äºŒå±‚ï¼šLLMç²¾ç‚¼æ”¹å†™ + é€’å½’åˆ†ç‰‡')
                
                refined_slices, success = self._try_llm_refinement_slice(
                    text=text,
                    metadata=metadata,
                    config=merged_config
                )
                
                if success and refined_slices:
                    logger.info(f"ã€ç¬¬äºŒå±‚ã€‘LLMç²¾ç‚¼æ”¹å†™åˆ†ç‰‡æˆåŠŸï¼Œç”Ÿæˆ {len(refined_slices)} ä¸ªåˆ‡ç‰‡")
                    attempt_log['success'] = True
                    attempt_log['final_method'] = 'ç¬¬äºŒå±‚ï¼šLLMç²¾ç‚¼æ”¹å†™ + é€’å½’åˆ†ç‰‡'
                    return self._finalize_slices(refined_slices, merged_config, attempt_log)
                
                logger.warning("ã€ç¬¬äºŒå±‚ã€‘LLMç²¾ç‚¼æ”¹å†™åˆ†ç‰‡å¤±è´¥ï¼Œè¿›å…¥ç¬¬ä¸‰å±‚")
            
            # ã€ç¬¬ä¸‰å±‚ã€‘å›°æƒ‘åº¦è®¡ç®— + å¤åˆåˆ†ç‰‡
            if merged_config['enable_perplexity_analysis']:
                logger.info("ã€ç¬¬ä¸‰å±‚ã€‘å°è¯•å›°æƒ‘åº¦å¤åˆåˆ†ç‰‡...")
                attempt_log['attempts'].append('ç¬¬ä¸‰å±‚ï¼šå›°æƒ‘åº¦å¤åˆåˆ†ç‰‡')
                
                perplexity_slices, success = self._try_perplexity_compound_slice(
                    text=text,
                    metadata=metadata,
                    config=merged_config
                )
                
                if success and perplexity_slices:
                    logger.info(f"ã€ç¬¬ä¸‰å±‚ã€‘å›°æƒ‘åº¦å¤åˆåˆ†ç‰‡æˆåŠŸï¼Œç”Ÿæˆ {len(perplexity_slices)} ä¸ªåˆ‡ç‰‡")
                    attempt_log['success'] = True
                    attempt_log['final_method'] = 'ç¬¬ä¸‰å±‚ï¼šå›°æƒ‘åº¦å¤åˆåˆ†ç‰‡'
                    return self._finalize_slices(perplexity_slices, merged_config, attempt_log)
                
                logger.warning("ã€ç¬¬ä¸‰å±‚ã€‘å›°æƒ‘åº¦å¤åˆåˆ†ç‰‡å¤±è´¥ï¼Œè¿›å…¥ç¬¬å››å±‚")
            
            # ã€ç¬¬å››å±‚ã€‘å¼ºåˆ¶åˆ†ç‰‡ + è®°å½•é—®é¢˜åˆ°æ³¡æ³¡
            logger.warning("ã€ç¬¬å››å±‚ã€‘æ‰€æœ‰æ™ºèƒ½åˆ†ç‰‡æ–¹æ³•å¤±è´¥ï¼Œæ‰§è¡Œå¼ºåˆ¶åˆ†ç‰‡")
            attempt_log['attempts'].append('ç¬¬å››å±‚ï¼šå¼ºåˆ¶åˆ†ç‰‡ï¼ˆå…œåº•ï¼‰')
            
            forced_slices = self._force_slice_and_log(
                text=text,
                metadata=metadata,
                config=merged_config,
                attempt_log=attempt_log
            )
            
            attempt_log['success'] = len(forced_slices) > 0
            attempt_log['final_method'] = 'ç¬¬å››å±‚ï¼šå¼ºåˆ¶åˆ†ç‰‡ï¼ˆå…œåº•ï¼‰'
            
            return self._finalize_slices(forced_slices, merged_config, attempt_log)
            
        except Exception as e:
            logger.error(f"è®°å¿†åˆ‡ç‰‡å®Œå…¨å¤±è´¥: {e}")
            attempt_log['error'] = str(e)
            self._log_failure_to_bubble(attempt_log)
            return []
    
    # =========================================================================
    # å¤šå±‚æ¬¡åˆ†ç‰‡ç­–ç•¥æ–¹æ³•
    # =========================================================================
    
    def _try_entropy_recursive_slice(self, text: str, metadata: Dict[str, Any], 
                                     config: Dict[str, Any]) -> tuple:
        """ç¬¬ä¸€å±‚ï¼šä¿¡æ¯ç†µé€’å½’åˆ†ç‰‡ï¼ˆæ— LLMè°ƒç”¨ï¼‰
        
        Returns:
            (slices, success): åˆ‡ç‰‡ç»“æœå’ŒæˆåŠŸæ ‡å¿—
        """
        try:
            # æ‰§è¡Œé€’å½’åˆ†ç‰‡
            slices = self._recursive_slice(
                text=text,
                metadata=metadata,
                config=config,
                current_depth=0,
                parent_id=""
            )
            
            if not slices:
                return [], False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ‡ç‰‡æ˜¯ç”±äºè¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦è€Œå¼ºåˆ¶ç”Ÿæˆçš„
            has_max_depth_slices = any(
                slice_data.get('slice_method') == 'recursive_max_depth' 
                for slice_data in slices
            )
            
            if has_max_depth_slices:
                logger.warning("æ£€æµ‹åˆ°è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦çš„åˆ‡ç‰‡ï¼Œç¬¬ä¸€å±‚åˆ†ç‰‡è®¤ä¸ºå¤±è´¥")
                return slices, False
            
            return slices, True
            
        except Exception as e:
            logger.error(f"ç¬¬ä¸€å±‚ä¿¡æ¯ç†µé€’å½’åˆ†ç‰‡å¤±è´¥: {e}")
            return [], False
    
    def _try_llm_refinement_slice(self, text: str, metadata: Dict[str, Any], 
                                  config: Dict[str, Any]) -> tuple:
        """ç¬¬äºŒå±‚ï¼šLLMç²¾ç‚¼æ”¹å†™ + å†æ¬¡é€’å½’åˆ†ç‰‡
        
        Returns:
            (slices, success): åˆ‡ç‰‡ç»“æœå’ŒæˆåŠŸæ ‡å¿—
        """
        try:
            if not self.reconstruction_engine:
                logger.warning("LLMé‡æ„å¼•æ“æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ç¬¬äºŒå±‚")
                return [], False
            
            logger.info("ä½¿ç”¨LLMè¿›è¡Œæ–‡æœ¬ç²¾ç‚¼æ”¹å†™...")
            
            # è°ƒç”¨è®°å¿†é‡æ„å¼•æ“è¿›è¡Œæ–‡æœ¬ç²¾ç‚¼
            reconstruction_result = self.reconstruction_engine.reconstruct_memory(
                memory_content=text,
                context=metadata or {}
            )
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®åˆ¤å®šé‡æ„æ˜¯å¦æœ‰æ•ˆ
            # åŸé€»è¾‘ï¼šif refined_text == text â†’ å¤±è´¥ï¼ˆé”™è¯¯ï¼ï¼‰
            # æ–°é€»è¾‘ï¼šæ£€æŸ¥ä»¥ä¸‹æ¡ä»¶
            refined_text = reconstruction_result.get('reconstructed_content', '')
            confidence = reconstruction_result.get('confidence', 0.0)
            should_delete = reconstruction_result.get('should_delete', False)
            
            # åˆ¤å®šæ¡ä»¶ï¼š
            # 1. ä¸åº”è¯¥åˆ é™¤
            # 2. æœ‰æ•ˆå†…å®¹ï¼ˆéç©ºä¸”é•¿åº¦åˆç†ï¼‰
            # 3. å¯ä¿¡åº¦è¾¾æ ‡ï¼ˆ>70%ï¼‰
            if should_delete:
                logger.warning(f"LLMé‡æ„å»ºè®®åˆ é™¤æ­¤è®°å¿†ï¼š{reconstruction_result.get('delete_reason', 'æœªçŸ¥')}")
                return [], False
            
            if not refined_text or len(refined_text.strip()) < 20:
                logger.warning("LLMç²¾ç‚¼è¿”å›æ— æ•ˆå†…å®¹ï¼ˆè¿‡çŸ­æˆ–ä¸ºç©ºï¼‰")
                return [], False
            
            if confidence < 0.7:
                logger.warning(f"LLMç²¾ç‚¼å¯ä¿¡åº¦ä¸è¶³ï¼š{confidence:.2%} < 70%")
                return [], False
            
            logger.info(f"LLMç²¾ç‚¼å®Œæˆï¼ŒåŸæ–‡æœ¬ {len(text)} å­—ç¬¦ -> ç²¾ç‚¼å {len(refined_text)} å­—ç¬¦ï¼Œå¯ä¿¡åº¦: {confidence:.2%}")
            
            # å¯¹ç²¾ç‚¼åçš„æ–‡æœ¬å†æ¬¡æ‰§è¡Œé€’å½’åˆ†ç‰‡
            slices = self._recursive_slice(
                text=refined_text,
                metadata=metadata,
                config=config,
                current_depth=0,
                parent_id=""
            )
            
            if not slices:
                return [], False
            
            # æ ‡è®°åˆ‡ç‰‡ç»è¿‡LLMç²¾ç‚¼
            for slice_data in slices:
                slice_data['llm_refined'] = True
                slice_data['original_text_length'] = len(text)
                slice_data['refined_text_length'] = len(refined_text)
            
            return slices, True
            
        except Exception as e:
            logger.error(f"ç¬¬äºŒå±‚LLMç²¾ç‚¼æ”¹å†™åˆ†ç‰‡å¤±è´¥: {e}")
            return [], False
    
    def _try_perplexity_compound_slice(self, text: str, metadata: Dict[str, Any], 
                                       config: Dict[str, Any]) -> tuple:
        """ç¬¬ä¸‰å±‚ï¼šå›°æƒ‘åº¦è®¡ç®— + å¤åˆåˆ†ç‰‡
        
        Returns:
            (slices, success): åˆ‡ç‰‡ç»“æœå’ŒæˆåŠŸæ ‡å¿—
        """
        try:
            logger.info("è®¡ç®—æ–‡æœ¬å›°æƒ‘åº¦ï¼ŒæŸ¥æ‰¾å›°æƒ‘åº¦å˜åŒ–ç‚¹...")
            
            # è®¡ç®—å›°æƒ‘åº¦åˆ†å‰²ç‚¹
            perplexity_split_points = self._find_perplexity_boundaries(text, config)
            
            if not perplexity_split_points:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›°æƒ‘åº¦åˆ†å‰²ç‚¹")
                return [], False
            
            # åŸºäºå›°æƒ‘åº¦åˆ†å‰²ç‚¹è¿›è¡Œåˆ†ç‰‡
            slices = []
            start_pos = 0
            
            for i, split_point in enumerate(perplexity_split_points):
                end_pos = split_point
                segment = text[start_pos:end_pos].strip()
                
                if len(segment) >= config['min_slice_size']:
                    slice_id = f"{i + 1}"
                    slices.append({
                        'content': segment,
                        'slice_id': slice_id,
                        'slice_depth': 0,
                        'parent_id': '',
                        'slice_method': 'perplexity_compound',
                        'entropy': self._calculate_entropy(segment),
                        'perplexity': self._calculate_perplexity(segment)
                    })
                
                start_pos = end_pos
            
            # å¤„ç†å‰©ä½™æ–‡æœ¬
            if start_pos < len(text):
                remaining = text[start_pos:].strip()
                if len(remaining) >= config['min_slice_size']:
                    slices.append({
                        'content': remaining,
                        'slice_id': f"{len(slices) + 1}",
                        'slice_depth': 0,
                        'parent_id': '',
                        'slice_method': 'perplexity_compound',
                        'entropy': self._calculate_entropy(remaining),
                        'perplexity': self._calculate_perplexity(remaining)
                    })
            
            if not slices:
                return [], False
            
            logger.info(f"å›°æƒ‘åº¦å¤åˆåˆ†ç‰‡ç”Ÿæˆ {len(slices)} ä¸ªåˆ‡ç‰‡")
            return slices, True
            
        except Exception as e:
            logger.error(f"ç¬¬ä¸‰å±‚å›°æƒ‘åº¦å¤åˆåˆ†ç‰‡å¤±è´¥: {e}")
            return [], False
    
    def _force_slice_and_log(self, text: str, metadata: Dict[str, Any], 
                            config: Dict[str, Any], attempt_log: Dict) -> List[Dict[str, Any]]:
        """ç¬¬å››å±‚ï¼šå¼ºåˆ¶åˆ†ç‰‡ + è®°å½•é—®é¢˜åˆ°æ³¡æ³¡
        
        Returns:
            å¼ºåˆ¶åˆ†ç‰‡ç»“æœ
        """
        logger.warning("æ‰§è¡Œå¼ºåˆ¶åˆ†ç‰‡ä½œä¸ºå…œåº•ç­–ç•¥")
        
        # æ‰§è¡Œç®€å•çš„å¼ºåˆ¶åˆ†ç‰‡
        slices = []
        max_size = config['size_thresholds'][0] if config['size_thresholds'] else 1000
        
        start = 0
        slice_index = 0
        
        while start < len(text):
            end = min(start + max_size, len(text))
            
            # å°è¯•åœ¨å¥å­è¾¹ç•Œå¤„åˆ†å‰²
            if end < len(text):
                sentence_end = text.rfind('ã€‚', start, end)
                if sentence_end == -1:
                    sentence_end = text.rfind('.', start, end)
                
                if sentence_end != -1 and sentence_end > start + max_size * 0.5:
                    end = sentence_end + 1
            
            segment = text[start:end].strip()
            
            if segment:
                slices.append({
                    'content': segment,
                    'slice_id': f"{slice_index + 1}",
                    'slice_depth': 0,
                    'parent_id': '',
                    'slice_method': 'forced_fallback',
                    'entropy': self._calculate_entropy(segment),
                    'warning': 'æ­¤åˆ‡ç‰‡é€šè¿‡å¼ºåˆ¶åˆ†ç‰‡ç”Ÿæˆï¼Œå¯èƒ½ç ´åé€»è¾‘å®Œæ•´æ€§'
                })
                slice_index += 1
            
            start = end
        
        # è®°å½•å¤±è´¥åˆ°æ³¡æ³¡
        self._log_failure_to_bubble(attempt_log)
        
        return slices
    
    def _recursive_slice(self, text: str, metadata: Dict[str, Any], 
                        config: Dict[str, Any], current_depth: int, 
                        parent_id: str) -> List[Dict[str, Any]]:
        """
        é€’å½’åˆ†ç‰‡æ ¸å¿ƒæ–¹æ³• - åŸºäºä¿¡æ¯ç†µé©±åŠ¨çš„é€’å½’åˆ†ç‰‡æœºåˆ¶
        
        Args:
            text: å¾…åˆ†ç‰‡æ–‡æœ¬
            metadata: å…ƒæ•°æ®
            config: é…ç½®å‚æ•°
            current_depth: å½“å‰é€’å½’æ·±åº¦
            parent_id: çˆ¶çº§åˆ‡ç‰‡ID
            
        Returns:
            åˆ†ç‰‡ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«å±‚çº§ç¼–ç 
        """
        
        # æ£€æŸ¥é€’å½’æ·±åº¦é™åˆ¶
        if current_depth >= config['max_recursion_depth']:
            logger.warning(f"è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦ {current_depth}ï¼Œå°†å½“å‰æ–‡æœ¬ä½œä¸ºæœ€ç»ˆåˆ‡ç‰‡")
            # å½“è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦æ—¶ï¼Œå°†å½“å‰æ–‡æœ¬ä½œä¸ºä¸€ä¸ªåˆ‡ç‰‡è¿”å›ï¼Œè€Œä¸æ˜¯ç©ºåˆ—è¡¨
            slice_id = self._generate_slice_id(parent_id, current_depth, 0)
            return [{
                'content': text,
                'slice_id': slice_id,
                'slice_depth': current_depth,
                'parent_id': parent_id,
                'slice_method': 'recursive_max_depth',
                'entropy': self._calculate_entropy(text)
            }]
        
        # è·å–å½“å‰å±‚çº§çš„é˜ˆå€¼
        if current_depth < len(config['size_thresholds']):
            current_threshold = config['size_thresholds'][current_depth]
        else:
            # å¦‚æœæ·±åº¦è¶…è¿‡é˜ˆå€¼åˆ—è¡¨é•¿åº¦ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªé˜ˆå€¼
            current_threshold = config['size_thresholds'][-1]
        
        min_size = config['min_slice_size']
        
        # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºç­‰äºå½“å‰é˜ˆå€¼ï¼Œç›´æ¥è¿”å›
        if len(text) <= current_threshold:
            slice_id = self._generate_slice_id(parent_id, current_depth, 0)
            return [{
                'content': text,
                'slice_id': slice_id,
                'slice_depth': current_depth,
                'parent_id': parent_id,
                'slice_method': 'recursive_final',
                'entropy': self._calculate_entropy(text)
            }]
        
        # ä¿¡æ¯ç†µæ£€æµ‹ï¼šè®¡ç®—æ–‡æœ¬çš„ä¿¡æ¯ç†µ
        entropy = self._calculate_entropy(text)
        
        # å¦‚æœä¿¡æ¯ç†µè¾ƒä½ï¼ˆå†…å®¹å•ä¸€ï¼‰ï¼Œç›´æ¥è¿”å›
        if entropy < 2.0 and len(text) <= current_threshold * 1.5:
            slice_id = self._generate_slice_id(parent_id, current_depth, 0)
            return [{
                'content': text,
                'slice_id': slice_id,
                'slice_depth': current_depth,
                'parent_id': parent_id,
                'slice_method': 'recursive_low_entropy',
                'entropy': entropy
            }]
        
        # åŸºäºä¿¡æ¯ç†µå’Œé€»è¾‘è¾¹ç•Œè¿›è¡Œåˆ†ç‰‡
        slices = []
        
        # 1. æŸ¥æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
        split_points = self._find_optimal_split_points(text, current_threshold, entropy)
        
        if not split_points:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ï¼Œå°è¯•å¼ºåˆ¶åˆ†å‰²
            split_points = self._force_split_by_size(text, current_threshold)
        
        # 2. åŸºäºåˆ†å‰²ç‚¹è¿›è¡Œåˆ†ç‰‡
        start_pos = 0
        for i, split_point in enumerate(split_points):
            end_pos = split_point
            segment = text[start_pos:end_pos]
            
            if len(segment) >= min_size:
                slice_id = self._generate_slice_id(parent_id, current_depth, i)
                
                # é€’å½’åˆ†ç‰‡å­ç‰‡æ®µ
                child_slices = self._recursive_slice(
                    text=segment,
                    metadata=metadata,
                    config=config,
                    current_depth=current_depth + 1,
                    parent_id=slice_id
                )
                
                slices.extend(child_slices)
            
            start_pos = end_pos
        
        # å¤„ç†å‰©ä½™æ–‡æœ¬
        if start_pos < len(text):
            remaining_text = text[start_pos:]
            if len(remaining_text) >= min_size:
                slice_id = self._generate_slice_id(parent_id, current_depth, len(split_points))
                
                child_slices = self._recursive_slice(
                    text=remaining_text,
                    metadata=metadata,
                    config=config,
                    current_depth=current_depth + 1,
                    parent_id=slice_id
                )
                
                slices.extend(child_slices)
        
        return slices
    
    def _generate_slice_id(self, parent_id: str, depth: int, index: int) -> str:
        """ç”Ÿæˆå±‚çº§ç¼–ç çš„åˆ‡ç‰‡ID"""
        if not parent_id:
            return f"{index + 1}"  # ç¬¬ä¸€å±‚ï¼š1, 2, 3...
        else:
            return f"{parent_id}.{index + 1}"  # å­å±‚ï¼š1.1, 1.2, 2.1...
    
    def parse_slice_id(self, slice_id: str) -> Dict[str, Any]:
        """
        è§£æå±‚çº§ç¼–ç ï¼Œæå–å±‚çº§ä¿¡æ¯
        
        Args:
            slice_id: å±‚çº§ç¼–ç çš„åˆ‡ç‰‡ID
            
        Returns:
            åŒ…å«å±‚çº§ä¿¡æ¯çš„å­—å…¸
        """
        if not slice_id:
            return {'depth': 0, 'path': [], 'is_root': True}
        
        parts = slice_id.split('.')
        depth = len(parts)
        
        return {
            'depth': depth,
            'path': [int(part) for part in parts],
            'is_root': depth == 1,
            'parent_id': '.'.join(parts[:-1]) if depth > 1 else "",
            'level': parts[-1],
            'full_path': slice_id
        }
    
    def get_slice_hierarchy(self, slices: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºåˆ‡ç‰‡å±‚çº§ç»“æ„
        
        Args:
            slices: åˆ‡ç‰‡åˆ—è¡¨
            
        Returns:
            å±‚çº§ç»“æ„å­—å…¸
        """
        hierarchy = {}
        
        for slice_data in slices:
            slice_id = slice_data.get('slice_id', '')
            if not slice_id:
                continue
                
            parsed = self.parse_slice_id(slice_id)
            
            # æ„å»ºå±‚çº§è·¯å¾„
            current_level = hierarchy
            for level in parsed['path'][:-1]:
                if str(level) not in current_level:
                    current_level[str(level)] = {'children': {}, 'slices': []}
                current_level = current_level[str(level)]['children']
            
            # æ·»åŠ å½“å‰åˆ‡ç‰‡
            current_level_key = str(parsed['path'][-1])
            if current_level_key not in current_level:
                current_level[current_level_key] = {'children': {}, 'slices': []}
            
            current_level[current_level_key]['slices'].append(slice_data)
        
        return hierarchy
    
    def hierarchical_retrieval(self, query: str, slices: List[Dict[str, Any]], 
                              top_k: int = 10) -> List[Dict[str, Any]]:
        """
        åŸºäºå±‚çº§ç¼–ç çš„æ™ºèƒ½æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            slices: åˆ‡ç‰‡åˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ’åºåçš„æ£€ç´¢ç»“æœ
        """
        # 1. è®¡ç®—æ¯ä¸ªåˆ‡ç‰‡çš„ç›¸ä¼¼åº¦
        scored_slices = []
        
        for slice_data in slices:
            content = slice_data.get('content', '')
            
            # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå¯ä»¥æ›¿æ¢ä¸ºæ›´å¤æ‚çš„è¯­ä¹‰ç›¸ä¼¼åº¦ç®—æ³•ï¼‰
            similarity = self._calculate_text_similarity(query, content)
            
            # è€ƒè™‘å±‚çº§æ·±åº¦æƒé‡ï¼šæ·±å±‚åˆ‡ç‰‡æƒé‡æ›´é«˜
            slice_id = slice_data.get('slice_id', '')
            parsed = self.parse_slice_id(slice_id)
            depth_weight = 1.0 + (parsed['depth'] * 0.1)  # æ¯å±‚å¢åŠ 10%æƒé‡
            
            # è€ƒè™‘è¯­ä¹‰è´¨é‡æƒé‡
            quality_weight = slice_data.get('semantic_quality', 0.5)
            
            # ç»¼åˆå¾—åˆ†
            final_score = similarity * depth_weight * quality_weight
            
            scored_slices.append({
                'slice_data': slice_data,
                'similarity': similarity,
                'depth_weight': depth_weight,
                'quality_weight': quality_weight,
                'final_score': final_score
            })
        
        # 2. æŒ‰å¾—åˆ†æ’åº
        scored_slices.sort(key=lambda x: x['final_score'], reverse=True)
        
        # 3. è¿”å›top_kç»“æœ
        return [item['slice_data'] for item in scored_slices[:top_k]]
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆç®€å•å®ç°ï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        # ç®€å•çš„è¯é¢‘ç›¸ä¼¼åº¦
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _find_optimal_split_points(self, text: str, threshold: int, entropy: float) -> List[int]:
        """åŸºäºä¿¡æ¯ç†µå’Œé€»è¾‘è¾¹ç•ŒæŸ¥æ‰¾æœ€ä½³åˆ†å‰²ç‚¹"""
        
        split_points = []
        
        # é€»è¾‘è¾¹ç•Œæ¨¡å¼
        boundary_patterns = [
            r'[ã€‚ï¼ï¼Ÿ!?]\s*\n',  # å¥å­ç»“æŸ+æ¢è¡Œ
            r'[ã€‚ï¼ï¼Ÿ!?]\s{2,}',  # å¥å­ç»“æŸ+å¤šä¸ªç©ºæ ¼
            r'\n\s*\n',         # ç©ºè¡Œ
            r'##\s+',            # æ ‡é¢˜æ ‡è®°
            r'\d+\.\s+',        # æ•°å­—æ ‡é¢˜
        ]
        
        # æŸ¥æ‰¾æ‰€æœ‰é€»è¾‘è¾¹ç•Œ
        boundaries = []
        for pattern in boundary_patterns:
            matches = list(re.finditer(pattern, text))
            for match in matches:
                boundaries.append(match.end())
        
        boundaries = sorted(set(boundaries))
        
        # æ»‘åŠ¨çª—å£åˆ†æä¿¡æ¯ç†µå˜åŒ–
        window_size = min(100, len(text) // 10)
        entropy_changes = []
        
        for i in range(window_size, len(text) - window_size, window_size // 2):
            prev_window = text[max(0, i - window_size):i]
            next_window = text[i:min(len(text), i + window_size)]
            
            prev_entropy = self._calculate_entropy(prev_window)
            next_entropy = self._calculate_entropy(next_window)
            
            entropy_change = abs(next_entropy - prev_entropy)
            entropy_changes.append((i, entropy_change))
        
        # é€‰æ‹©ä¿¡æ¯ç†µå˜åŒ–æœ€å¤§çš„ç‚¹ä½œä¸ºå€™é€‰åˆ†å‰²ç‚¹
        entropy_changes.sort(key=lambda x: x[1], reverse=True)
        candidate_points = [point for point, _ in entropy_changes[:5]]
        
        # åˆå¹¶é€»è¾‘è¾¹ç•Œå’Œç†µå˜åŒ–ç‚¹
        all_candidates = sorted(set(boundaries + candidate_points))
        
        # é€‰æ‹©æ»¡è¶³é˜ˆå€¼è¦æ±‚çš„åˆ†å‰²ç‚¹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        current_pos = 0
        for candidate in all_candidates:
            segment_length = candidate - current_pos
            
            # ğŸ”§ ä¿®å¤ï¼šæ”¾å®½é˜ˆå€¼èŒƒå›´ï¼Œä»Â±30%æ”¹ä¸ºÂ±50%ï¼Œé¿å…åˆç†åˆ†å‰²è¢«æ‹’ç»
            # åŸé€»è¾‘ï¼šthreshold * 0.7 ~ 1.3ï¼ˆ700-1300ï¼‰å¯¹2151å­—ç¬¦çš„æ–‡æœ¬ä¼šå¤±è´¥
            # æ–°é€»è¾‘ï¼šthreshold * 0.5 ~ 2.0ï¼ˆ500-2000ï¼‰æ›´åˆç†
            min_acceptable = threshold * 0.5  # æœ€å°50%
            max_acceptable = threshold * 2.0  # æœ€å¤§200%
            
            if segment_length >= min_acceptable and segment_length <= max_acceptable:
                split_points.append(candidate)
                current_pos = candidate
            elif segment_length > max_acceptable:
                # å¦‚æœæ®µé•¿ä»ç„¶è¿‡å¤§ï¼Œæ™ºèƒ½æ·»åŠ ä¸­é—´ç‚¹
                # ä¼˜å…ˆé€‰æ‹©é€»è¾‘è¾¹ç•Œï¼Œå¦‚æœæ²¡æœ‰åˆ™å‡åˆ†
                mid_boundary = self._find_nearest_boundary(text, current_pos, candidate)
                if mid_boundary:
                    split_points.append(mid_boundary)
                    current_pos = mid_boundary
                else:
                    mid_point = current_pos + segment_length // 2
                    split_points.append(mid_point)
                    current_pos = mid_point
        
        return split_points
    
    def _find_nearest_boundary(self, text: str, start: int, end: int) -> Optional[int]:
        """åœ¨æŒ‡å®šèŒƒå›´å†…æŸ¥æ‰¾æœ€è¿‘çš„é€»è¾‘è¾¹ç•Œ"""
        middle = (start + end) // 2
        search_range = (end - start) // 4  # åœ¨ä¸­ç‚¹Â±25%èŒƒå›´å†…æŸ¥æ‰¾
        
        search_start = max(start, middle - search_range)
        search_end = min(end, middle + search_range)
        
        # åœ¨ä¸­ç‚¹é™„è¿‘æŸ¥æ‰¾é€»è¾‘è¾¹ç•Œ
        boundary_patterns = [
            r'[ã€‚ï¼ï¼Ÿ!?]\s*\n',  # å¥å­ç»“æŸ+æ¢è¡Œï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            r'\n\s*\n',         # ç©ºè¡Œ
            r'[ã€‚ï¼ï¼Ÿ!?]',       # å¥å­ç»“æŸ
        ]
        
        best_boundary = None
        min_distance = float('inf')
        
        for pattern in boundary_patterns:
            for match in re.finditer(pattern, text[search_start:search_end]):
                boundary_pos = search_start + match.end()
                distance = abs(boundary_pos - middle)
                
                if distance < min_distance:
                    min_distance = distance
                    best_boundary = boundary_pos
        
        return best_boundary
    
    def _force_split_by_size(self, text: str, threshold: int) -> List[int]:
        """åŸºäºå¤§å°å¼ºåˆ¶åˆ†å‰²"""
        split_points = []
        
        current_pos = 0
        while current_pos < len(text):
            next_pos = current_pos + threshold
            
            if next_pos >= len(text):
                break
            
            # å°è¯•åœ¨å¥å­è¾¹ç•Œé™„è¿‘åˆ†å‰²
            sentence_end = text.rfind('ã€‚', current_pos, next_pos)
            if sentence_end != -1 and sentence_end > current_pos + threshold * 0.5:
                split_points.append(sentence_end + 1)  # åŒ…æ‹¬å¥å·
                current_pos = sentence_end + 1
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥å­è¾¹ç•Œï¼Œåœ¨ç©ºæ ¼å¤„åˆ†å‰²
                space_pos = text.rfind(' ', current_pos, next_pos)
                if space_pos != -1 and space_pos > current_pos + threshold * 0.5:
                    split_points.append(space_pos + 1)
                    current_pos = space_pos + 1
                else:
                    # å¼ºåˆ¶åœ¨é˜ˆå€¼ä½ç½®åˆ†å‰²
                    split_points.append(next_pos)
                    current_pos = next_pos
        
        return split_points
    
    def _split_large_segment(self, segment: str, max_size: int) -> List[str]:
        """åˆ†å‰²è¿‡å¤§çš„æ–‡æœ¬æ®µ"""
        
        if len(segment) <= max_size:
            return [segment]
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
        sentence_pattern = r'[ã€‚ï¼ï¼Ÿ!?]\s*'
        sentences = re.split(sentence_pattern, segment)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            # æ·»åŠ æ ‡ç‚¹ç¬¦å·
            sentence_with_punct = sentence + "ã€‚"
            
            if len(current_chunk) + len(sentence_with_punct) <= max_size:
                current_chunk += sentence_with_punct
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence_with_punct
        
        if current_chunk:
            chunks.append(current_chunk)
        
        # å¦‚æœä»ç„¶è¿‡å¤§ï¼Œå¼ºåˆ¶åˆ†å‰²
        if len(chunks) == 0 or any(len(chunk) > max_size * 1.5 for chunk in chunks):
            chunks = [segment[i:i+max_size] for i in range(0, len(segment), max_size)]
        
        return chunks
    
    def _optimize_slices_by_entropy(self, slices: List[Dict[str, Any]], 
                                  config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºä¿¡æ¯ç†µä¼˜åŒ–åˆ†ç‰‡"""
        
        if not config['enable_entropy_analysis']:
            return slices
        
        optimized_slices = []
        
        for slice_data in slices:
            content = slice_data['content']
            
            # è®¡ç®—ä¿¡æ¯ç†µ
            entropy = self._calculate_entropy(content)
            
            # åŸºäºä¿¡æ¯ç†µå†³å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if entropy > 4.0 and len(content) > config['max_slice_size'] * 0.8:
                # é«˜ä¿¡æ¯ç†µä¸”é•¿åº¦æ¥è¿‘ä¸Šé™ï¼Œè€ƒè™‘åˆ†å‰²
                sub_slices = self._split_by_entropy(content, config)
                
                for i, sub_content in enumerate(sub_slices):
                    sub_slice_data = slice_data.copy()
                    sub_slice_data['content'] = sub_content
                    sub_slice_data['entropy'] = self._calculate_entropy(sub_content)
                    sub_slice_data['slice_method'] = 'entropy_optimized'
                    sub_slice_data['slice_level'] = 'optimized'
                    optimized_slices.append(sub_slice_data)
            else:
                slice_data['entropy'] = entropy
                slice_data['slice_method'] = 'logic_boundary_only'
                optimized_slices.append(slice_data)
        
        return optimized_slices
    
    def _calculate_entropy(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬ä¿¡æ¯ç†µ H(X) = -âˆ‘ p(x) * logâ‚‚ p(x)"""
        
        if not text:
            return 0.0
        
        # è®¡ç®—å­—ç¬¦é¢‘ç‡
        char_freq = {}
        total_chars = len(text)
        
        for char in text:
            char_freq[char] = char_freq.get(char, 0) + 1
        
        # è®¡ç®—ç†µ
        entropy = 0.0
        for count in char_freq.values():
            probability = count / total_chars
            entropy -= probability * math.log2(probability)
        
        return entropy
    
    def _calculate_perplexity(self, text: str, n: int = 2) -> float:
        """è®¡ç®—æ–‡æœ¬å›°æƒ‘åº¦ï¼ˆåŸºäºn-gramçš„ç®€åŒ–å®ç°ï¼Œæ— éœ€LLMï¼‰
        
        å›°æƒ‘åº¦å…¬å¼ï¼šPerplexity = 2^H(X)
        è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„n-gramæ¨¡å‹ä¼°ç®—
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            n: n-gramçš„nå€¼ï¼ˆé»˜è®¤2-gramï¼‰
            
        Returns:
            å›°æƒ‘åº¦å€¼
        """
        if not text or len(text) < n:
            return 0.0
        
        try:
            # æ„å»ºn-gram
            ngrams = []
            for i in range(len(text) - n + 1):
                ngram = text[i:i+n]
                ngrams.append(ngram)
            
            if not ngrams:
                return 0.0
            
            # è®¡ç®—n-gramé¢‘ç‡
            ngram_freq = Counter(ngrams)
            total_ngrams = len(ngrams)
            
            # è®¡ç®—äº¤å‰ç†µ
            cross_entropy = 0.0
            for count in ngram_freq.values():
                probability = count / total_ngrams
                if probability > 0:
                    cross_entropy -= probability * math.log2(probability)
            
            # å›°æƒ‘åº¦ = 2^(äº¤å‰ç†µ)
            perplexity = math.pow(2, cross_entropy)
            
            return perplexity
            
        except Exception as e:
            logger.error(f"è®¡ç®—å›°æƒ‘åº¦å¤±è´¥: {e}")
            return 0.0
    
    def _find_perplexity_boundaries(self, text: str, config: Dict[str, Any]) -> List[int]:
        """åŸºäºå›°æƒ‘åº¦å˜åŒ–æŸ¥æ‰¾åˆ†å‰²è¾¹ç•Œ
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            config: é…ç½®å‚æ•°
            
        Returns:
            åˆ†å‰²ç‚¹ä½ç½®åˆ—è¡¨
        """
        if len(text) < 200:
            return []
        
        # æ»‘åŠ¨çª—å£å¤§å°
        window_size = min(100, len(text) // 10)
        step_size = window_size // 2
        
        # è®¡ç®—æ¯ä¸ªçª—å£çš„å›°æƒ‘åº¦
        perplexity_values = []
        
        for i in range(0, len(text) - window_size + 1, step_size):
            window_text = text[i:i+window_size]
            perplexity = self._calculate_perplexity(window_text)
            perplexity_values.append((i, perplexity))
        
        if len(perplexity_values) < 3:
            return []
        
        # æŸ¥æ‰¾å›°æƒ‘åº¦å˜åŒ–å¤§çš„ä½ç½®
        split_points = []
        threshold = config['size_thresholds'][0] if config['size_thresholds'] else 1000
        
        for i in range(1, len(perplexity_values) - 1):
            prev_perplexity = perplexity_values[i-1][1]
            curr_perplexity = perplexity_values[i][1]
            next_perplexity = perplexity_values[i+1][1]
            
            # è®¡ç®—å›°æƒ‘åº¦å˜åŒ–ç‡
            perplexity_change = abs(curr_perplexity - prev_perplexity) + abs(next_perplexity - curr_perplexity)
            
            # å¦‚æœå˜åŒ–ç‡è¾ƒå¤§ï¼Œä¸”æ»¡è¶³é•¿åº¦è¦æ±‚ï¼Œæ·»åŠ åˆ†å‰²ç‚¹
            if perplexity_change > 5.0:  # å›°æƒ‘åº¦å˜åŒ–é˜ˆå€¼
                position = perplexity_values[i][0]
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°é—´éš”
                if not split_points or position - split_points[-1] >= threshold * 0.5:
                    split_points.append(position)
        
        return split_points
    
    def _finalize_slices(self, slices: List[Dict[str, Any]], 
                        config: Dict[str, Any], 
                        attempt_log: Dict) -> List[Dict[str, Any]]:
        """å®Œæˆåˆ‡ç‰‡åå¤„ç†ï¼šè´¨é‡è¯„ä¼°ã€è¿‡æ»¤ã€æ·»åŠ å…ƒä¿¡æ¯
        
        Args:
            slices: åŸå§‹åˆ‡ç‰‡åˆ—è¡¨
            config: é…ç½®å‚æ•°
            attempt_log: å°è¯•æ—¥å¿—
            
        Returns:
            å¤„ç†åçš„åˆ‡ç‰‡åˆ—è¡¨
        """
        # è¯­ä¹‰è´¨é‡è¯„ä¼°
        if config['enable_semantic_evaluation']:
            evaluated_slices = self._evaluate_semantic_quality(slices)
            logger.info("è¯­ä¹‰è´¨é‡è¯„ä¼°å®Œæˆ")
        else:
            evaluated_slices = slices
        
        # è´¨é‡è¿‡æ»¤
        if config['quality_threshold'] > 0:
            filtered_slices = [
                slice_data for slice_data in evaluated_slices 
                if slice_data.get('semantic_quality', 0) >= config['quality_threshold']
            ]
            logger.info(f"è´¨é‡è¿‡æ»¤: {len(evaluated_slices)} -> {len(filtered_slices)} ä¸ªåˆ‡ç‰‡")
            # è‹¥å…¨éƒ¨è¢«è¿‡æ»¤ï¼Œæ”¾å®½ä¸ºè¿”å›æœªè¿‡æ»¤çš„è¯„ä¼°åˆ‡ç‰‡ï¼ˆé¿å…åˆ‡ç‰‡ç»“æœä¸ºç©ºå¯¼è‡´æ—©åœï¼‰
            final_slices = filtered_slices if filtered_slices else evaluated_slices
            relaxed = filtered_slices == []
        else:
            final_slices = evaluated_slices
            relaxed = False
        
        # æ·»åŠ å·¥å…·ä¿¡æ¯å’Œé‡è¦æ€§è¯„ä¼°
        for i, slice_data in enumerate(final_slices):
            slice_data['slicer_tool'] = 'memory_slicer'
            slice_data['slice_timestamp'] = datetime.now().isoformat()
            slice_data['slice_config'] = {
                'method': attempt_log.get('final_method', 'unknown'),
                'attempts': len(attempt_log.get('attempts', [])),
                'thresholds': config['size_thresholds'],
                'quality_filter_relaxed': relaxed
            }
            
            # å¦‚æœåˆ‡ç‰‡ç¼ºå°‘importanceå­—æ®µï¼Œåˆ™åŸºäºè¯­ä¹‰è´¨é‡è®¡ç®—é‡è¦æ€§
            if 'importance' not in slice_data:
                semantic_quality = slice_data.get('semantic_quality', 0.5)
                content = slice_data.get('content', '')
                importance = self._calculate_slice_importance(content, semantic_quality)
                slice_data['importance'] = importance
        
        logger.info(f"è®°å¿†åˆ‡ç‰‡å®Œæˆï¼Œå…±ç”Ÿæˆ {len(final_slices)} ä¸ªé«˜è´¨é‡åˆ‡ç‰‡")
        return final_slices
    
    def _log_failure_to_bubble(self, attempt_log: Dict):
        """è®°å½•åˆ†ç‰‡å¤±è´¥ä¿¡æ¯åˆ°æ³¡æ³¡
        
        Args:
            attempt_log: å°è¯•æ—¥å¿—ï¼ŒåŒ…å«æ–‡ä»¶åã€å°è¯•æ–¹æ³•ã€å¤±è´¥åŸå› ç­‰
        """
        if not self.bubble_manager or not self.default_config['enable_bubble_logging']:
            return
        
        try:
            # æ„å»ºæ³¡æ³¡å†…å®¹
            bubble_content = f"""
åˆ†ç‰‡å¤±è´¥è®°å½•

æ–‡ä»¶å: {attempt_log.get('source_file', 'unknown')}
æ–‡æœ¬é•¿åº¦: {attempt_log.get('text_length', 0)} å­—ç¬¦

å°è¯•çš„æ–¹æ³•ï¼š
{chr(10).join(f'- {method}' for method in attempt_log.get('attempts', []))}

æœ€ç»ˆä½¿ç”¨æ–¹æ³•: {attempt_log.get('final_method', 'æœªçŸ¥')}
æ˜¯å¦æˆåŠŸ: {'\u6210\u529f' if attempt_log.get('success') else '\u5931\u8d25'}

ä¼˜åŒ–å»ºè®®ï¼š
1. æ£€æŸ¥æ–‡æœ¬ç»“æ„æ˜¯å¦å¼‚å¸¸ï¼ˆå¦‚è¿‡é•¿ã€ç¼ºä¹é€»è¾‘è¾¹ç•Œç­‰ï¼‰
2. è€ƒè™‘è°ƒæ•´size_thresholdsé…ç½®
3. å¦‚æœå¤šLLMç²¾ç‚¼å¤±è´¥ï¼Œæ£€æŸ¥è®°å¿†é‡æ„å¼•æ“é…ç½®
4. å¦‚æœå›°æƒ‘åº¦åˆ†ç‰‡å¤±è´¥ï¼Œæ£€æŸ¥æ–‡æœ¬æ˜¯å¦å…·æœ‰æ˜æ˜¾çš„ä¸»é¢˜è½¬æ¢
"""
            
            if 'error' in attempt_log:
                bubble_content += f"\n\né”™è¯¯ä¿¡æ¯: {attempt_log['error']}"
            
            # è®°å½•åˆ°æ³¡æ³¡
            bubble_id = self.bubble_manager.quick_note(
                category="åˆ†ç‰‡é—®é¢˜",
                content=bubble_content,
                context={
                    'source_file': attempt_log.get('source_file'),
                    'text_length': attempt_log.get('text_length'),
                    'attempts': attempt_log.get('attempts'),
                    'final_method': attempt_log.get('final_method')
                },
                priority="high" if not attempt_log.get('success') else "normal"
            )
            
            logger.info(f"åˆ†ç‰‡å¤±è´¥ä¿¡æ¯å·²è®°å½•åˆ°æ³¡æ³¡: {bubble_id}")
            
        except Exception as e:
            logger.error(f"è®°å½•åˆ†ç‰‡å¤±è´¥ä¿¡æ¯åˆ°æ³¡æ³¡å¤±è´¥: {e}")
    
    def _split_by_entropy(self, text: str, config: Dict[str, Any]) -> List[str]:
        """åŸºäºä¿¡æ¯ç†µè¿›è¡Œåˆ†å‰²"""
        
        # åœ¨ä¿¡æ¯ç†µå˜åŒ–è¾ƒå¤§çš„ä½ç½®è¿›è¡Œåˆ†å‰²
        window_size = min(100, len(text) // 10)
        entropy_values = []
        
        # è®¡ç®—æ»‘åŠ¨çª—å£çš„ä¿¡æ¯ç†µ
        for i in range(0, len(text) - window_size + 1, window_size // 2):
            window_text = text[i:i+window_size]
            entropy = self._calculate_entropy(window_text)
            entropy_values.append((i, entropy))
        
        # æ‰¾åˆ°ç†µå€¼å˜åŒ–è¾ƒå¤§çš„ä½ç½®ä½œä¸ºåˆ†å‰²ç‚¹
        split_points = [0]
        for i in range(1, len(entropy_values) - 1):
            prev_entropy = entropy_values[i-1][1]
            curr_entropy = entropy_values[i][1]
            next_entropy = entropy_values[i+1][1]
            
            # è®¡ç®—ç†µå€¼å˜åŒ–ç‡
            entropy_change = abs(curr_entropy - prev_entropy) + abs(next_entropy - curr_entropy)
            
            if entropy_change > 1.0:  # å˜åŒ–é˜ˆå€¼
                split_points.append(entropy_values[i][0])
        
        split_points.append(len(text))
        
        # åŸºäºåˆ†å‰²ç‚¹è¿›è¡Œåˆ†å‰²
        chunks = []
        for i in range(len(split_points) - 1):
            start = split_points[i]
            end = split_points[i + 1]
            chunk = text[start:end]
            if len(chunk) >= config['min_slice_size']:
                chunks.append(chunk)
        
        return chunks
    
    def _evaluate_semantic_quality(self, slices: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯„ä¼°åˆ‡ç‰‡è¯­ä¹‰è´¨é‡"""
        
        evaluated_slices = []
        
        for slice_data in slices:
            content = slice_data['content']
            
            # åŸºäºå¤šä¸ªæŒ‡æ ‡è¯„ä¼°è¯­ä¹‰è´¨é‡
            quality_score = self._calculate_semantic_quality(content)
            
            slice_data['semantic_quality'] = quality_score
            evaluated_slices.append(slice_data)
        
        return evaluated_slices
    
    def _calculate_semantic_quality(self, content: str) -> float:
        """è®¡ç®—è¯­ä¹‰è´¨é‡åˆ†æ•°"""
        
        if not content:
            return 0.0
        
        # 1. ä¿¡æ¯ç†µå› å­
        entropy = self._calculate_entropy(content)
        entropy_factor = min(1.0, entropy / 6.0)  # å‡è®¾æœ€å¤§ç†µä¸º6
        
        # 2. é•¿åº¦å› å­
        length = len(content)
        if 200 <= length <= 1500:
            length_factor = 0.9
        elif length > 1500:
            length_factor = 0.7
        else:
            length_factor = 0.5
        
        # 3. ç»“æ„å®Œæ•´æ€§å› å­
        has_complete_sentences = any(marker in content for marker in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?'])
        structure_factor = 0.8 if has_complete_sentences else 0.4
        
        # 4. è¯æ±‡å¤šæ ·æ€§å› å­
        unique_words = len(set(re.findall(r'[\w\u4e00-\u9fff]+', content)))
        total_words = len(re.findall(r'[\w\u4e00-\u9fff]+', content))
        diversity_factor = unique_words / max(1, total_words)
        
        # ç»¼åˆè´¨é‡åˆ†æ•°
        quality = (entropy_factor * 0.3 + length_factor * 0.25 + 
                  structure_factor * 0.25 + diversity_factor * 0.2)
        
        return min(1.0, max(0.0, quality))
    
    def _calculate_slice_importance(self, content: str, semantic_quality: float) -> float:
        """è®¡ç®—åˆ‡ç‰‡é‡è¦æ€§"""
        
        # åŸºç¡€é‡è¦æ€§åŸºäºè¯­ä¹‰è´¨é‡
        base_importance = semantic_quality
        
        # é•¿åº¦å› å­ï¼šé€‚ä¸­é•¿åº¦çš„å†…å®¹æ›´é‡è¦
        content_length = len(content)
        if 100 <= content_length <= 2000:
            length_factor = 0.8
        elif content_length > 2000:
            # è¿‡é•¿å†…å®¹å¯èƒ½åŒ…å«å†—ä½™ä¿¡æ¯
            length_factor = 0.6
        else:
            # è¿‡çŸ­å†…å®¹ä¿¡æ¯é‡ä¸è¶³
            length_factor = 0.4
        
        # ä¿¡æ¯å¯†åº¦å› å­ï¼šåŸºäºç‹¬ç‰¹å­—ç¬¦æ¯”ä¾‹
        unique_chars = len(set(content))
        density_factor = min(1.0, unique_chars / max(1, content_length) * 2)
        
        # ç»“æ„å®Œæ•´æ€§å› å­ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„å¥å­ç»“æ„
        has_complete_structure = any(marker in content for marker in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?'])
        structure_factor = 0.9 if has_complete_structure else 0.6
        
        # ç»¼åˆé‡è¦æ€§è®¡ç®—
        importance = base_importance * 0.4 + length_factor * 0.2 + density_factor * 0.2 + structure_factor * 0.2
        
        return min(1.0, max(0.1, importance))
    
    def slice_file(self, file_path: str, config: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        å¯¹æ–‡ä»¶å†…å®¹è¿›è¡Œåˆ‡ç‰‡
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            config: åˆ‡ç‰‡é…ç½®
            
        Returns:
            åˆ‡ç‰‡ç»“æœåˆ—è¡¨
        """
        
        try:
            full_path = self.base_path / file_path
            
            if not full_path.exists():
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return []
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(full_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if not content.strip():
                logger.warning(f"æ–‡ä»¶å†…å®¹ä¸ºç©º: {file_path}")
                return []
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                'source_type': 'file',
                'source_path': file_path,
                'file_name': full_path.name,
                'file_size': len(content)
            }
            
            # è°ƒç”¨æ–‡æœ¬åˆ‡ç‰‡
            slices = self.slice_text(content, metadata, config)
            
            logger.info(f"æ–‡ä»¶åˆ‡ç‰‡å®Œæˆ: {file_path} -> {len(slices)} ä¸ªåˆ‡ç‰‡")
            return slices
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶åˆ‡ç‰‡å¤±è´¥ {file_path}: {e}")
            return []
    
    def batch_slice_files(self, file_pattern: str = "*.txt", 
                         config: Dict[str, Any] = None) -> Dict[str, List[Dict[str, Any]]]:
        """
        æ‰¹é‡åˆ‡ç‰‡å¤šä¸ªæ–‡ä»¶
        
        Args:
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            config: åˆ‡ç‰‡é…ç½®
            
        Returns:
            æ–‡ä»¶ååˆ°åˆ‡ç‰‡åˆ—è¡¨çš„æ˜ å°„
        """
        
        results = {}
        
        try:
            # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            matched_files = list(self.base_path.rglob(file_pattern))
            
            if not matched_files:
                logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {file_pattern}")
                return results
            
            logger.info(f"æ‰¾åˆ° {len(matched_files)} ä¸ªåŒ¹é…æ–‡ä»¶ï¼Œå¼€å§‹æ‰¹é‡åˆ‡ç‰‡")
            
            for file_path in matched_files:
                relative_path = str(file_path.relative_to(self.base_path))
                
                # åˆ‡ç‰‡å½“å‰æ–‡ä»¶
                slices = self.slice_file(relative_path, config)
                results[relative_path] = slices
                
                logger.info(f"æ–‡ä»¶åˆ‡ç‰‡å®Œæˆ: {relative_path} -> {len(slices)} ä¸ªåˆ‡ç‰‡")
            
            logger.info(f"æ‰¹é‡åˆ‡ç‰‡å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªæ–‡ä»¶")
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ‡ç‰‡å¤±è´¥: {e}")
            return {}

# å…¼å®¹æ€§æ¥å£ï¼Œä¿æŒä¸åŸæœ‰ä»£ç ä¸€è‡´
def create_memory_slicer() -> MemorySlicerTool:
    """åˆ›å»ºè®°å¿†åˆ‡ç‰‡å™¨å®ä¾‹"""
    return MemorySlicerTool()