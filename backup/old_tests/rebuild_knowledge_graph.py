#!/usr/bin/env python3
# @self-expose: {"id": "rebuild_knowledge_graph", "name": "Rebuild Knowledge Graph", "type": "component", "version": "1.0.0", "needs": {"deps": [], "resources": []}, "provides": {"capabilities": ["Rebuild Knowledge GraphåŠŸèƒ½"]}}
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†å›¾è°±é‡å»ºè„šæœ¬ - åŸºäºå…ˆè¿›è®°å¿†é”šç‚¹é€»è¾‘

å¼€å‘æç¤ºè¯æ¥æºï¼šè®°å¿†é”šç‚¹_åŠ¨æ€çŸ¥è¯†å›¾è°±ç”Ÿæˆè¿‡ç¨‹.md
æ ¸å¿ƒæ¶æ„ï¼šç½‘çŠ¶æ€ç»´å¼•æ“ + ä¸»é¢˜ç»´åº¦æ ‘å½¢ç»“æ„ + äº‹ä»¶ç»´åº¦ç¼–ç ç´¢å¼•
å…ˆè¿›ç‰¹æ€§ï¼šåŠ¨æ€ååº”æœºåˆ¶ã€é€»è¾‘é“¾å®Œæ•´æ€§ã€æ—¶é—´åºåˆ—ã€å› æœå…³ç³»

âš ï¸ é‡è¦æé†’ï¼šæ­¤è„šæœ¬åŸºäºå…ˆè¿›é€»è¾‘ï¼Œé¿å…ä½¿ç”¨è½åçš„ç®€å•èšç±»æ–¹æ³•
"""

import json
import sqlite3
import os
from datetime import datetime
from typing import Dict, List, Any, Set, Tuple
import hashlib
import re
from collections import defaultdict

class AdvancedKnowledgeGraphRebuilder:
    """å…ˆè¿›çŸ¥è¯†å›¾è°±é‡å»ºå™¨ - åŸºäºè®°å¿†é”šç‚¹å…ˆè¿›é€»è¾‘"""
    
    def __init__(self, db_path: str = 'data/rag_memory.db', 
                 graph_path: str = 'data/advanced_hierarchical_knowledge_graph.json'):
        self.db_path = db_path
        self.graph_path = graph_path
        
    def get_all_memories(self) -> List[Dict]:
        """è·å–æ‰€æœ‰è®°å¿†æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è®°å¿†æ•°æ®
        cursor.execute('SELECT COUNT(*) FROM memory_units')
        count = cursor.fetchone()[0]
        
        if count == 0:
            print("âš ï¸ å‘é‡åº“ä¸ºç©ºï¼Œéœ€è¦å…ˆæ”¶é›†æ•°æ®")
            return []
        
        # è·å–æ‰€æœ‰è®°å¿†
        cursor.execute('SELECT * FROM memory_units ORDER BY timestamp DESC')
        columns = [desc[0] for desc in cursor.description]
        memories = []
        
        for row in cursor.fetchall():
            memory = dict(zip(columns, row))
            memories.append(memory)
        
        conn.close()
        print(f"âœ… è·å–åˆ° {len(memories)} ä¸ªè®°å¿†å•å…ƒ")
        return memories
    
    def build_mesh_thought_engine(self, memories: List[Dict]) -> Dict:
        """æ„å»ºç½‘çŠ¶æ€ç»´å¼•æ“ï¼ˆåŒ…å«äº‹ä»¶ç»´åº¦åˆ‡ç‰‡ï¼‰"""
        print("ğŸ§  æ„å»ºç½‘çŠ¶æ€ç»´å¼•æ“...")
        
        # 1. ä¸»é¢˜ç»´åº¦æ ‘å½¢ç»“æ„æ„å»º
        topic_hierarchy = self._build_topic_hierarchy(memories)
        
        # 2. äº‹ä»¶ç»´åº¦ç¼–ç ç´¢å¼•
        event_dimensions = self._encode_event_dimensions(memories)
        
        # 3. é€»è¾‘é“¾å®Œæ•´æ€§åˆ†æï¼ˆåŒ…å«äº‹ä»¶ç»´åº¦åˆ‡ç‰‡ï¼‰
        logic_chains = self._analyze_logic_chains(memories)
        
        # 4. æ—¶é—´åºåˆ—å…³è”
        temporal_relations = self._build_temporal_relations(memories)
        
        # 5. å› æœå…³ç³»æŒ–æ˜
        causal_relations = self._extract_causal_relations(memories)
        
        # ç»Ÿè®¡äº‹ä»¶ç»´åº¦åˆ‡ç‰‡ä¿¡æ¯
        event_slices = []
        event_slice_count = 0
        
        for chain in logic_chains:
            if chain.get('event_dimension', False):
                event_slice_count += 1
                if 'event_slices' in chain:
                    event_slices.extend(chain['event_slices'])
        
        mesh_engine = {
            'topic_hierarchy': topic_hierarchy,
            'event_dimensions': event_dimensions,
            'logic_chains': logic_chains,
            'temporal_relations': temporal_relations,
            'causal_relations': causal_relations,
            'event_slices': event_slices,
            'total_memories': len(memories),
            'event_slices_count': event_slice_count,
            'total_event_slices': len(event_slices),
            'event_dimension_analysis': {
                'large_text_blocks': sum(1 for mem in memories if len(mem.get('content', '')) > 500),
                'sliced_blocks': event_slice_count,
                'slice_types': list(set(slice.get('slice_type', '') for slice in event_slices))
            }
        }
        
        print(f"âœ… ç½‘çŠ¶æ€ç»´å¼•æ“æ„å»ºå®Œæˆ: {len(logic_chains)} æ¡é€»è¾‘é“¾, {len(temporal_relations)} ä¸ªæ—¶é—´å…³ç³»")
        print(f"ğŸ“Š äº‹ä»¶ç»´åº¦åˆ‡ç‰‡: {event_slice_count} ä¸ªå¤§å‹æ–‡æœ¬å—è¢«åˆ‡ç‰‡, å…±ç”Ÿæˆ {len(event_slices)} ä¸ªäº‹ä»¶ç»´åº¦åˆ‡ç‰‡")
        return mesh_engine
    
    def _build_topic_hierarchy(self, memories: List[Dict]) -> Dict:
        """æ„å»ºä¸»é¢˜ç»´åº¦æ ‘å½¢ç»“æ„"""
        print("ğŸŒ³ æ„å»ºä¸»é¢˜ç»´åº¦æ ‘å½¢ç»“æ„...")
        
        # åŸºäºè®°å¿†ä¸»é¢˜æ„å»ºå±‚æ¬¡ç»“æ„
        topic_groups = defaultdict(list)
        for mem in memories:
            topic = mem.get('topic', 'æœªåˆ†ç±»')
            topic_groups[topic].append(mem)
        
        # æ„å»ºä¸»é¢˜å±‚æ¬¡ï¼ˆå…¨å±€-ä¸»é¢˜-å­ä¸»é¢˜ä¸‰çº§ç»“æ„ï¼‰
        hierarchy = {
            'global': {
                'name': 'å…¨å±€çŸ¥è¯†å›¾è°±',
                'children': {},
                'memories': memories,
                'coverage': len(memories)
            }
        }
        
        # ä¸»é¢˜å±‚
        for topic, topic_memories in topic_groups.items():
            # å­ä¸»é¢˜åˆ†æï¼ˆåŸºäºå†…å®¹å…³é”®è¯ï¼‰
            subtopics = self._extract_subtopics(topic_memories)
            
            hierarchy['global']['children'][topic] = {
                'name': topic,
                'children': subtopics,
                'memories': topic_memories,
                'coverage': len(topic_memories)
            }
        
        return hierarchy
    
    def _encode_event_dimensions(self, memories: List[Dict]) -> Dict:
        """äº‹ä»¶ç»´åº¦ç¼–ç ç´¢å¼•"""
        print("ğŸ“Š äº‹ä»¶ç»´åº¦ç¼–ç ç´¢å¼•...")
        
        event_dimensions = {
            'logic_chain_integrity': [],  # é€»è¾‘é“¾å®Œæ•´æ€§
            'time_sequence': [],          # æ—¶é—´åºåˆ—
            'causal_relationships': []   # å› æœå…³ç³»
        }
        
        # æŒ‰æ—¶é—´æ’åºè®°å¿†
        sorted_memories = sorted(memories, key=lambda x: x.get('timestamp', ''))
        
        # åˆ†æé€»è¾‘é“¾å®Œæ•´æ€§
        for i in range(len(sorted_memories) - 1):
            current_mem = sorted_memories[i]
            next_mem = sorted_memories[i + 1]
            
            # æ£€æŸ¥é€»è¾‘è¿ç»­æ€§
            logic_score = self._calculate_logic_continuity(current_mem, next_mem)
            if logic_score > 0.3:
                event_dimensions['logic_chain_integrity'].append({
                    'source': current_mem['id'],
                    'target': next_mem['id'],
                    'score': logic_score,
                    'type': 'logic_continuity'
                })
        
        # æ„å»ºæ—¶é—´åºåˆ—
        for i in range(len(sorted_memories) - 1):
            current_mem = sorted_memories[i]
            next_mem = sorted_memories[i + 1]
            
            event_dimensions['time_sequence'].append({
                'source': current_mem['id'],
                'target': next_mem['id'],
                'time_gap': self._calculate_time_gap(current_mem, next_mem),
                'sequence_type': 'temporal'
            })
        
        return event_dimensions
    
    def _analyze_logic_chains(self, memories: List[Dict]) -> List[Dict]:
        """åˆ†æé€»è¾‘é“¾å®Œæ•´æ€§ï¼ˆåŒ…å«äº‹ä»¶ç»´åº¦äºŒæ¬¡åˆ‡ç‰‡ï¼‰"""
        logic_chains = []
        
        # åŸºäºå†…å®¹ç›¸ä¼¼åº¦å’Œæ—¶é—´è¿ç»­æ€§æ„å»ºé€»è¾‘é“¾
        visited = set()
        
        for mem in memories:
            if mem['id'] in visited:
                continue
            
            # å¯¹å¤§å‹æ–‡æœ¬å—è¿›è¡Œäº‹ä»¶ç»´åº¦äºŒæ¬¡åˆ‡ç‰‡
            if len(mem.get('content', '')) > 500:  # è¶…è¿‡500å­—ç¬¦çš„æ–‡æœ¬å—éœ€è¦äºŒæ¬¡åˆ‡ç‰‡
                event_slices = self._slice_event_dimension(mem)
                if event_slices:
                    # ä¸ºæ¯ä¸ªåˆ‡ç‰‡åˆ›å»ºé€»è¾‘é“¾
                    for slice_data in event_slices:
                        logic_chains.append({
                            'chain_id': f"event_chain_{len(logic_chains)}",
                            'memories': [mem['id']],  # åŸå§‹è®°å¿†ID
                            'event_slices': [slice_data],
                            'length': 1,
                            'coherence_score': 0.8,
                            'event_dimension': True,
                            'slice_type': slice_data['slice_type'],
                            'sequence_order': slice_data['sequence_order']
                        })
                    visited.add(mem['id'])
                    continue
            
            # å¸¸è§„é€»è¾‘é“¾æ„å»º
            chain = [mem]
            visited.add(mem['id'])
            
            # å¯»æ‰¾é€»è¾‘ç›¸å…³çš„åç»­è®°å¿†
            current_mem = mem
            while True:
                next_mem = self._find_logical_successor(current_mem, memories, visited)
                if next_mem:
                    chain.append(next_mem)
                    visited.add(next_mem['id'])
                    current_mem = next_mem
                else:
                    break
            
            if len(chain) > 1:
                logic_chains.append({
                    'chain_id': f"logic_chain_{len(logic_chains)}",
                    'memories': [m['id'] for m in chain],
                    'length': len(chain),
                    'coherence_score': self._calculate_chain_coherence(chain),
                    'event_dimension': False
                })
        
        return logic_chains
    
    def _slice_event_dimension(self, memory: Dict) -> List[Dict]:
        """å¯¹å¤§å‹æ–‡æœ¬å—è¿›è¡Œäº‹ä»¶ç»´åº¦äºŒæ¬¡åˆ‡ç‰‡ï¼ˆæ”¯æŒé€’å½’åˆ†ç‰‡å±‚çº§ç¼–ç ï¼‰"""
        content = memory.get('content', '')
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯é€’å½’åˆ†ç‰‡çš„ç»“æœï¼ˆåŒ…å«slice_idå±‚çº§ç¼–ç ï¼‰
        if 'slice_id' in memory and '.' in memory.get('slice_id', ''):
            # å·²ç»æ˜¯é€’å½’åˆ†ç‰‡ç»“æœï¼Œç›´æ¥è¿”å›
            return [{
                'slice_id': memory['slice_id'],
                'slice_type': 'recursive_slice',
                'content': content,
                'sequence_order': self._parse_slice_order(memory['slice_id']),
                'keywords': [],
                'original_memory_id': memory['id'],
                'slice_length': len(content),
                'slice_depth': self._parse_slice_depth(memory['slice_id']),
                'hierarchical_path': memory['slice_id']
            }]
        
        if len(content) <= 500:
            return []
        
        slices = []
        
        # äº‹ä»¶ç»´åº¦åˆ‡ç‰‡æ¨¡å¼ï¼šåŸå› ã€è¿‡ç¨‹ã€ç»“æœã€æ·±åŒ–ç­‰
        event_patterns = {
            'cause': ['å› ä¸º', 'åŸå› ', 'ç”±äº', 'èƒŒæ™¯', 'èµ·å› '],
            'process': ['è¿‡ç¨‹', 'æ­¥éª¤', 'æ–¹æ³•', 'å®æ–½', 'è¿›è¡Œ'],
            'deepening': ['æ·±åŒ–', 'æ·±å…¥', 'è¿›ä¸€æ­¥', 'æ‰©å±•', 'å‘å±•'],
            'result': ['ç»“æœ', 'ç»“è®º', 'æ•ˆæœ', 'æˆæœ', 'å½±å“']
        }
        
        # åŸºäºå…³é”®è¯è¯†åˆ«äº‹ä»¶ç»´åº¦åˆ‡ç‰‡
        for slice_type, keywords in event_patterns.items():
            # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ®µè½
            paragraphs = content.split('ã€‚')
            relevant_paragraphs = []
            
            for para in paragraphs:
                if any(keyword in para for keyword in keywords):
                    relevant_paragraphs.append(para.strip())
            
            if relevant_paragraphs:
                slice_content = 'ã€‚'.join(relevant_paragraphs)
                if len(slice_content) > 50:  # ç¡®ä¿åˆ‡ç‰‡æœ‰è¶³å¤Ÿå†…å®¹
                    slices.append({
                        'slice_id': f"{memory['id']}_{slice_type}",
                        'slice_type': slice_type,
                        'content': slice_content,
                        'sequence_order': len(slices) + 1,
                        'keywords': keywords,
                        'original_memory_id': memory['id'],
                        'slice_length': len(slice_content)
                    })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„äº‹ä»¶ç»´åº¦ï¼ŒæŒ‰æ®µè½è¿›è¡Œæ™ºèƒ½åˆ‡ç‰‡
        if not slices and len(content) > 800:
            paragraphs = content.split('ã€‚')
            for i, para in enumerate(paragraphs):
                if len(para.strip()) > 100:  # åªå¤„ç†æœ‰å†…å®¹çš„æ®µè½
                    slice_type = 'paragraph'
                    if i == 0:
                        slice_type = 'introduction'
                    elif i == len(paragraphs) - 1:
                        slice_type = 'conclusion'
                    
                    slices.append({
                        'slice_id': f"{memory['id']}_{slice_type}_{i}",
                        'slice_type': slice_type,
                        'content': para.strip(),
                        'sequence_order': i + 1,
                        'keywords': [],
                        'original_memory_id': memory['id'],
                        'slice_length': len(para.strip())
                    })
        
        return slices
    
    def _parse_slice_order(self, slice_id: str) -> int:
        """è§£æåˆ‡ç‰‡IDè·å–é¡ºåºç¼–å·"""
        if not slice_id:
            return 1
        
        # å±‚çº§ç¼–ç æ ¼å¼ï¼š1.2.3 -> å–æœ€åä¸€éƒ¨åˆ†ä½œä¸ºé¡ºåº
        parts = slice_id.split('.')
        try:
            return int(parts[-1])
        except:
            return 1
    
    def _parse_slice_depth(self, slice_id: str) -> int:
        """è§£æåˆ‡ç‰‡IDè·å–æ·±åº¦"""
        if not slice_id:
            return 1
        
        parts = slice_id.split('.')
        return len(parts)
    
    def _build_temporal_relations(self, memories: List[Dict]) -> List[Dict]:
        """æ„å»ºæ—¶é—´åºåˆ—å…³è”"""
        temporal_relations = []
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_memories = sorted(memories, key=lambda x: x.get('timestamp', ''))
        
        for i in range(len(sorted_memories) - 1):
            source_mem = sorted_memories[i]
            target_mem = sorted_memories[i + 1]
            
            temporal_relations.append({
                'source': source_mem['id'],
                'target': target_mem['id'],
                'relation_type': 'temporal_sequence',
                'time_gap': self._calculate_time_gap(source_mem, target_mem),
                'strength': max(0.8 - (self._calculate_time_gap(source_mem, target_mem) / 86400) * 0.5, 0.3)
            })
        
        return temporal_relations
    
    def _extract_causal_relations(self, memories: List[Dict]) -> List[Dict]:
        """æå–å› æœå…³ç³»"""
        causal_relations = []
        
        # åŸºäºå› æœå…³é”®è¯å’Œé€»è¾‘åˆ†æ
        causal_keywords = ['å› ä¸º', 'æ‰€ä»¥', 'å¯¼è‡´', 'å¼•èµ·', 'ç»“æœ', 'åŸå› ', 'å› æ­¤', 'äºæ˜¯']
        
        for i, mem1 in enumerate(memories):
            content1 = mem1.get('content', '').lower()
            
            for j, mem2 in enumerate(memories):
                if i == j:
                    continue
                
                content2 = mem2.get('content', '').lower()
                
                # æ£€æŸ¥å› æœå…³é”®è¯
                causal_score = self._calculate_causal_score(content1, content2, causal_keywords)
                
                if causal_score > 0.4:
                    causal_relations.append({
                        'cause': mem1['id'],
                        'effect': mem2['id'],
                        'score': causal_score,
                        'evidence': 'keyword_analysis'
                    })
        
        return causal_relations
    
    # è¾…åŠ©æ–¹æ³•
    def _extract_subtopics(self, memories: List[Dict]) -> Dict:
        """æå–å­ä¸»é¢˜"""
        subtopics = {}
        
        # åŸºäºå†…å®¹å…³é”®è¯èšç±»
        keyword_patterns = {
            'æŠ€æœ¯å®ç°': ['ä»£ç ', 'å®ç°', 'å¼€å‘', 'ç¼–ç¨‹'],
            'é—®é¢˜åˆ†æ': ['é—®é¢˜', 'é”™è¯¯', 'è§£å†³', 'è°ƒè¯•'],
            'è®¾è®¡è®¨è®º': ['è®¾è®¡', 'æ¶æ„', 'æ–¹æ¡ˆ', 'è§„åˆ’'],
            'å­¦ä¹ ç ”ç©¶': ['å­¦ä¹ ', 'ç ”ç©¶', 'æ¢ç´¢', 'åˆ†æ']
        }
        
        for pattern_name, keywords in keyword_patterns.items():
            pattern_memories = []
            for mem in memories:
                content = mem.get('content', '').lower()
                if any(keyword in content for keyword in keywords):
                    pattern_memories.append(mem)
            
            if pattern_memories:
                subtopics[pattern_name] = {
                    'memories': pattern_memories,
                    'coverage': len(pattern_memories)
                }
        
        return subtopics
    
    def _calculate_logic_continuity(self, mem1: Dict, mem2: Dict) -> float:
        """è®¡ç®—é€»è¾‘è¿ç»­æ€§å¾—åˆ†"""
        content1 = mem1.get('content', '').lower()
        content2 = mem2.get('content', '').lower()
        
        # åŸºäºå…³é”®è¯é‡å å’Œè¯­ä¹‰è¿ç»­æ€§
        words1 = set(content1.split()[:20])
        words2 = set(content2.split()[:20])
        
        if not words1 or not words2:
            return 0.0
        
        similarity = len(words1.intersection(words2)) / len(words1.union(words2))
        
        # æ—¶é—´è¿ç»­æ€§åŠ æˆ
        time_gap = self._calculate_time_gap(mem1, mem2)
        time_bonus = max(0, 1.0 - time_gap / 3600)  # 1å°æ—¶å†…åŠ æˆ
        
        return similarity * 0.7 + time_bonus * 0.3
    
    def _calculate_time_gap(self, mem1: Dict, mem2: Dict) -> float:
        """è®¡ç®—æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰"""
        try:
            time1 = datetime.fromisoformat(mem1.get('timestamp', '').replace('Z', '+00:00'))
            time2 = datetime.fromisoformat(mem2.get('timestamp', '').replace('Z', '+00:00'))
            return abs((time2 - time1).total_seconds())
        except:
            return float('inf')
    
    def _find_logical_successor(self, current_mem: Dict, memories: List[Dict], visited: Set) -> Dict:
        """å¯»æ‰¾é€»è¾‘åç»§è®°å¿†"""
        best_successor = None
        best_score = 0.0
        
        for mem in memories:
            if mem['id'] in visited or mem['id'] == current_mem['id']:
                continue
            
            score = self._calculate_logic_continuity(current_mem, mem)
            if score > best_score and score > 0.4:
                best_score = score
                best_successor = mem
        
        return best_successor
    
    def _calculate_chain_coherence(self, chain: List[Dict]) -> float:
        """è®¡ç®—é€»è¾‘é“¾è¿è´¯æ€§å¾—åˆ†"""
        if len(chain) < 2:
            return 0.0
        
        total_score = 0.0
        for i in range(len(chain) - 1):
            total_score += self._calculate_logic_continuity(chain[i], chain[i+1])
        
        return total_score / (len(chain) - 1)
    
    def _calculate_causal_score(self, content1: str, content2: str, keywords: List[str]) -> float:
        """è®¡ç®—å› æœå…³ç³»å¾—åˆ†"""
        score = 0.0
        
        # æ£€æŸ¥å› æœå…³é”®è¯
        for keyword in keywords:
            if keyword in content1 and any(kw in content2 for kw in ['ç»“æœ', 'å¯¼è‡´', 'å¼•èµ·']):
                score += 0.3
            if keyword in content2 and any(kw in content1 for kw in ['å› ä¸º', 'åŸå› ', 'ç”±äº']):
                score += 0.3
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦åŠ æˆ
        words1 = set(content1.split()[:15])
        words2 = set(content2.split()[:15])
        
        if words1 and words2:
            similarity = len(words1.intersection(words2)) / len(words1.union(words2))
            score += similarity * 0.4
        
        return min(score, 1.0)
    
    def build_advanced_knowledge_graph(self, memories: List[Dict]) -> Dict:
        """æ„å»ºé«˜çº§çŸ¥è¯†å›¾è°±ï¼ˆæ”¯æŒé€’å½’åˆ†ç‰‡å±‚çº§ç¼–ç ï¼‰"""
        print("ğŸš€ æ„å»ºæ”¯æŒé€’å½’åˆ†ç‰‡å±‚çº§ç¼–ç çš„é«˜çº§çŸ¥è¯†å›¾è°±...")
        
        # åˆå§‹åŒ–å›¾è°±ç»“æ„
        knowledge_graph = {
            'global_layer': {'nodes': [], 'edges': []},
            'topic_layer': {'nodes': [], 'edges': []},
            'event_layer': {'nodes': [], 'edges': []},
            'hierarchical_layer': {'nodes': [], 'edges': []},  # æ–°å¢å±‚çº§ç¼–ç å±‚
            'metadata': {
                'total_nodes': 0,
                'total_edges': 0,
                'creation_time': datetime.now().isoformat(),
                'version': '2.1',  # ç‰ˆæœ¬æ›´æ–°
                'hierarchical_support': True
            }
        }
        
        # æ„å»ºä¸»é¢˜å±‚çº§
        topic_hierarchy = self._build_topic_hierarchy(memories)
        knowledge_graph['topic_layer']['nodes'] = topic_hierarchy
        
        # å¤„ç†æ¯ä¸ªè®°å¿†ï¼Œæ„å»ºäº‹ä»¶ç»´åº¦åˆ‡ç‰‡
        for memory in memories:
            # æ„å»ºå…¨å±€å±‚èŠ‚ç‚¹
            global_node = {
                'id': memory['id'],
                'type': 'memory',
                'content': memory.get('content', '')[:200],  # æˆªå–å‰200å­—ç¬¦
                'timestamp': memory.get('timestamp', ''),
                'importance': memory.get('importance', 0),
                'slice_count': 0,
                'hierarchical_depth': 1  # é»˜è®¤æ·±åº¦ä¸º1
            }
            knowledge_graph['global_layer']['nodes'].append(global_node)
            
            # äº‹ä»¶ç»´åº¦åˆ‡ç‰‡ï¼ˆæ”¯æŒé€’å½’åˆ†ç‰‡ï¼‰
            event_slices = self._slice_event_dimension(memory)
            memory['slice_count'] = len(event_slices)
            
            for slice_data in event_slices:
                # æ„å»ºäº‹ä»¶å±‚èŠ‚ç‚¹
                event_node = {
                    'id': slice_data['slice_id'],
                    'type': 'event_slice',
                    'slice_type': slice_data['slice_type'],
                    'content': slice_data['content'][:150],
                    'sequence_order': slice_data['sequence_order'],
                    'original_memory_id': memory['id'],
                    'keywords': slice_data.get('keywords', []),
                    'slice_length': slice_data['slice_length'],
                    'slice_depth': slice_data.get('slice_depth', 1),
                    'hierarchical_path': slice_data.get('hierarchical_path', '')
                }
                knowledge_graph['event_layer']['nodes'].append(event_node)
                
                # æ„å»ºå…¨å±€å±‚åˆ°äº‹ä»¶å±‚çš„å…³è”
                edge = {
                    'id': f"{memory['id']}_{slice_data['slice_id']}",
                    'source': memory['id'],
                    'target': slice_data['slice_id'],
                    'type': 'contains',
                    'weight': 1.0
                }
                knowledge_graph['global_layer']['edges'].append(edge)
                
                # å¦‚æœæ˜¯é€’å½’åˆ†ç‰‡ï¼Œæ„å»ºå±‚çº§ç¼–ç å±‚
                if 'hierarchical_path' in slice_data and slice_data['hierarchical_path']:
                    self._build_hierarchical_structure(knowledge_graph, slice_data, memory)
        
        # æ„å»ºæ—¶é—´åºåˆ—å…³è”
        temporal_relations = self._build_temporal_relations(memories)
        knowledge_graph['event_layer']['edges'].extend(temporal_relations)
        
        # æ„å»ºå› æœå…³è”
        causal_relations = self._extract_causal_relations(memories)
        knowledge_graph['event_layer']['edges'].extend(causal_relations)
        
        # æ„å»ºé€»è¾‘é“¾
        logic_chains = self._analyze_logic_chains(memories)
        knowledge_graph['event_layer']['edges'].extend(logic_chains)
        
        # æ›´æ–°å…ƒæ•°æ®
        knowledge_graph['metadata']['total_nodes'] = (
            len(knowledge_graph['global_layer']['nodes']) +
            len(knowledge_graph['topic_layer']['nodes']) +
            len(knowledge_graph['event_layer']['nodes']) +
            len(knowledge_graph['hierarchical_layer']['nodes'])
        )
        knowledge_graph['metadata']['total_edges'] = (
            len(knowledge_graph['global_layer']['edges']) +
            len(knowledge_graph['topic_layer']['edges']) +
            len(knowledge_graph['event_layer']['edges']) +
            len(knowledge_graph['hierarchical_layer']['edges'])
        )
        
        print(f"âœ… æ”¯æŒé€’å½’åˆ†ç‰‡å±‚çº§ç¼–ç çš„é«˜çº§çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼Œå…± {knowledge_graph['metadata']['total_nodes']} ä¸ªèŠ‚ç‚¹ï¼Œ"
              f"{knowledge_graph['metadata']['total_edges']} æ¡è¾¹")
        
        return knowledge_graph
    
    def _build_hierarchical_structure(self, knowledge_graph: Dict, slice_data: Dict, memory: Dict):
        """æ„å»ºé€’å½’åˆ†ç‰‡çš„å±‚çº§ç»“æ„"""
        hierarchical_path = slice_data['hierarchical_path']
        parts = hierarchical_path.split('.')
        
        # æ„å»ºå±‚çº§èŠ‚ç‚¹
        for i in range(1, len(parts)):
            parent_id = '.'.join(parts[:i])
            current_id = '.'.join(parts[:i+1])
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥å±‚çº§èŠ‚ç‚¹
            existing_nodes = [n for n in knowledge_graph['hierarchical_layer']['nodes'] 
                            if n['id'] == current_id]
            
            if not existing_nodes:
                # åˆ›å»ºå±‚çº§èŠ‚ç‚¹
                hierarchical_node = {
                    'id': current_id,
                    'type': 'hierarchical_slice',
                    'depth': i + 1,
                    'parent_id': parent_id if i > 0 else memory['id'],
                    'sequence_order': int(parts[i]),
                    'content_preview': slice_data['content'][:100] if i == len(parts) - 1 else '',
                    'original_memory_id': memory['id']
                }
                knowledge_graph['hierarchical_layer']['nodes'].append(hierarchical_node)
                
                # æ„å»ºå±‚çº§å…³è”è¾¹
                edge = {
                    'id': f"{parent_id}_{current_id}",
                    'source': parent_id,
                    'target': current_id,
                    'type': 'hierarchical_contains',
                    'weight': 1.0 - (i * 0.1)  # æ·±åº¦è¶Šæ·±ï¼Œæƒé‡è¶Šä½
                }
                knowledge_graph['hierarchical_layer']['edges'].append(edge)
    
    def save_advanced_knowledge_graph(self, graph_data: Dict):
        """ä¿å­˜æ”¯æŒé€’å½’åˆ†ç‰‡å±‚çº§ç¼–ç çš„é«˜çº§çŸ¥è¯†å›¾è°±"""
        print("ğŸ’¾ ä¿å­˜æ”¯æŒé€’å½’åˆ†ç‰‡å±‚çº§ç¼–ç çš„é«˜çº§çŸ¥è¯†å›¾è°±...")
        
        # åˆ›å»ºåˆ†å±‚ç»“æ„ï¼ˆæ”¯æŒå±‚çº§ç¼–ç ï¼‰
        hierarchical_graph = {
            'global_layer': {
                'layer': 'global',
                'name': 'å…¨å±€çŸ¥è¯†å›¾è°±',
                'nodes': graph_data['global_layer']['nodes'],
                'edges': graph_data['global_layer']['edges'],
                'metadata': {
                    'version': graph_data['metadata']['version'],
                    'hierarchical_support': graph_data['metadata']['hierarchical_support'],
                    'build_time': graph_data['metadata']['creation_time'],
                    'total_nodes': len(graph_data['global_layer']['nodes']),
                    'total_edges': len(graph_data['global_layer']['edges'])
                }
            },
            'topic_layer': {
                'layer': 'topic',
                'nodes': graph_data['topic_layer']['nodes'],
                'edges': graph_data['topic_layer']['edges'],
                'metadata': {
                    'total_nodes': len(graph_data['topic_layer']['nodes']),
                    'total_edges': len(graph_data['topic_layer']['edges']),
                    'build_time': graph_data['metadata']['creation_time']
                }
            },
            'event_layer': {
                'layer': 'event',
                'nodes': graph_data['event_layer']['nodes'],
                'edges': graph_data['event_layer']['edges'],
                'metadata': {
                    'total_nodes': len(graph_data['event_layer']['nodes']),
                    'total_edges': len(graph_data['event_layer']['edges']),
                    'build_time': graph_data['metadata']['creation_time']
                }
            },
            'hierarchical_layer': {
                'layer': 'hierarchical',
                'name': 'å±‚çº§ç¼–ç å›¾è°±',
                'nodes': graph_data['hierarchical_layer']['nodes'],
                'edges': graph_data['hierarchical_layer']['edges'],
                'metadata': {
                    'total_nodes': len(graph_data['hierarchical_layer']['nodes']),
                    'total_edges': len(graph_data['hierarchical_layer']['edges']),
                    'max_depth': max([n.get('depth', 1) for n in graph_data['hierarchical_layer']['nodes']]) if graph_data['hierarchical_layer']['nodes'] else 1,
                    'build_time': graph_data['metadata']['creation_time'],
                    'hierarchical_support': True
                }
            }
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.graph_path), exist_ok=True)
        
        with open(self.graph_path, 'w', encoding='utf-8') as f:
            json.dump(hierarchical_graph, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ”¯æŒé€’å½’åˆ†ç‰‡å±‚çº§ç¼–ç çš„é«˜çº§çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ°: {self.graph_path}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = graph_data['metadata']
        print(f"ğŸ“Š é€’å½’åˆ†ç‰‡å±‚çº§ç¼–ç é‡å»ºç»Ÿè®¡:")
        print(f"   æ€»èŠ‚ç‚¹æ•°: {stats['total_nodes']}")
        print(f"   æ€»è¾¹æ•°: {stats['total_edges']}")
        print(f"   å…¨å±€å±‚èŠ‚ç‚¹: {len(graph_data['global_layer']['nodes'])}")
        print(f"   ä¸»é¢˜å±‚èŠ‚ç‚¹: {len(graph_data['topic_layer']['nodes'])}")
        print(f"   äº‹ä»¶å±‚èŠ‚ç‚¹: {len(graph_data['event_layer']['nodes'])}")
        print(f"   å±‚çº§ç¼–ç å±‚èŠ‚ç‚¹: {len(graph_data['hierarchical_layer']['nodes'])}")
        print(f"   æœ€å¤§å±‚çº§æ·±åº¦: {max([n.get('slice_depth', 1) for n in graph_data['event_layer']['nodes']]) if graph_data['event_layer']['nodes'] else 1}")
        print(f"   å±‚çº§ç¼–ç æ”¯æŒ: {'å·²å¯ç”¨' if stats['hierarchical_support'] else 'æœªå¯ç”¨'}")
        print(f"   ç‰ˆæœ¬: {stats['version']}")
        print(f"   æ„å»ºæ—¶é—´: {stats['creation_time']}")
    
    def rebuild_advanced(self):
        """æ‰§è¡Œå…ˆè¿›é‡å»ºæµç¨‹ï¼ˆåŸºäºè®°å¿†é”šç‚¹å…ˆè¿›é€»è¾‘ï¼‰"""
        print("ğŸš€ å¼€å§‹å…ˆè¿›çŸ¥è¯†å›¾è°±é‡å»º...")
        print("âš ï¸  ä½¿ç”¨è®°å¿†é”šç‚¹å…ˆè¿›é€»è¾‘ï¼šç½‘çŠ¶æ€ç»´å¼•æ“+ä¸»é¢˜ç»´åº¦æ ‘å½¢ç»“æ„")
        
        # 1. è·å–æ‰€æœ‰è®°å¿†
        memories = self.get_all_memories()
        if not memories:
            print("âŒ æ²¡æœ‰è®°å¿†æ•°æ®ï¼Œæ— æ³•é‡å»ºçŸ¥è¯†å›¾è°±")
            return False
        
        # 2. æ„å»ºå…ˆè¿›çŸ¥è¯†å›¾è°±
        advanced_graph = self.build_advanced_knowledge_graph(memories)
        
        # 3. ä¿å­˜å…ˆè¿›çŸ¥è¯†å›¾è°±
        self.save_advanced_knowledge_graph(advanced_graph)
        
        print("ğŸ‰ å…ˆè¿›çŸ¥è¯†å›¾è°±é‡å»ºå®Œæˆï¼")
        return True

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨å…ˆè¿›é€»è¾‘é‡å»ºçŸ¥è¯†å›¾è°±"""
    rebuilder = AdvancedKnowledgeGraphRebuilder()
    
    print("=" * 60)
    print("ğŸ§  å…ˆè¿›çŸ¥è¯†å›¾è°±é‡å»ºå·¥å…·ï¼ˆåŸºäºè®°å¿†é”šç‚¹å…ˆè¿›é€»è¾‘ï¼‰")
    print("=" * 60)
    print("âš ï¸  é‡è¦æé†’ï¼šæ­¤å·¥å…·ä½¿ç”¨å…ˆè¿›æ¶æ„ï¼Œé¿å…è½åé€»è¾‘")
    print("ğŸ“š å‚è€ƒæ–‡æ¡£ï¼šè®°å¿†é”šç‚¹_åŠ¨æ€çŸ¥è¯†å›¾è°±ç”Ÿæˆè¿‡ç¨‹.md")
    print("-" * 60)
    
    success = rebuilder.rebuild_advanced()
    
    if success:
        print("\nâœ… å…ˆè¿›é‡å»ºæˆåŠŸï¼çŸ¥è¯†å›¾è°±å·²ä¸å‘é‡åº“åŒæ­¥")
        print("ğŸ’¡ å…ˆè¿›ç‰¹æ€§å·²å¯ç”¨ï¼šäº‹ä»¶ç»´åº¦ç¼–ç ç´¢å¼•ã€é€»è¾‘é“¾å®Œæ•´æ€§åˆ†æç­‰")
    else:
        print("\nâŒ é‡å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥å‘é‡åº“ä¸­æ˜¯å¦æœ‰æ•°æ®")

if __name__ == "__main__":
    main()