# @self-expose: {"id": "agent_tool_integration", "name": "Agent Tool Integration", "type": "component", "version": "1.8.1", "needs": {"deps": ["memory_reconstruction_engine", "mesh_thought_engine", "multimodal_alignment_engine", "multimodal_retrieval_engine", "multimodal_fusion_engine", "vision_processing_engine", "audio_processing_engine", "abductive_reasoning_engine", "cognitive_barrier_break_engine", "reasoning_engine"], "resources": []}, "provides": {"capabilities": ["Agent Tool IntegrationåŠŸèƒ½", "æ¯”è¾ƒå›ç­”å¢å¼ºRAG"]}}
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ä½“å·¥å…·é›†æˆæ¨¡å—
å®ç°æ™ºèƒ½ä½“ä¸RAGç³»ç»Ÿç°æœ‰å·¥å…·çš„é›†æˆè°ƒç”¨
"""

import os
import sys
import importlib
import json
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename='logs/tool_calls.log',
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)
logger = logging.getLogger(__name__)

# æ·»åŠ RAGç³»ç»Ÿè·¯å¾„
rag_system_path = Path("E:\\RAGç³»ç»Ÿ")
sys.path.insert(0, str(rag_system_path))
sys.path.insert(0, str(rag_system_path / "src"))

# ğŸ”¥ å…¨å±€å•ä¾‹å®ä¾‹
_global_tool_integrator = None

def get_tool_integrator() -> 'AgentToolIntegration':
    """è·å–å…¨å±€å·¥å…·é›†æˆå™¨å•ä¾‹"""
    global _global_tool_integrator
    if _global_tool_integrator is None:
        _global_tool_integrator = AgentToolIntegration()
    return _global_tool_integrator

class AgentToolIntegration:
    """æ™ºèƒ½ä½“å·¥å…·é›†æˆå™¨ï¼ˆæ”¯æŒæ‡’åŠ è½½ï¼‰"""
    
    def __init__(self):
        self.tool_instances = {}  # å·²åˆå§‹åŒ–çš„å·¥å…·å®ä¾‹
        self.chat_tool_manager = None
        self._advanced_tools_config = {}  # é«˜çº§å·¥å…·é…ç½®ï¼ˆæ‡’åŠ è½½ï¼‰
        self._initialize_basic_tools()  # ğŸ”¥ åªåˆå§‹åŒ–åŸºç¡€å·¥å…·
    
    def _initialize_basic_tools(self):
        """åˆå§‹åŒ–åŸºç¡€å·¥å…·ï¼ˆç³»ç»Ÿå¯åŠ¨æ—¶åŠ è½½ï¼‰"""
        # åŸºç¡€å·¥å…·ï¼šèŠå¤©å·¥å…·ç®¡ç†å™¨
        try:
            from tools.chat_tools import create_tool_manager
            self.chat_tool_manager = create_tool_manager()
            logger.debug("èŠå¤©å·¥å…·ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            logger.warning(f"æ— æ³•åˆå§‹åŒ–èŠå¤©å·¥å…·ç®¡ç†å™¨: {e}")
        
        # æ³¨å†Œæ€ç»´é€æ˜åŒ–è¿½è¸ªå·¥å…·ï¼ˆåŸºç¡€å·¥å…·ï¼‰
        try:
            from src.thinking_tracer_tool import register_thinking_tracer_tool, get_thinking_tracer
            register_thinking_tracer_tool(self)
            self.tool_instances['thinking_tracer'] = get_thinking_tracer()
            logger.debug("æ€ç»´é€æ˜åŒ–è¿½è¸ªå·¥å…·(thinking_tracer)æ³¨å†ŒæˆåŠŸ")
        except ImportError as e:
            logger.debug(f"æ€ç»´é€æ˜åŒ–è¿½è¸ªå·¥å…·æœªåŠ è½½: {e}")
        
        # æ³¨å†Œå½’çº³å¼•æ“å·¥å…·ï¼ˆåŸºç¡€å·¥å…·ï¼‰
        try:
            from tools import induction_engine
            self.tool_instances['InductionEngine'] = induction_engine
            logger.debug("å½’çº³å¼•æ“å·¥å…·(InductionEngine)æ³¨å†ŒæˆåŠŸ")
        except ImportError as e:
            logger.debug(f"å½’çº³å¼•æ“å·¥å…·æœªåŠ è½½: {e}")
        
        # ğŸ”¥ ç³»ç»Ÿæ ¸å¿ƒè®¤çŸ¥å¼•æ“ï¼ˆé«˜é¢‘ä½¿ç”¨ï¼Œå¯åŠ¨æ—¶å…¨é‡åŠ è½½ï¼‰
        # è¿™3ä¸ªå¼•æ“æ˜¯ç³»ç»Ÿæ ¸å¿ƒä¾èµ–ï¼Œè¢«è®°å¿†é‡æ„ã€æ–‡ä»¶ä¸Šä¼ ã€ç»Ÿè®¡æœåŠ¡ç­‰å¤šå¤„ä½¿ç”¨
        try:
            from src.mesh_thought_engine import MeshThoughtEngine
            self.tool_instances['MeshThoughtEngine'] = MeshThoughtEngine()
            logger.info("ğŸ§  ç½‘çŠ¶æ€ç»´å¼•æ“åŠ è½½æˆåŠŸï¼ˆç³»ç»Ÿæ ¸å¿ƒå·¥å…·ï¼‰")
        except ImportError as e:
            logger.warning(f"ç½‘çŠ¶æ€ç»´å¼•æ“åŠ è½½å¤±è´¥: {e}")
        
        try:
            from src.cognitive_engines.reasoning_engine import ReasoningEngine
            self.tool_instances['ReasoningEngine'] = ReasoningEngine()
            logger.info("ğŸ§  ç†æ€§è®¤çŸ¥å¼•æ“åŠ è½½æˆåŠŸï¼ˆè®°å¿†é‡æ„ä¾èµ–ï¼‰")
        except ImportError as e:
            logger.warning(f"ç†æ€§è®¤çŸ¥å¼•æ“åŠ è½½å¤±è´¥: {e}")
        
        try:
            from src.cognitive_engines.cognitive_barrier_break_engine import CognitiveBarrierBreakEngine
            self.tool_instances['CognitiveBarrierBreakEngine'] = CognitiveBarrierBreakEngine()
            logger.info("ğŸ§  è®¤çŸ¥ç ´éšœå¼•æ“åŠ è½½æˆåŠŸï¼ˆè®°å¿†é‡æ„ä¾èµ–ï¼‰")
        except ImportError as e:
            logger.warning(f"è®¤çŸ¥ç ´éšœå¼•æ“åŠ è½½å¤±è´¥: {e}")
        
        # ğŸ”¥ é…ç½®é«˜çº§å·¥å…·çš„æ‡’åŠ è½½æ˜ å°„ï¼ˆä¸ç«‹å³å®ä¾‹åŒ–ï¼‰
        # æ³¨æ„ï¼šMeshThoughtEngineã€ReasoningEngineã€CognitiveBarrierBreakEngine
        # å·²åœ¨ä¸Šæ–¹ä½œä¸ºåŸºç¡€å·¥å…·åŠ è½½ï¼Œæ­¤å¤„ä¸å†é…ç½®æ‡’åŠ è½½
        # 
        # âš ï¸ å¤šæ¨¡æ€å¼•æ“å·²ç§»é™¤ï¼š
        # - VisionProcessingEngineã€AudioProcessingEngineã€MultimodalFusionEngineç­‰
        #   ä¸æ˜¯æ™ºèƒ½ä½“é€šç”¨å·¥å…·ï¼Œä»…åœ¨ç‰¹å®šåœºæ™¯ä½¿ç”¨ï¼š
        #   1. æ–‡ä»¶ä¸Šä¼ æ¥å£ï¼šç³»ç»Ÿçº§è°ƒç”¨ï¼Œå¤„ç†å›¾ç‰‡/éŸ³é¢‘æ—¶å®ä¾‹åŒ–
        #   2. æ•°æ®æ”¶é›†å¸ˆï¼šæ™ºèƒ½ä½“çº§è°ƒç”¨ï¼Œçˆ¬å–ç½‘é¡µå¤šåª’ä½“å†…å®¹æ—¶ä½¿ç”¨
        self._advanced_tools_config = {
            'MemoryReconstructionEngine': {
                'module': 'src.cognitive_engines.memory_reconstruction_engine',
                'class': 'MemoryReconstructionEngine',
                'description': 'è®°å¿†é‡æ„å¼•æ“'
            },
            'AbductiveReasoningEngine': {
                'module': 'src.abductive_reasoning_engine',
                'class': 'AbductiveReasoningTool',
                'description': 'æº¯å› æ¨ç†å¼•æ“'
            },
            'HierarchicalLearningEngine': {
                'module': 'hierarchical_learning_engine',
                'class': 'HierarchicalLearningTool',
                'description': 'åˆ†å±‚å­¦ä¹ å¼•æ“'
            }
        }
        logger.info(f"ğŸ”§ åŸºç¡€å·¥å…·åˆå§‹åŒ–å®Œæˆï¼ˆå«3ä¸ªæ ¸å¿ƒè®¤çŸ¥å¼•æ“ï¼‰ï¼Œé«˜çº§å·¥å…·({len(self._advanced_tools_config)}ä¸ª)å°†æŒ‰éœ€åŠ è½½")
        logger.info("ğŸš¨ å¤šæ¨¡æ€å¼•æ“ä¸åœ¨é€šç”¨å·¥å…·é›†ï¼Œä»…ç‰¹å®šåœºæ™¯ä½¿ç”¨ï¼šæ–‡ä»¶ä¸Šä¼ /æ•°æ®æ”¶é›†å¸ˆ")
    
    def _lazy_load_tool(self, tool_name: str) -> bool:
        """æ‡’åŠ è½½é«˜çº§å·¥å…·
        
        Args:
            tool_name: å·¥å…·åç§°
            
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        # å¦‚æœå·²åŠ è½½ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        if tool_name in self.tool_instances:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é«˜çº§å·¥å…·é…ç½®ä¸­
        if tool_name not in self._advanced_tools_config:
            return False
        
        config = self._advanced_tools_config[tool_name]
        try:
            # åŠ¨æ€å¯¼å…¥æ¨¡å—
            module = importlib.import_module(config['module'])
            tool_class = getattr(module, config['class'])
            
            # å®ä¾‹åŒ–å·¥å…·
            self.tool_instances[tool_name] = tool_class()
            logger.info(f"ğŸ”§ æ‡’åŠ è½½: {config['description']}åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.warning(f"æ‡’åŠ è½½{config['description']}å¤±è´¥: {e}")
            return False
    
    def _log_tool_call(self, tool_name: str, parameters: Dict[str, Any], result: Dict[str, Any], duration: float, success: bool, caller_info: Dict[str, Any] = None, usage_intention: str = None, active_call: bool = True):
        """è®°å½•å·¥å…·è°ƒç”¨æ—¥å¿—
        
        Args:
            tool_name: å·¥å…·åç§°
            parameters: è°ƒç”¨å‚æ•°
            result: è¿”å›ç»“æœ
            duration: è°ƒç”¨è€—æ—¶ï¼ˆç§’ï¼‰
            success: è°ƒç”¨æ˜¯å¦æˆåŠŸ
            caller_info: è°ƒç”¨è€…ä¿¡æ¯
            usage_intention: ä½¿ç”¨æ„å›¾
            active_call: æ˜¯å¦ä¸»åŠ¨è°ƒç”¨
        """
        # ç¡®ä¿logsç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname('logs/tool_calls.log'), exist_ok=True)
        
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "tool_name": tool_name,
            "parameters": parameters,
            "result": result,
            "duration": duration,
            "success": success,
            "caller_info": caller_info or {},
            "usage_intention": usage_intention,
            "active_call": active_call
        }
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        with open("logs/tool_calls.log", "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        
        # åŒæ—¶è®°å½•åˆ°logger
        logger.info(f"Tool call: {tool_name}, Success: {success}, Duration: {duration:.3f}s, Intention: {usage_intention}")
    
    def call_tool(self, tool_name: str, parameters: Dict[str, Any], caller_info: Dict[str, Any] = None, usage_intention: str = None, active_call: bool = True) -> Dict[str, Any]:
        """è°ƒç”¨å·¥å…·"""
        start_time = datetime.now()
        # çŸ¥è¯†å›¾è°±èƒ½åŠ›æ˜ å°„ï¼ˆLLM+å·¥å…·é›†è·¯ç”±ï¼‰
        if isinstance(tool_name, str) and tool_name.startswith('knowledge_graph'):
            try:
                from src.mesh_database_interface import MeshDatabaseInterface
                from src.multi_layer_graph_manager import MultiLayerGraphManager
                interface = MeshDatabaseInterface()
                if tool_name == 'knowledge_graph.build':
                    topic = parameters.get('topic')
                    max_nodes = parameters.get('max_nodes', 500)
                    min_importance = parameters.get('min_importance', 0.05)
                    dynamic_inclusion = parameters.get('dynamic_inclusion', True)
                    graph = interface.build_knowledge_graph(topic=topic, max_nodes=max_nodes, min_importance=min_importance, dynamic_inclusion=dynamic_inclusion)
                    edges = graph.get('edges', [])
                    time_edges = [e for e in edges if e.get('type') == 'time_sequence']
                    causal_edges = [e for e in edges if e.get('type') == 'causal']
                    return {
                        'success': True,
                        'data': {
                            'graph': graph,
                            'stats': {
                                'nodes': len(graph.get('nodes', [])),
                                'edges': len(edges),
                                'time_sequence_edges': len(time_edges),
                                'causal_edges': len(causal_edges),
                            }
                        }
                    }
                elif tool_name == 'knowledge_graph.search_across_layers':
                    query = parameters.get('query', '')
                    max_results = parameters.get('max_results', 10)
                    manager = MultiLayerGraphManager(interface)
                    manager.build_multi_layer_graphs()
                    res = manager.search_across_layers(query, max_results=max_results)
                    return {'success': True, 'data': res}
                elif tool_name == 'knowledge_graph.get_layer_navigation':
                    layer_id = parameters.get('layer_id')
                    manager = MultiLayerGraphManager(interface)
                    result = manager.build_multi_layer_graphs()
                    if not layer_id and result.get('layer_graphs'):
                        layer_id = list(result['layer_graphs'].keys())[0]
                    nav = manager.get_layer_navigation(layer_id)
                    return {'success': True, 'data': nav}
            except Exception as e:
                return {'success': False, 'error': str(e)}
        
       # æ¯”è¾ƒå›ç­”å¢å¼ºRAGå…¥å£ï¼ˆåŸºçº¿å‡è®¾ vs RAGå›æ‹¼ vs ç†æ€§è®¤çŸ¥ç»¼åˆ â†’ åˆ†æ®µè¾“å‡ºï¼‰
        if tool_name == 'comparative_answer':
            question = parameters.get('question', '')
            file_path = parameters.get('file_path')
            content = parameters.get('content')
            enable_baseline = parameters.get('enable_baseline', True)
            if not question:
                return {'success': False, 'error': 'ç¼ºå°‘questionå‚æ•°', 'tool': 'comparative_answer'}
            # 1. åŸºçº¿ç”Ÿæˆï¼ˆæ— ä»˜è´¹LLMæ—¶ä½¿ç”¨æ¨¡æ¿ï¼‰
            baseline_answer = "äººæ°‘çš„å¸¸è§„å®šä¹‰é€šå¸¸æŒ‡ç¤¾ä¼šä¸­çš„ç»å¤§å¤šæ•°åŠ³åŠ¨è€…ä¸æ‹¥æŠ¤å…¬å…±åˆ©ç›Šçš„ç¾¤ä½“ï¼Œé€šå¸¸ä¸é™å®šä¸ºæŸä¸€ç‰¹å®šé˜¶å±‚ã€‚"
            try:
                if enable_baseline and self.chat_tool_manager:
                    llm_client = self.chat_tool_manager.llm_client
                    if hasattr(llm_client, 'chat') and callable(llm_client.chat):
                        baseline_prompt = f"è¯·åŸºäºä½ çš„é¢„è®­ç»ƒçŸ¥è¯†ç›´æ¥å›ç­”ï¼š{question}"
                        baseline_answer = llm_client.chat(baseline_prompt)
            except Exception:
                pass  # æ¨¡æ¿å…œåº•
            # 2. RAGå›æ‹¼
            slices = []
            try:
                ati_tmp = AgentToolIntegration()
                if file_path:
                    res = ati_tmp.call_tool('memory_slicer', {'file_path': file_path, 'config': {}}, {'agent_type': 'implementer'})
                    slices = res.get('data', [])
                elif content:
                    res = ati_tmp.call_tool('memory_slicer', {'content': content, 'config': {}}, {'agent_type': 'implementer'})
                    slices = res.get('data', [])
            except Exception as e:
                return {'success': False, 'error': f'åˆ†ç‰‡å¤±è´¥: {e}', 'tool': 'comparative_answer'}
            # å…³é”®è¯åŠ æƒç­›é€‰
            keywords = parameters.get('keywords', ['å·¥å†œ', 'äººæ°‘', 'ä¸ºäººæ°‘æœåŠ¡', 'ç”Ÿäº§', 'è½¬åŒ–', 'ä»·å€¼', 'æ ¸å¿ƒ', 'ç†µ', 'å…±ç”Ÿ'])
            scored_slices = []
            for s in slices:
                txt = s.get('content', '')
                score = sum(txt.count(k) for k in keywords)
                if score > 0:
                    scored_slices.append({
                        'slice_id': s.get('slice_id'),
                        'quality': round(s.get('semantic_quality', 0), 3),
                        'importance': round(s.get('importance', 0), 3),
                        'score': score,
                        'content': txt
                    })
            scored_slices.sort(key=lambda x: (x['score'], x['quality'], len(x['content'])), reverse=True)
            rag_refs = [{'id': x['slice_id'], 'quality': x['quality'], 'importance': x['importance'], 'preview': x['content'][:160]} for x in scored_slices[:8]]
            # 3. ç†æ€§è®¤çŸ¥å¼•æ“ç»¼åˆï¼ˆå››å¾‹ï¼‰
            reasoning_summary = "åŸºäºåŒä¸€å¾‹ã€ä¸çŸ›ç›¾å¾‹ã€æ’ä¸­å¾‹ã€å……è¶³ç†ç”±è¿›è¡Œç»¼åˆï¼šåŸºçº¿ä¸ºå¹¿ä¹‰å®šä¹‰ï¼ŒRAGé”šå®šç‹­ä¹‰æ ¸å¿ƒï¼ˆå·¥å†œ=ä»·å€¼åŸç‚¹ï¼‰ï¼›äºŒè€…æ— çŸ›ç›¾ï¼Œç‹­ä¹‰ä¸ºå¹¿ä¹‰å­é›†ï¼›ç³»ç»Ÿè®ºè§†è§’ä¸‹å·¥å†œä¸ºç§©åºåŸºåº•ï¼Œç¬¦åˆå……è¶³ç†ç”±ã€‚"
            try:
                if 'ReasoningEngine' in self.tool_instances:
                    premise = {'baseline': baseline_answer, 'rag_top_slices': [x['content'][:200] for x in scored_slices[:3]]}
                    res_reasoning = self._call_reasoning_engine(self.tool_instances['ReasoningEngine'], {'premise': premise, 'rules': ['contradiction', 'identity', 'excluded_middle', 'sufficient_reason']})
                    if res_reasoning.get('success'):
                        reasoning_summary = str(res_reasoning.get('data', {}).get('reasoning_results', {}))
            except Exception:
                pass
             # 4. åˆ†æ®µé‡æ„è¾“å‡º
            seg1 = 'ä¸€ã€ä¸ºä»€ä¹ˆæ˜¯"å”¯æœ‰å·¥å†œ"\n- å·¥å†œç›´æ¥åˆ›é€ ç¤¾ä¼šç‰©è´¨åŸºç¡€ï¼Œæ˜¯ä»·å€¼åŸç‚¹ï¼ˆè§åˆ‡ç‰‡12.1ã€15.1.1.1.1.1ï¼‰ã€‚'
            seg2 = 'äºŒã€å…¶ä»–é˜¶å±‚å¦‚ä½•æœåŠ¡\n- åŒ»ç”Ÿä¿éšœå·¥å†œå¥åº·ï¼›å®˜å‘˜ä¼˜åŒ–ç”Ÿäº§ä¸åˆ†é…ï¼›çŸ¥è¯†åˆ†å­ä»¥æŠ€æœ¯ææ•ˆã€‚å…¶ä»·å€¼é¡»ç»å·¥å†œç”Ÿäº§è½¬åŒ–è½åœ°ï¼ˆè§åˆ‡ç‰‡13.1.1.2.1ã€13.1.2.1.1.1ï¼‰ã€‚'
            seg3 = 'ä¸‰ã€ä»å£å·åˆ°ç”Ÿå­˜é€»è¾‘\n- "ä¸ºäººæ°‘æœåŠ¡"æ˜¯æŠ—ç†µå…±ç”Ÿçš„ç”Ÿå­˜é€»è¾‘ï¼Œè„±ç¦»å·¥å†œå³ä»·å€¼ç©ºè½¬ä¸é“¾æ¡æ–­è£‚ï¼ˆè§åˆ‡ç‰‡13.1.1.2.1ï¼‰ã€‚'
            seg4 = 'å››ã€ç³»ç»Ÿè®ºä¸æ¨ªæ¸ å››å¥\n- å·¥å†œä¸ºç†µå‡ä¸»åŠ›ï¼›ååŒå³ç§©åºæœ€å¤§åŒ–ï¼›"ä¸ºç”Ÿæ°‘ç«‹å‘½"å³é”šå®šå·¥å†œå½¢æˆä»·å€¼é—­ç¯ï¼ˆè§åˆ‡ç‰‡16.2.1ã€22.1.2.1ã€20.2.1ï¼‰ã€‚'
            segments = [seg1, seg2, seg3, seg4]
            return {
                'success': True,
                'data': {
                    'baseline': baseline_answer,
                    'rag_refs': rag_refs,
                    'reasoning_summary': reasoning_summary,
                    'synthesized': segments
                },
                'tool': 'comparative_answer'
            }
            
        # æ–°å¢ï¼šåˆ†ç‰‡å¯¼å…¥ç½‘çŠ¶æ€ç»´å¼•æ“ï¼ˆé©±åŠ¨å‰ç«¯æ–‡æœ¬å—ç»Ÿè®¡ï¼‰
        if tool_name == 'ingest_slices_to_mesh':
            file_path = parameters.get('file_path')
            content = parameters.get('content')
            topic = parameters.get('topic') or (file_path or content or '')
            try:
                from src.mesh_database_interface import MeshDatabaseInterface
                mdi = MeshDatabaseInterface()
                slices = []
                cfg = parameters.get('config', {})
                if file_path:
                    res = self.call_tool('memory_slicer', {'file_path': file_path, 'config': cfg}, caller_info, usage_intention, active_call)
                    slices = res.get('data', []) if isinstance(res, dict) else []
                elif content:
                    res = self.call_tool('memory_slicer', {'content': content, 'config': cfg}, caller_info, usage_intention, active_call)
                    slices = res.get('data', []) if isinstance(res, dict) else []
                ingested = 0
                duplicates = 0
                for s in slices:
                    txt = (s.get('content') or '').strip()
                    if not txt:
                        continue
                    data = {
                        'topic': topic or 'æœªåˆ†ç±»',
                        'content': txt,
                        'source_type': 'slice',
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'importance': s.get('importance', 0.5)
                    }
                    r = mdi.store_memory_with_mesh(data)
                    if r.get('is_duplicate'):
                        duplicates += 1
                    else:
                        ingested += 1
                return {
                    'success': True,
                    'data': {
                        'total_slices': len(slices),
                        'ingested': ingested,
                        'duplicates': duplicates,
                        'mesh_node_count': mdi.thought_engine.get_node_count()
                    },
                    'tool': 'ingest_slices_to_mesh'
                }
            except Exception as e:
                return {'success': False, 'error': f'å¯¼å…¥å¤±è´¥: {e}', 'tool': 'ingest_slices_to_mesh'}

        # å·¥å…·æ³¨å†Œè‡ªæ£€å…¥å£ï¼ˆä¸ä¾èµ–èŠå¤©å·¥å…·ç®¡ç†å™¨ï¼‰
        if tool_name == 'tool_registry_check':
            try:
                from tools.chat_tools import create_tool_manager
                mgr = create_tool_manager()
                available = mgr.list_available_tools()
                expected = [
                    'memory_retrieval','file_reading','web_search','memory_iteration',
                    'command_line','file_writing','equality_assessment','memory_slicer',
                    'networked_thinking','reasoning_engine','cognitive_barrier_break','terminal_display',
                    'thinking_tracer'
                ]
                external = ['code_index_build','code_symbol_search']
                missing = [t for t in expected if t not in available]
                return {
                    'success': True,
                    'data': {
                        'available_tools': available,
                        'expected_tools': expected,
                        'missing_in_manager': missing,
                        'external_tools_handled_by_agent_integration': external
                    },
                    'tool': 'tool_registry_check'
                }
            except Exception as e:
                return {'success': False, 'error': f'å·¥å…·æ³¨å†Œè‡ªæ£€å¤±è´¥: {e}', 'tool': 'tool_registry_check'}

        # ä»£ç ç´¢å¼•ç›´è¿å…¥å£ï¼ˆä¸ä¾èµ–èŠå¤©å·¥å…·ç®¡ç†å™¨ï¼‰
        if tool_name == 'code_index_build':
            caller_type = (caller_info or {}).get('agent_type')
            if caller_type not in ('implementer', 'developer'):
                try:
                    from src.error_reporting import get_error_reporting_service
                    er = get_error_reporting_service()
                    er.report_component_error({
                        "error_id": er.generate_error_id("agent_tool_integration", "CodeIndexBuildDenied"),
                        "level": "component",
                        "type": "PermissionDenied",
                        "message": "ç´¢å¼•æ„å»ºä»…é™å®ç°å¸ˆ/å¼€å‘è€…è§¦å‘",
                        "timestamp": datetime.now().isoformat(),
                        "component": "agent_tool_integration",
                        "function": "code_index_build",
                        "context": {"caller_info": caller_info}
                    })
                except Exception:
                    pass
                return {'success': False, 'error': 'æƒé™ä¸è¶³ï¼šç´¢å¼•æ„å»ºä»…é™å®ç°å¸ˆ/å¼€å‘è€…', 'tool': 'code_index_build', 'data': {'denied_for_role': caller_type}}
            import sqlite3, hashlib, ast
            base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
            data_dir = os.path.join(base_dir, 'data')
            os.makedirs(data_dir, exist_ok=True)
            db_path = os.path.join(data_dir, 'code_index_db.sqlite')
            mode = parameters.get('mode', 'incremental')
            scope = parameters.get('scope', 'src')
            target_root = os.path.join(base_dir, scope) if not os.path.isabs(scope) else scope
            conn = sqlite3.connect(db_path)
            cur = conn.cursor()
            cur.execute("CREATE TABLE IF NOT EXISTS files (file_path TEXT PRIMARY KEY, file_hash TEXT NOT NULL, owner_component TEXT, protocol_version TEXT, last_modified DATETIME NOT NULL)")
            cur.execute("CREATE TABLE IF NOT EXISTS symbols (symbol_id TEXT PRIMARY KEY, file_path TEXT NOT NULL, symbol_name TEXT NOT NULL, symbol_type TEXT NOT NULL, signature TEXT, docstring TEXT, start_line INTEGER, end_line INTEGER)")
            cur.execute("CREATE TABLE IF NOT EXISTS relations (source_symbol_id TEXT NOT NULL, relation_type TEXT NOT NULL, target_symbol_id TEXT NOT NULL, PRIMARY KEY (source_symbol_id, relation_type, target_symbol_id))")
            cur.execute("CREATE TABLE IF NOT EXISTS components (component_id TEXT PRIMARY KEY, name TEXT, depends_on TEXT, provides TEXT)")
            # è‡ªæ›å…‰ç»„ä»¶åŒæ­¥
            try:
                exposures_path = os.path.join(base_dir, 'self_exposures.json')
                if os.path.exists(exposures_path):
                    exposures = json.load(open(exposures_path, 'r', encoding='utf-8'))
                    for exp in exposures:
                        cid = exp.get('id')
                        if cid:
                            cur.execute("INSERT OR REPLACE INTO components(component_id,name,depends_on,provides) VALUES (?,?,?,?)", (
                                cid,
                                exp.get('name'),
                                json.dumps(exp.get('needs', {}).get('deps', []), ensure_ascii=False),
                                json.dumps(exp.get('provides', {}), ensure_ascii=False)
                            ))
            except Exception:
                pass
            indexed_files = 0
            indexed_symbols = 0
            indexed_relations = 0
            for root, dirs, files in os.walk(target_root):
                if any(seg in root for seg in ('__pycache__', 'venv', '.git')):
                    continue
                for fname in files:
                    if not fname.endswith('.py'):
                        continue
                    fpath = os.path.join(root, fname)
                    try:
                        with open(fpath, 'r', encoding='utf-8', errors='ignore') as rf:
                            content = rf.read()
                        file_hash = hashlib.sha256(content.encode('utf-8', errors='ignore')).hexdigest()
                        last_modified = datetime.fromtimestamp(os.path.getmtime(fpath)).isoformat()
                        cur.execute("SELECT file_hash FROM files WHERE file_path=?", (fpath,))
                        row = cur.fetchone()
                        if mode == 'incremental' and row and row[0] == file_hash:
                            continue
                        tree = ast.parse(content)
                        cur.execute("INSERT OR REPLACE INTO files(file_path,file_hash,owner_component,protocol_version,last_modified) VALUES (?,?,?,?,?)", (
                            fpath, file_hash, None, None, last_modified
                        ))
                        indexed_files += 1
                        def make_id(name, start):
                            return f"{fpath}:{name}:{start}"
                        for node in ast.walk(tree):
                            if isinstance(node, ast.FunctionDef):
                                name = node.name
                                start = getattr(node, 'lineno', 1)
                                end = getattr(node, 'end_lineno', start)
                                doc = ast.get_docstring(node) or ''
                                sid = make_id(name, start)
                                cur.execute("INSERT OR REPLACE INTO symbols(symbol_id,file_path,symbol_name,symbol_type,signature,docstring,start_line,end_line) VALUES (?,?,?,?,?,?,?,?)", (
                                    sid, fpath, name, 'function', None, doc, start, end
                                ))
                                indexed_symbols += 1
                                for inner in ast.walk(node):
                                    if isinstance(inner, ast.Call):
                                        callee = None
                                        if isinstance(inner.func, ast.Name):
                                            callee = inner.func.id
                                        elif isinstance(inner.func, ast.Attribute):
                                            callee = inner.func.attr
                                        if callee:
                                            target_id = f"{fpath}:{callee}:"
                                            cur.execute("INSERT OR REPLACE INTO relations(source_symbol_id,relation_type,target_symbol_id) VALUES (?,?,?)", (
                                                sid, 'calls', target_id
                                            ))
                                            indexed_relations += 1
                            elif isinstance(node, ast.ClassDef):
                                name = node.name
                                start = getattr(node, 'lineno', 1)
                                end = getattr(node, 'end_lineno', start)
                                doc = ast.get_docstring(node) or ''
                                sid = make_id(name, start)
                                cur.execute("INSERT OR REPLACE INTO symbols(symbol_id,file_path,symbol_name,symbol_type,signature,docstring,start_line,end_line) VALUES (?,?,?,?,?,?,?,?)", (
                                    sid, fpath, name, 'class', None, doc, start, end
                                ))
                                indexed_symbols += 1
                                for base in (node.bases or []):
                                    try:
                                        if isinstance(base, ast.Name):
                                            base_name = base.id
                                        elif isinstance(base, ast.Attribute):
                                            base_name = base.attr
                                        else:
                                            base_name = None
                                        if base_name:
                                            target_id = f"{fpath}:{base_name}:"
                                            cur.execute("INSERT OR REPLACE INTO relations(source_symbol_id,relation_type,target_symbol_id) VALUES (?,?,?)", (
                                                sid, 'extends', target_id
                                            ))
                                            indexed_relations += 1
                                    except Exception:
                                        continue
                    except Exception:
                        continue
            conn.commit()
            return {
                'success': True,
                'data': {'db_path': db_path, 'indexed_files': indexed_files, 'indexed_symbols': indexed_symbols, 'indexed_relations': indexed_relations, 'mode': mode, 'scope': target_root},
                'tool': 'code_index_build'
            }
        if tool_name == 'code_symbol_search':
            import sqlite3
            base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
            db_path = os.path.join(base_dir, 'data', 'code_index_db.sqlite')
            if not os.path.exists(db_path):
                return {'success': False, 'error': 'ç´¢å¼•åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ„å»º', 'tool': 'code_symbol_search'}
            conn = sqlite3.connect(db_path)
            cur = conn.cursor()
            query = parameters.get('query', '')
            filters = parameters.get('filters', {})
            symbol_type = filters.get('symbol_type')
            file_filter = filters.get('file_path')
            limit = int(parameters.get('limit', 50))
            conditions = ["symbol_name LIKE ?"]
            params = [f"%{query}%"]
            if symbol_type:
                conditions.append("symbol_type = ?")
                params.append(symbol_type)
            if file_filter:
                conditions.append("file_path LIKE ?")
                params.append(f"%{file_filter}%")
            where_clause = ' AND '.join(conditions)
            cur.execute(f"SELECT symbol_id,file_path,symbol_name,symbol_type,start_line,end_line FROM symbols WHERE {where_clause} LIMIT ?", (*params, limit))
            rows = cur.fetchall()
            results = [{
                'symbol_id': r[0], 'file_path': r[1], 'symbol_name': r[2], 'symbol_type': r[3], 'start_line': r[4], 'end_line': r[5]
            } for r in rows]
            relation = filters.get('relation')
            if relation in ('calls', 'called_by', 'extends', 'implemented_by') and results:
                expanded = []
                for item in results:
                    sid = item['symbol_id']
                    if relation == 'called_by':
                        cur.execute("SELECT source_symbol_id FROM relations WHERE relation_type='calls' AND target_symbol_id LIKE ?", (sid.split(':')[0] + ':%',))
                        callers = [row[0] for row in cur.fetchall()]
                        item['called_by'] = callers
                    else:
                        cur.execute("SELECT target_symbol_id FROM relations WHERE relation_type=? AND source_symbol_id=?", (relation, sid))
                        targets = [row[0] for row in cur.fetchall()]
                        item[relation] = targets
                    expanded.append(item)
                results = expanded
            return {'success': True, 'data': {'results': results, 'count': len(results)}, 'tool': 'code_symbol_search'}

        if tool_name == 'terminal_display':
            td = self.chat_tool_manager.get_tool('terminal_display') if self.chat_tool_manager else None
            if not td:
                return {'success': False, 'error': 'ç»ˆç«¯æ˜¾ç¤ºæ å·¥å…·æœªæ³¨å†Œ', 'tool': 'terminal_display'}
            action = parameters.get('action', 'list_logs')
            try:
                if action == 'list_logs':
                    res = td.list_logs()
                elif action == 'tail_log':
                    res = td.tail_log(parameters.get('file_name', 'system_errors.log'), parameters.get('lines', 200))
                elif action == 'get_startup_status':
                    res = td.get_startup_status()
                elif action == 'tail_interactions':
                    res = td.tail_interactions(parameters.get('date'), parameters.get('lines', 100))
                else:
                    return {'success': False, 'error': f'æœªçŸ¥action: {action}', 'tool': 'terminal_display'}
                return {'success': res.get('success', False), 'data': res.get('data'), 'error': res.get('error'), 'tool': 'terminal_display'}
            except Exception as e:
                return {'success': False, 'error': f'ç»ˆç«¯æ˜¾ç¤ºæ è°ƒç”¨å¤±è´¥: {e}', 'tool': 'terminal_display'}

        # å¦‚æœæ˜¯æ€ç»´é€æ˜åŒ–è¿½è¸ªå™¨ï¼Œç›´æ¥è¿”å›å·¥å…·å®ä¾‹å…ƒä¿¡æ¯
        if tool_name == 'thinking_tracer':
            tracer_tool = self.tool_instances.get('thinking_tracer')
            if tracer_tool:
                return {
                    'success': True,
                    'data': {
                        'tool_name': 'thinking_tracer',
                        'description': 'ç»Ÿä¸€ç®¡ç†æ™ºèƒ½ä½“ä¸èŠå¤©å®¤çš„æ€ç»´é€æ˜åŒ–æ­¥éª¤è®°å½•',
                        'type': 'tool',
                        'scope': 'global',
                        'capabilities': ['æŒ‰ä¼šè¯ç»´åº¦è®°å½•æ€ç»´æ­¥éª¤', 'æ”¯æŒå¤šæ¥æº(æ™ºèƒ½ä½“/èŠå¤©å®¤)', 'ç»Ÿä¸€ç»“æ„ä¾›å‰ç«¯å±•ç¤º']
                    },
                    'tool': 'thinking_tracer'
                }
            else:
                return {'success': False, 'error': 'æ€ç»´é€æ˜åŒ–è¿½è¸ªå·¥å…·æœªæ³¨å†Œ', 'tool': 'thinking_tracer'}
        
        # 1. ä¼˜å…ˆä»èŠå¤©å·¥å…·ç®¡ç†å™¨ä¸­è·å–é«˜é¢‘æ ¸å¿ƒå·¥å…·
        if self.chat_tool_manager:
            chat_tool = self.chat_tool_manager.get_tool(tool_name)
            if chat_tool:
                try:
                    # æ ¹æ®å·¥å…·ç±»å‹è°ƒç”¨ç›¸åº”æ–¹æ³•
                    if tool_name == 'file_reading':
                        # æ–‡ä»¶è¯»å–å·¥å…·ï¼ˆå§”æ‰˜ç»™å·¥å…·ç®¡ç†å™¨ï¼Œéµå¾ªé»‘ç®±åŸåˆ™ï¼‰
                        file_path = parameters.get('file_path')
                        encoding = parameters.get('encoding')
                        start_line = parameters.get('start_line')
                        num_lines = parameters.get('num_lines')
                        query = parameters.get('query')
                        pattern = parameters.get('pattern')
                        
                        # ä¼˜å…ˆæŒ‰æ˜ç¡®è·¯å¾„è¯»å–
                        if file_path:
                            # æ”¯æŒå¼ºåˆ¶ç¼–ç ä¸ç‰‡æ®µè¯»å–
                            if start_line and num_lines:
                                content = chat_tool.read_file_chunk(file_path, start_line=int(start_line), num_lines=int(num_lines))
                            else:
                                content = chat_tool.read_text_file(file_path, encoding=encoding)
                            if content is not None:
                                return {
                                    'success': True,
                                    'data': {'content': content},
                                    'tool': 'file_reading'
                                }
                            else:
                                return {
                                    'success': False,
                                    'error': f'æ–‡ä»¶è¯»å–å¤±è´¥: {file_path}',
                                    'data': {'path': file_path, 'reason': 'read_failed'}
                                }
                        
                        # è·¯å¾„ç¼ºå¤±æ—¶ï¼Œå°è¯•æŒ‰æŸ¥è¯¢/æ¨¡å¼è‡ªåŠ¨å®šä½å€™é€‰æ–‡ä»¶
                        # è·¯å¾„ç¼ºå¤±æ—¶ï¼Œå°è¯•æŒ‰å›¾è°±/å†…å®¹/æ¨¡å¼è‡ªåŠ¨å®šä½å€™é€‰æ–‡ä»¶
                        # æ ¹æ®è°ƒç”¨è€…è§’è‰²è®¾å®šé»˜è®¤pattern
                        role = (caller_info or {}).get('agent_type')
                        role_patterns = {
                            'architect': '**/*.md',
                            'evaluator': 'docs/**/*.md',
                            'implementer': 'src/**/*.py',
                            'data_collector': 'data/**/*.json',
                            'maintenance': 'config/**/*'
                        }
                        if not pattern:
                            pattern = role_patterns.get(role, '*')

                        candidates = []
                        # å…ˆå°è¯•å›¾è°±å®šä½
                        try:
                            base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
                            graph_path = os.path.join(base_dir, 'data', 'component_graph.json')
                            if os.path.exists(graph_path):
                                with open(graph_path, 'r', encoding='utf-8') as gf:
                                    graph = json.load(gf)
                                file_nodes = [n for n in (graph.get('nodes') or []) if n.get('type') == 'file']
                                if query:
                                    for fn in file_nodes:
                                        p = fn.get('path','')
                                        if query.lower() in p.lower():
                                            # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ä»¥é€‚é…å·¥å…·è¯»å–
                                            try:
                                                relp = os.path.relpath(p, base_dir)
                                            except Exception:
                                                relp = p
                                            if relp not in candidates:
                                                candidates.append(relp)
                                candidates = candidates[:5]
                        except Exception:
                            pass

                        # å†…å®¹æ£€ç´¢å®šä½ï¼ˆè¡¥å……ï¼‰
                        if query and len(candidates) < 5:
                            search_hits = chat_tool.search_in_files(pattern, query, case_sensitive=False)
                            for hit in search_hits:
                                if hit['file'] not in candidates:
                                    candidates.append(hit['file'])
                                if len(candidates) >= 5:
                                    break
                        # å›é€€ï¼šæŒ‰æ–‡ä»¶æ¨¡å¼åˆ—å‡ºæœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
                        if not candidates:
                            listed = chat_tool.list_available_files(pattern=pattern)
                            candidates = [f['path'] for f in listed[:5]]
                        
                        # ä¾åºå°è¯•è¯»å–å€™é€‰æ–‡ä»¶
                        for cand in candidates:
                            content = chat_tool.read_text_file(cand, encoding=encoding)
                            if content:
                                return {
                                    'success': True,
                                    'data': {'content': content, 'resolved_path': cand},
                                    'tool': 'file_reading'
                                }
                        
                        # æœªèƒ½è¯»å–ï¼Œè¿”å›ç»“æ„åŒ–é”™è¯¯
                        return {
                            'success': False,
                            'error': 'æ–‡ä»¶è‡ªåŠ¨å®šä½/è¯»å–å¤±è´¥',
                            'data': {
                                'query': query,
                                'pattern': pattern,
                                'candidates': candidates
                            }
                        }
                    elif tool_name == 'file_writing':
                        # æ–‡ä»¶å†™å…¥å·¥å…·ï¼ˆå§”æ‰˜ç»™å·¥å…·ç®¡ç†å™¨ï¼Œéµå¾ªé»‘ç®±åŸåˆ™ï¼‰
                        # æƒé™æ§åˆ¶ï¼šåŒºåˆ†ä»£ç å†™å…¥å’Œæ–‡æœ¬å†™å…¥
                        caller_type = (caller_info or {}).get('agent_type')
                        file_path = parameters.get('file_path', '')
                        
                        # åˆ¤æ–­æ˜¯å¦ä¸ºä»£ç æ–‡ä»¶
                        code_extensions = ('.py', '.js', '.java', '.cpp', '.h', '.c', '.hpp', '.ts', '.jsx', '.tsx')
                        is_code_file = file_path.endswith(code_extensions)
                        
                        if is_code_file:
                            # ä»£ç å†™å…¥ - ä¸¥æ ¼é™åˆ¶ä»…å®ç°å¸ˆå¯å†™
                            if caller_type not in ('implementer', 'text_implementer', 'developer'):
                                try:
                                    from src.error_reporting import get_error_reporting_service
                                    error_service = get_error_reporting_service()
                                    component_error = {
                                        "error_id": error_service.generate_error_id(caller_type or 'unknown', "CodeWritePermissionDenied"),
                                        "level": "component",
                                        "type": "PermissionDenied",
                                        "message": "ä»£ç å†™å…¥ä»…é™å®ç°å¸ˆï¼Œå½“å‰è§’è‰²è¢«ç¦æ­¢",
                                        "timestamp": datetime.now().isoformat(),
                                        "component": caller_type or 'unknown',
                                        "function": "file_writing",
                                        "file_path": file_path,
                                        "line_number": 0,
                                        "stack_trace": "agent_tool_integration.call_tool",
                                        "context": {"caller_info": caller_info, "usage_intention": usage_intention}
                                    }
                                    error_service.report_component_error(component_error)
                                except Exception:
                                    pass
                                return {
                                    'success': False,
                                    'error': 'æƒé™ä¸è¶³ï¼šä»£ç å†™å…¥ä»…é™å®ç°å¸ˆ',
                                    'tool': 'file_writing',
                                    'data': {'denied_for_role': caller_type, 'file_type': 'code'}
                                }
                        else:
                            # æ–‡æœ¬å†™å…¥ - æ‰€æœ‰æ™ºèƒ½ä½“å¯å†™ï¼Œä½†é™åˆ¶ç›®å½•
                            allowed_text_dirs = [
                                'logs/', 'data/agent_diaries/', 'data/feedback/', 
                                'docs/reports/', 'data/bubbles/', 'data/agent_logs/',
                                'temp/', 'output/'
                            ]
                            # æ£€æŸ¥æ˜¯å¦å†™å…¥å…è®¸çš„ç›®å½•
                            is_allowed_dir = any(file_path.startswith(d) for d in allowed_text_dirs)
                            if not is_allowed_dir:
                                return {
                                    'success': False,
                                    'error': f'æ–‡æœ¬æ–‡ä»¶åªèƒ½å†™å…¥æŒ‡å®šç›®å½•: {", ".join(allowed_text_dirs)}',
                                    'tool': 'file_writing',
                                    'data': {'allowed_dirs': allowed_text_dirs, 'requested_path': file_path}
                                }
                        # å…±è¯†ä¸Šä¸‹æ–‡åŠ è½½ï¼ˆåŸºäºç»„ä»¶-æ–‡ä»¶å›¾è°±ä¸è‡ªæ›å…‰æ±‡æ€»ï¼‰
                        # æ³¨æ„ï¼šä»…å¯¹ä»£ç æ–‡ä»¶å¯ç”¨å…±è¯†æ£€æŸ¥ï¼Œæ–‡æœ¬æ–‡ä»¶è·³è¿‡
                        enable_consensus = parameters.get('enable_consensus', True) and is_code_file
                        dry_run = parameters.get('dry_run', False)
                        target_path = parameters.get('file_path')
                        content = parameters.get('content')
                        overwrite = parameters.get('overwrite', False)
                        # ä»£ç ç´¢å¼•åº“åªè¯»é—¨ç¦ï¼šé˜²æ­¢å°†æ´¾ç”Ÿç´¢å¼•åº“è§†ä¸ºæºä»£ç å¹¶ä¿®æ”¹
                        try:
                            base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
                            code_index_db = os.path.join(base_dir, 'data', 'code_index_db.sqlite')
                            if target_path and os.path.abspath(target_path) == os.path.abspath(code_index_db):
                                from src.error_reporting import get_error_reporting_service
                                er = get_error_reporting_service()
                                er.report_component_error({
                                    "error_id": er.generate_error_id("agent_tool_integration", "CodeIndexWriteDenied"),
                                    "level": "component",
                                    "type": "CodeIndexWriteDenied",
                                    "message": "ä»£ç ç´¢å¼•åº“ä¸ºåªè¯»ï¼ˆæ´¾ç”Ÿæ•°æ®ï¼‰ï¼Œè¯·ç¼–è¾‘æºä»£ç æ–‡ä»¶è€Œéç´¢å¼•åº“",
                                    "timestamp": datetime.now().isoformat(),
                                    "component": "agent_tool_integration",
                                    "function": "file_writing",
                                    "file_path": target_path or "",
                                    "line_number": 0,
                                    "stack_trace": "agent_tool_integration.call_tool",
                                    "context": {"caller_info": caller_info, "usage_intention": usage_intention}
                                })
                                return {
                                    'success': False,
                                    'error': 'ä»£ç ç´¢å¼•åº“ä¸ºåªè¯»ï¼ˆæ´¾ç”Ÿæ•°æ®ï¼‰ï¼Œè¯·ç¼–è¾‘æºä»£ç æ–‡ä»¶',
                                    'tool': 'file_writing',
                                    'data': {'denied_target': target_path}
                                }
                        except Exception:
                            pass

                        consensus_context = {
                            'owner_component': None,
                            'depends_on': [],
                            'protocol': {},
                            'graph_found': False,
                            'exposure_found': False
                        }
                        try:
                            base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
                            graph_path = os.path.join(base_dir, 'data', 'component_graph.json')
                            exposures_path = os.path.join(base_dir, 'self_exposures.json')
                            # å›¾è°±åŠ è½½
                            if os.path.exists(graph_path):
                                with open(graph_path, 'r', encoding='utf-8') as gf:
                                    graph = json.load(gf)
                                consensus_context['graph_found'] = True
                                file_abs = target_path if (target_path and os.path.isabs(target_path)) else str((rag_system_path / (target_path or '')).resolve())
                                file_node_id = f"file:{file_abs}"
                                owner = None
                                for e in graph.get('edges', []):
                                    if e.get('relation') == 'contains' and (e.get('target') == file_node_id or (target_path and str(e.get('target','')).endswith(target_path))):
                                        owner = e.get('source')
                                        break
                                consensus_context['owner_component'] = owner
                                if owner:
                                    consensus_context['depends_on'] = [edge.get('target') for edge in graph.get('edges', []) if edge.get('relation') == 'depends_on' and edge.get('source') == owner]
                            # è‡ªæ›å…‰æ±‡æ€»åŠ è½½
                            if os.path.exists(exposures_path):
                                with open(exposures_path, 'r', encoding='utf-8') as ef:
                                    exposures = json.load(ef)
                                consensus_context['exposure_found'] = True
                                file_abs = target_path if (target_path and os.path.isabs(target_path)) else str((rag_system_path / (target_path or '')).resolve())
                                for exp in exposures:
                                    if isinstance(exp, dict) and exp.get('source_file') == file_abs:
                                        consensus_context['protocol'] = {
                                            'id': exp.get('id'),
                                            'name': exp.get('name'),
                                            'type': exp.get('type'),
                                            'version': exp.get('version'),
                                            'needs': exp.get('needs'),
                                            'provides': exp.get('provides')
                                        }
                                        break
                        except Exception:
                            pass

                        # è‡ªæ›å…‰åè®®ä¸€è‡´æ€§é—¨ç¦ï¼šowneræˆ–protocolç¼ºå¤±åˆ™è§¦å‘äºŒçº§æŠ¥é”™å¹¶é˜»æ­¢å†™å…¥
                        if enable_consensus:
                            if not consensus_context['owner_component'] or not consensus_context['protocol']:
                                try:
                                    from src.error_reporting import get_error_reporting_service
                                    er = get_error_reporting_service()
                                    er.report_component_error({
                                        "error_id": er.generate_error_id("agent_tool_integration", "ConsensusMissing"),
                                        "level": "component",
                                        "type": "ConsensusMissing",
                                        "message": "å†™å…¥å‰å…±è¯†ä¿¡æ¯ç¼ºå¤±ï¼ˆowner_componentæˆ–protocolï¼‰",
                                        "timestamp": datetime.now().isoformat(),
                                        "component": "agent_tool_integration",
                                        "function": "file_writing",
                                        "file_path": target_path or "",
                                        "line_number": 0,
                                        "stack_trace": "agent_tool_integration.call_tool",
                                        "context": {"consensus_context": consensus_context, "caller_info": caller_info}
                                    })
                                except Exception:
                                    pass
                                return {
                                    'success': False,
                                    'error': 'å†™å…¥å‰å…±è¯†ä¿¡æ¯ç¼ºå¤±ï¼ˆowner_componentæˆ–protocolï¼‰',
                                    'tool': 'file_writing',
                                    'data': {'consensus_context': consensus_context}
                                }

                        # åªè¿”å›å…±è¯†ä¸Šä¸‹æ–‡ï¼ˆä¸å†™å…¥ï¼‰
                        if dry_run:
                            return {
                                'success': True,
                                'data': {'consensus_context': consensus_context, 'message': 'dry_run: æœªæ‰§è¡Œå†™å…¥'},
                                'tool': 'file_writing'
                            }

                        # æ‰§è¡Œå†™å…¥
                        if target_path and content is not None:
                            result_dict = chat_tool.write_to_file(
                                target_path,
                                content,
                                overwrite=overwrite,
                                enable_assessment=parameters.get('enable_assessment', False)
                            )
                            return {
                                'success': result_dict.get('success', False),
                                'data': {
                                    'message': result_dict.get('message', ''),
                                    'consensus_context': consensus_context,
                                    **{k: v for k, v in result_dict.items() if k not in ('message')}
                                },
                                'tool': 'file_writing'
                            }
                        return {
                            'success': False,
                            'error': 'ç¼ºå°‘å¿…è¦å‚æ•° file_path/content',
                            'tool': 'file_writing'
                        }
                    elif tool_name == 'consensus_handshake':
                        # å¹¶è¡Œä»»åŠ¡å…±è¯†æ¡æ‰‹ï¼ˆä¸å†™å…¥ï¼Œä»…ç”Ÿæˆå…±è¯†æ¸…å•ï¼‰
                        task_id = parameters.get('task_id')
                        objective = parameters.get('objective')
                        participants = parameters.get('participants', [])  # [{agent, role, responsibility}]
                        targets = parameters.get('targets', [])  # æ–‡ä»¶/æ¨¡å—é›†åˆ
                        base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
                        graph_path = os.path.join(base_dir, 'data', 'component_graph.json')
                        exposures_path = os.path.join(base_dir, 'self_exposures.json')
                        manifest = {
                            'task_id': task_id,
                            'objective': objective,
                            'participants': participants,
                            'owner_component': None,
                            'depends_on': [],
                            'protocol_version': None,
                            'change_set': [],
                            'lock_strategy': {'mode': 'file_lock', 'scope': targets},
                            'conflict_resolution': {'arbiter': 'system_manager', 'fallback': 'revert-and-queue'},
                            'checklist': ['protocol_version_consistency', 'consensus_context_present', 'equality_assessment_passed']
                        }
                        try:
                            graph = {}
                            exposures = []
                            if os.path.exists(graph_path):
                                with open(graph_path, 'r', encoding='utf-8') as gf:
                                    graph = json.load(gf)
                            if os.path.exists(exposures_path):
                                with open(exposures_path, 'r', encoding='utf-8') as ef:
                                    exposures = json.load(ef)
                            # è§£æç›®æ ‡çš„owner/depends_on/protocol_version
                            for target in targets:
                                file_abs = target if os.path.isabs(target) else str((rag_system_path / target).resolve())
                                file_node_id = f"file:{file_abs}"
                                owner = None
                                for e in (graph.get('edges') or []):
                                    if e.get('relation') == 'contains' and (e.get('target') == file_node_id or str(e.get('target','')).endswith(target)):
                                        owner = e.get('source'); break
                                if owner and not manifest['owner_component']:
                                    manifest['owner_component'] = owner
                                    manifest['depends_on'] = [edge.get('target') for edge in (graph.get('edges') or []) if edge.get('relation') == 'depends_on' and edge.get('source') == owner]
                                # åè®®ç‰ˆæœ¬
                                for exp in exposures:
                                    if isinstance(exp, dict) and exp.get('source_file') == file_abs:
                                        manifest['protocol_version'] = f"{exp.get('id')}@{exp.get('version')}"
                                        break
                                manifest['change_set'].append({'file': target, 'action': 'edit', 'anchors': []})
                        except Exception:
                            pass
                        return {'success': True, 'data': {'consensus_manifest': manifest}, 'tool': 'consensus_handshake'}
                    elif tool_name == 'code_acceptance':
                        # ä»£ç éªŒæ”¶ï¼ˆä¸å†™å…¥ï¼‰ï¼šæ£€æµ‹å¹¶è¡Œå†™æ‰‹çš„ä»£ç æ˜¯å¦ä¸€è‡´ã€æ»¡è¶³æ–¹æ¡ˆ
                        scheme_summary = parameters.get('scheme_summary', '')
                        code_proposals = parameters.get('code_proposals', [])  # [{file_path, content, author}]
                        consensus_manifest = parameters.get('consensus_manifest', {})
                        acceptance = {'accepted': True, 'reasons': [], 'conflicts': [], 'files': []}
                        try:
                            # ç®€å•ä¸€è‡´æ€§è§„åˆ™ï¼šåŒä¸€æ–‡ä»¶ä¸å…è®¸å‡ºç°ä¸åŒå†…å®¹ï¼›å¿…é¡»å­˜åœ¨owner_componentä¸protocol_version
                            if not consensus_manifest.get('owner_component') or not consensus_manifest.get('protocol_version'):
                                acceptance['accepted'] = False
                                acceptance['reasons'].append('ç¼ºå°‘å…±è¯†ä¸Šä¸‹æ–‡ï¼ˆowner_componentæˆ–protocol_versionï¼‰')
                            grouped = {}
                            for p in code_proposals:
                                f = p.get('file_path'); c = (p.get('content') or '').strip()
                                if not f: continue
                                grouped.setdefault(f, set()).add(c)
                            for f, contents in grouped.items():
                                if len(contents) > 1:
                                    acceptance['accepted'] = False
                                    acceptance['conflicts'].append({'file': f, 'versions': len(contents)})
                                else:
                                    acceptance['files'].append({'file': f, 'hash': hash(next(iter(contents)))})
                        except Exception as e:
                            acceptance['accepted'] = False
                            acceptance['reasons'].append(str(e))
                        return {'success': True, 'data': {'acceptance': acceptance}, 'tool': 'code_acceptance'}
                    elif tool_name == 'long_task_workflow':
                        # é•¿æœŸä»»åŠ¡å·¥ä½œæµï¼šè‡ªåŠ¨å…±è¯†æ¡æ‰‹ + ä¸€è‡´æ€§éªŒæ”¶ +ï¼ˆå¯é€‰ï¼‰å†™å…¥
                        task_id = parameters.get('task_id')
                        objective = parameters.get('objective')
                        participants = parameters.get('participants', [])
                        targets = parameters.get('targets', [])
                        proposals = parameters.get('proposals', [])  # [{file_path, content, author}]
                        task_type = parameters.get('task_type', 'code')  # code|content|data
                        overwrite = parameters.get('overwrite', False)
                        enable_assessment = parameters.get('enable_assessment', True)
                        # è‡ªåŠ¨åˆ¤å®šæ˜¯å¦ä¸ºé•¿æœŸä»»åŠ¡ï¼ˆè§„æ¨¡/å¹¶è¡Œåº¦ï¼‰
                        is_long = False
                        try:
                            total_size = sum(len((p.get('content') or '').encode('utf-8')) for p in proposals)
                            is_long = (len(proposals) >= 3) or (len(targets) >= 2) or (total_size > 200_000)
                        except Exception:
                            is_long = True
                        base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
                        graph_path = os.path.join(base_dir, 'data', 'component_graph.json')
                        exposures_path = os.path.join(base_dir, 'self_exposures.json')
                        manifest = {
                            'task_id': task_id,
                            'objective': objective,
                            'participants': participants,
                            'owner_component': None,
                            'depends_on': [],
                            'protocol_version': None,
                            'change_set': [],
                            'lock_strategy': {'mode': 'file_lock', 'scope': targets},
                            'checklist': ['protocol_version_consistency', 'consensus_context_present', 'equality_assessment_passed'],
                            'is_long_task': is_long
                        }
                        # å…±è¯†æ¡æ‰‹
                        try:
                            graph = {}
                            exposures = []
                            if os.path.exists(graph_path):
                                with open(graph_path, 'r', encoding='utf-8') as gf:
                                    graph = json.load(gf)
                            if os.path.exists(exposures_path):
                                with open(exposures_path, 'r', encoding='utf-8') as ef:
                                    exposures = json.load(ef)
                            for target in targets:
                                file_abs = target if os.path.isabs(target) else str((rag_system_path / target).resolve())
                                file_node_id = f"file:{file_abs}"
                                owner = None
                                for e in (graph.get('edges') or []):
                                    if e.get('relation') == 'contains' and (e.get('target') == file_node_id or str(e.get('target','')).endswith(target)):
                                        owner = e.get('source'); break
                                if owner and not manifest['owner_component']:
                                    manifest['owner_component'] = owner
                                    manifest['depends_on'] = [edge.get('target') for edge in (graph.get('edges') or []) if edge.get('relation') == 'depends_on' and edge.get('source') == owner]
                                for exp in exposures:
                                    if isinstance(exp, dict) and exp.get('source_file') == file_abs:
                                        manifest['protocol_version'] = f"{exp.get('id')}@{exp.get('version')}"; break
                                manifest['change_set'].append({'file': target, 'action': 'edit', 'anchors': []})
                        except Exception:
                            pass
                        # ä¸€è‡´æ€§éªŒæ”¶ï¼ˆé€šç”¨ï¼‰
                        acceptance = {'accepted': True, 'reasons': [], 'conflicts': [], 'files': []}
                        try:
                            if not manifest.get('owner_component') or not manifest.get('protocol_version'):
                                acceptance['accepted'] = False
                                acceptance['reasons'].append('ç¼ºå°‘å…±è¯†ä¸Šä¸‹æ–‡ï¼ˆowner_componentæˆ–protocol_versionï¼‰')
                            grouped = {}
                            for p in proposals:
                                f = p.get('file_path') or p.get('target'); c = (p.get('content') or '').strip()
                                if not f: continue
                                grouped.setdefault(f, set()).add(c)
                            for f, contents in grouped.items():
                                if len(contents) > 1:
                                    acceptance['accepted'] = False
                                    acceptance['conflicts'].append({'file': f, 'versions': len(contents)})
                                else:
                                    acceptance['files'].append({'file': f, 'hash': hash(next(iter(contents)))})
                        except Exception as e:
                            acceptance['accepted'] = False
                            acceptance['reasons'].append(str(e))
                        # å†™å…¥ï¼ˆä»…å®ç°å¸ˆï¼Œä¸”éªŒæ”¶é€šè¿‡ï¼‰
                        write_results = []
                        if acceptance['accepted'] and (caller_info or {}).get('agent_type') in ('implementer', 'text_implementer', 'developer'):
                            for p in proposals:
                                fp = p.get('file_path'); ct = p.get('content')
                                if fp and ct is not None:
                                    r = chat_tool.write_to_file(fp, ct, overwrite=overwrite, enable_assessment=enable_assessment)
                                    write_results.append({'file_path': fp, 'success': r.get('success'), 'message': r.get('message')})
                        return {
                            'success': True,
                            'data': {
                                'consensus_manifest': manifest,
                                'acceptance': acceptance,
                                'write_results': write_results
                            },
                            'tool': 'long_task_workflow'
                        }
                    elif tool_name == 'command_line':
                        # å‘½ä»¤è¡Œå·¥å…·ï¼ˆä»èŠå¤©å·¥å…·ç®¡ç†å™¨å§”æ‰˜æ‰§è¡Œï¼‰
                        if 'command' in parameters:
                            timeout = parameters.get('timeout', 30)
                            result = chat_tool.execute_command(parameters['command'], timeout=timeout)
                            return {
                                'success': result['success'],
                                'data': result,
                                'tool': 'command_line'
                            }
                        else:
                            return {
                                'success': False,
                                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: command',
                                'tool': 'command_line'
                            }
                    elif tool_name == 'preference_sync':
                        # åå¥½åŒæ­¥ï¼šæŠŠè®°å¿†ä¸­çš„åå¥½å†™å…¥è®¾ç½®æ–‡ä»¶ï¼ˆdata/user_preferences.jsonï¼‰
                        prefs = parameters.get('preferences', {})
                        base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
                        data_dir = os.path.join(base_dir, 'data')
                        os.makedirs(data_dir, exist_ok=True)
                        target = os.path.join(data_dir, 'user_preferences.json')
                        # è¯»å–ç°æœ‰åå¥½å¹¶æ·±åº¦åˆå¹¶
                        try:
                            existing = {}
                            if os.path.exists(target):
                                with open(target, 'r', encoding='utf-8') as f:
                                    existing = json.load(f) or {}
                            def _deep_merge(a, b):
                                if isinstance(a, dict) and isinstance(b, dict):
                                    r = dict(a or {})
                                    for k, v in b.items():
                                        r[k] = _deep_merge((a or {}).get(k), v) if (a or {}).get(k) is not None else v
                                    return r
                                return b if b is not None else a
                            merged = _deep_merge(existing, prefs)
                            with open(target, 'w', encoding='utf-8') as f:
                                json.dump(merged, f, ensure_ascii=False, indent=2)
                            return {
                                'success': True,
                                'data': {'saved_path': target, 'preferences': merged},
                                'tool': 'preference_sync'
                            }
                        except Exception as e:
                            return {
                                'success': False,
                                'error': f'åå¥½åŒæ­¥å¤±è´¥: {e}',
                                'tool': 'preference_sync'
                            }
                    elif tool_name == 'engineering_ideas_feed':
                        # å·¥ç¨‹å»ºè®®ä¾›ç¨¿ï¼šèšåˆå„æ™ºèƒ½ä½“çš„æ„æ€/ä¼˜åŒ–å»ºè®®æ³¡æ³¡ä¸ºå·¥ç¨‹å¸ˆå‚è€ƒ
                        days = int(parameters.get('days', 30))
                        include_resolved = bool(parameters.get('include_resolved', False))
                        categories = parameters.get('categories', ['æ„æ€', 'ä¼˜åŒ–å»ºè®®'])
                        base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
                        bubbles_root = os.path.join(base_dir, 'data', 'memory_bubbles')
                        output_path = os.path.join(base_dir, 'data', 'engineering_ideas_feed.json')
                        feed = []
                        try:
                            from datetime import timedelta
                            cutoff = datetime.now() - timedelta(days=days)
                            if os.path.isdir(bubbles_root):
                                for agent_id in os.listdir(bubbles_root):
                                    agent_dir = os.path.join(bubbles_root, agent_id)
                                    if not os.path.isdir(agent_dir):
                                        continue
                                    for fname in os.listdir(agent_dir):
                                        if not fname.endswith('.json'):
                                            continue
                                        fpath = os.path.join(agent_dir, fname)
                                        try:
                                            with open(fpath, 'r', encoding='utf-8') as f:
                                                bubble = json.load(f) or {}
                                            cat = bubble.get('category')
                                            if cat not in categories:
                                                continue
                                            status = bubble.get('status')
                                            if not include_resolved and status == 'å·²è§£å†³':
                                                continue
                                            ts = bubble.get('timestamp')
                                            ts_dt = datetime.fromisoformat(ts) if ts else None
                                            if ts_dt and ts_dt < cutoff:
                                                continue
                                            feed.append({
                                                'bubble_id': bubble.get('bubble_id'),
                                                'agent_id': bubble.get('agent_id'),
                                                'timestamp': ts,
                                                'category': cat,
                                                'content': bubble.get('content'),
                                                'context': bubble.get('context', {}),
                                                'priority': bubble.get('priority', 'normal'),
                                                'status': status
                                            })
                                        except Exception:
                                            continue
                            # æ’åºï¼šä¼˜å…ˆçº§+æ—¶é—´
                            priority_order = {'urgent': 0, 'high': 1, 'normal': 2, 'low': 3}
                            feed.sort(key=lambda x: (priority_order.get(x.get('priority','normal'), 2), x.get('timestamp') or ''))
                            # å†™å‡ºä¾›ç¨¿æ–‡ä»¶
                            os.makedirs(os.path.dirname(output_path), exist_ok=True)
                            with open(output_path, 'w', encoding='utf-8') as f:
                                json.dump({'items': feed, 'generated_at': datetime.now().isoformat(), 'window_days': days}, f, ensure_ascii=False, indent=2)
                            return {
                                'success': True,
                                'data': {'count': len(feed), 'output_path': output_path},
                                'tool': 'engineering_ideas_feed'
                            }
                        except Exception as e:
                            return {
                                'success': False,
                                'error': f'å·¥ç¨‹å»ºè®®ä¾›ç¨¿ç”Ÿæˆå¤±è´¥: {e}',
                                'tool': 'engineering_ideas_feed'
                            }
                    elif tool_name == 'memory_retrieval' or tool_name == 'unified_memory_retrieval':
                        # è®°å¿†æ£€ç´¢å·¥å…·
                        if 'query' in parameters:
                            limit = parameters.get('limit', 10)
                            result = chat_tool.search_memories(parameters['query'], limit=limit)
                            return {
                                'success': True,
                                'data': {'memories': result},
                                'tool': 'memory_retrieval'
                            }
                    elif tool_name == 'web_search':
                        # ç½‘ç»œæœç´¢å·¥å…·
                        if 'query' in parameters:
                            num_results = parameters.get('num_results', 5)
                            result = chat_tool.search_web(parameters['query'], num_results=num_results)
                            return {
                                'success': True,
                                'data': {'results': result},
                                'tool': 'web_search'
                            }
                    elif tool_name == 'memory_iteration':
                        # è®°å¿†è¿­ä»£å·¥å…·
                        if 'topic' in parameters:
                            result = chat_tool.complete_memory_iteration(parameters['topic'])
                            return {
                                'success': True,
                                'data': result,
                                'tool': 'memory_iteration'
                            }
                    elif tool_name == 'equality_assessment':
                        # å¹³ç­‰å¾‹è¯„ä¼°å·¥å…·
                        if 'file_path' in parameters and 'content' in parameters:
                            result = chat_tool.assess_write_operation(parameters['file_path'], parameters['content'])
                            return {
                                'success': True,
                                'data': result,
                                'tool': 'equality_assessment'
                            }
                    elif tool_name == 'memory_slicer':
                        # è®°å¿†åˆ‡ç‰‡å·¥å…·ï¼ˆæ”¯æŒæ–‡æœ¬æˆ–æ–‡ä»¶ï¼‰
                        cfg = parameters.get('config', {})
                        metadata = parameters.get('metadata', {})
                        if 'content' in parameters and isinstance(parameters.get('content'), str):
                            result = chat_tool.slice_text(parameters['content'], metadata=metadata, config=cfg)
                            return {
                                'success': True,
                                'data': result,
                                'tool': 'memory_slicer'
                            }
                        elif 'file_path' in parameters:
                            result = chat_tool.slice_file(parameters['file_path'], config=cfg)
                            return {
                                'success': True,
                                'data': result,
                                'tool': 'memory_slicer'
                            }
                        else:
                            return {
                                'success': False,
                                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°ï¼šcontent æˆ– file_path',
                                'tool': 'memory_slicer'
                            }
                    elif tool_name == 'networked_thinking':
                        # ç½‘çŠ¶æ€ç»´å·¥å…·ï¼ˆç›´æ¥è°ƒç”¨å·¥å…·å®ä¾‹ï¼‰
                        if 'input_text' in parameters:
                            context = parameters.get('context', {})
                            # è°ƒç”¨NetworkedThinkingEngine(MeshThoughtEngine)çš„analyze_text_dimensionsæ–¹æ³•
                            engine = chat_tool.tools.get('networked_thinking')
                            if engine and hasattr(engine, 'analyze_text_dimensions'):
                                result = engine.analyze_text_dimensions(parameters['input_text'], context)
                            else:
                                result = {'error': 'ç½‘çŠ¶æ€ç»´å¼•æ“æœªæ­£ç¡®åˆå§‹åŒ–'}
                            return {
                                'success': True,
                                'data': result,
                                'tool': 'networked_thinking'
                            }
                        else:
                            return {
                                'success': False,
                                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: input_text',
                                'tool': 'networked_thinking'
                            }
                    elif tool_name == 'reasoning_engine':
                        # ç†æ€§è®¤çŸ¥å¼•æ“ï¼ˆç›´æ¥è°ƒç”¨å·¥å…·å®ä¾‹ï¼‰
                        if 'premise' in parameters:
                            rules = parameters.get('rules', ['contradiction', 'identity', 'excluded_middle', 'sufficient_reason'])
                            # è°ƒç”¨ReasoningEngineçš„reasonæ–¹æ³•
                            engine = chat_tool.tools.get('reasoning_engine')
                            if engine and hasattr(engine, 'reason'):
                                result = engine.reason(parameters['premise'], context={})
                            else:
                                result = {'error': 'ç†æ€§è®¤çŸ¥å¼•æ“æœªæ­£ç¡®åˆå§‹åŒ–'}
                            return {
                                'success': True,
                                'data': result,
                                'tool': 'reasoning_engine'
                            }
                        else:
                            return {
                                'success': False,
                                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: premise',
                                'tool': 'reasoning_engine'
                            }
                    elif tool_name == 'cognitive_barrier_break':
                        # è®¤çŸ¥ç ´éšœå¼•æ“ï¼ˆç›´æ¥è°ƒç”¨å·¥å…·å®ä¾‹ï¼‰
                        if 'problem' in parameters:
                            barrier_type = parameters.get('barrier_type', 'conceptual')
                            # è°ƒç”¨CognitiveBarrierBreakEngineçš„detect_hallucinationæ–¹æ³•
                            engine = chat_tool.tools.get('cognitive_barrier_break')
                            if engine and hasattr(engine, 'detect_hallucination'):
                                # æ„å»ºç®€å•çš„reasoning_process
                                reasoning_process = {
                                    'reasoning_chain': [{'premise': parameters['problem'], 'conclusion': 'å¾…åˆ†æ'}]
                                }
                                context = {'domain': 'general', 'barrier_type': barrier_type}
                                result = engine.detect_hallucination(parameters['problem'], reasoning_process, context)
                            else:
                                result = {'error': 'è®¤çŸ¥ç ´éšœå¼•æ“æœªæ­£ç¡®åˆå§‹åŒ–'}
                            return {
                                'success': True,
                                'data': result,
                                'tool': 'cognitive_barrier_break'
                            }
                        else:
                            return {
                                'success': False,
                                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: problem',
                                'tool': 'cognitive_barrier_break'
                            }
                    elif tool_name == 'code_index_build':
                        # ä»£ç å®ç°å¸ˆä¸“ç”¨æ•°æ®åº“ç´¢å¼•æ„å»ºï¼ˆä»…å®ç°å¸ˆå…è®¸è§¦å‘ï¼‰
                        caller_type = (caller_info or {}).get('agent_type')
                        if caller_type not in ('implementer', 'developer'):
                            try:
                                from src.error_reporting import get_error_reporting_service
                                er = get_error_reporting_service()
                                er.report_component_error({
                                    "error_id": er.generate_error_id("agent_tool_integration", "CodeIndexBuildDenied"),
                                    "level": "component",
                                    "type": "PermissionDenied",
                                    "message": "ç´¢å¼•æ„å»ºä»…é™å®ç°å¸ˆ/å¼€å‘è€…è§¦å‘",
                                    "timestamp": datetime.now().isoformat(),
                                    "component": "agent_tool_integration",
                                    "function": "code_index_build",
                                    "context": {"caller_info": caller_info}
                                })
                            except Exception:
                                pass
                            return {'success': False, 'error': 'æƒé™ä¸è¶³ï¼šç´¢å¼•æ„å»ºä»…é™å®ç°å¸ˆ/å¼€å‘è€…', 'tool': 'code_index_build', 'data': {'denied_for_role': caller_type}}
                        import sqlite3, hashlib, ast
                        base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
                        data_dir = os.path.join(base_dir, 'data')
                        os.makedirs(data_dir, exist_ok=True)
                        db_path = os.path.join(data_dir, 'code_index_db.sqlite')
                        mode = parameters.get('mode', 'incremental')
                        scope = parameters.get('scope', 'src')
                        target_root = os.path.join(base_dir, scope) if not os.path.isabs(scope) else scope
                        conn = sqlite3.connect(db_path)
                        cur = conn.cursor()
                        # å»ºè¡¨
                        cur.execute("CREATE TABLE IF NOT EXISTS files (file_path TEXT PRIMARY KEY, file_hash TEXT NOT NULL, owner_component TEXT, protocol_version TEXT, last_modified DATETIME NOT NULL)")
                        cur.execute("CREATE TABLE IF NOT EXISTS symbols (symbol_id TEXT PRIMARY KEY, file_path TEXT NOT NULL, symbol_name TEXT NOT NULL, symbol_type TEXT NOT NULL, signature TEXT, docstring TEXT, start_line INTEGER, end_line INTEGER)")
                        cur.execute("CREATE TABLE IF NOT EXISTS relations (source_symbol_id TEXT NOT NULL, relation_type TEXT NOT NULL, target_symbol_id TEXT NOT NULL, PRIMARY KEY (source_symbol_id, relation_type, target_symbol_id))")
                        cur.execute("CREATE TABLE IF NOT EXISTS components (component_id TEXT PRIMARY KEY, name TEXT, depends_on TEXT, provides TEXT)")
                        # ç»„ä»¶åŒæ­¥ï¼ˆè‡ªæ›å…‰ï¼‰
                        try:
                            exposures_path = os.path.join(base_dir, 'self_exposures.json')
                            if os.path.exists(exposures_path):
                                exposures = json.load(open(exposures_path, 'r', encoding='utf-8'))
                                for exp in exposures:
                                    cid = exp.get('id')
                                    if cid:
                                        cur.execute("INSERT OR REPLACE INTO components(component_id,name,depends_on,provides) VALUES (?,?,?,?)", (
                                            cid,
                                            exp.get('name'),
                                            json.dumps(exp.get('needs', {}).get('deps', []), ensure_ascii=False),
                                            json.dumps(exp.get('provides', {}), ensure_ascii=False)
                                        ))
                        except Exception:
                            pass
                        # æ–‡ä»¶éå†
                        indexed_files = 0
                        indexed_symbols = 0
                        indexed_relations = 0
                        for root, dirs, files in os.walk(target_root):
                            # å¿½ç•¥ç¼“å­˜ä¸éæºç ç›®å½•
                            if any(seg in root for seg in ('__pycache__', 'venv', '.git')):
                                continue
                            for fname in files:
                                if not fname.endswith('.py'):
                                    continue
                                fpath = os.path.join(root, fname)
                                try:
                                    # è¯»å–æ–‡ä»¶ä¸hash
                                    with open(fpath, 'r', encoding='utf-8', errors='ignore') as rf:
                                        content = rf.read()
                                    file_hash = hashlib.sha256(content.encode('utf-8', errors='ignore')).hexdigest()
                                    last_modified = datetime.fromtimestamp(os.path.getmtime(fpath)).isoformat()
                                    # å¢é‡åˆ¤æ–­
                                    cur.execute("SELECT file_hash FROM files WHERE file_path=?", (fpath,))
                                    row = cur.fetchone()
                                    if mode == 'incremental' and row and row[0] == file_hash:
                                        continue
                                    # è§£æAST
                                    tree = ast.parse(content)
                                    # æ›´æ–°æ–‡ä»¶è¡¨
                                    cur.execute("INSERT OR REPLACE INTO files(file_path,file_hash,owner_component,protocol_version,last_modified) VALUES (?,?,?,?,?)", (
                                        fpath, file_hash, None, None, last_modified
                                    ))
                                    indexed_files += 1
                                    # æå–ç±»ä¸å‡½æ•°
                                    def make_id(name, start):
                                        return f"{fpath}:{name}:{start}"
                                    for node in ast.walk(tree):
                                        if isinstance(node, ast.FunctionDef):
                                            name = node.name
                                            start = getattr(node, 'lineno', 1)
                                            end = getattr(node, 'end_lineno', start)
                                            doc = ast.get_docstring(node) or ''
                                            sid = make_id(name, start)
                                            cur.execute("INSERT OR REPLACE INTO symbols(symbol_id,file_path,symbol_name,symbol_type,signature,docstring,start_line,end_line) VALUES (?,?,?,?,?,?,?,?)", (
                                                sid, fpath, name, 'function', None, doc, start, end
                                            ))
                                            indexed_symbols += 1
                                            # æå–è°ƒç”¨å…³ç³»
                                            for inner in ast.walk(node):
                                                if isinstance(inner, ast.Call):
                                                    callee = None
                                                    if isinstance(inner.func, ast.Name):
                                                        callee = inner.func.id
                                                    elif isinstance(inner.func, ast.Attribute):
                                                        callee = inner.func.attr
                                                    if callee:
                                                        # ç›®æ ‡ç¬¦å·IDï¼ˆåŒæ–‡ä»¶ï¼Œç²—ç•¥ï¼‰
                                                        target_id = f"{fpath}:{callee}:"  # å‰ç¼€åŒ¹é…ï¼Œåç»­æŸ¥è¯¢ç»†åŒ–
                                                        # ç”±äºæ— ç²¾ç¡®è¡Œå·ï¼Œå…ˆç”¨ç¬¦å·åå”¯ä¸€è¿‘ä¼¼
                                                        cur.execute("INSERT OR REPLACE INTO relations(source_symbol_id,relation_type,target_symbol_id) VALUES (?,?,?)", (
                                                            sid, 'calls', target_id
                                                        ))
                                                        indexed_relations += 1
                                        elif isinstance(node, ast.ClassDef):
                                            name = node.name
                                            start = getattr(node, 'lineno', 1)
                                            end = getattr(node, 'end_lineno', start)
                                            doc = ast.get_docstring(node) or ''
                                            sid = make_id(name, start)
                                            cur.execute("INSERT OR REPLACE INTO symbols(symbol_id,file_path,symbol_name,symbol_type,signature,docstring,start_line,end_line) VALUES (?,?,?,?,?,?,?,?)", (
                                                sid, fpath, name, 'class', None, doc, start, end
                                            ))
                                            indexed_symbols += 1
                                            # ç»§æ‰¿å…³ç³»
                                            for base in (node.bases or []):
                                                try:
                                                    if isinstance(base, ast.Name):
                                                        base_name = base.id
                                                    elif isinstance(base, ast.Attribute):
                                                        base_name = base.attr
                                                    else:
                                                        base_name = None
                                                    if base_name:
                                                        target_id = f"{fpath}:{base_name}:"
                                                        cur.execute("INSERT OR REPLACE INTO relations(source_symbol_id,relation_type,target_symbol_id) VALUES (?,?,?)", (
                                                            sid, 'extends', target_id
                                                        ))
                                                        indexed_relations += 1
                                                except Exception:
                                                    continue
                                except Exception:
                                    continue
                        conn.commit()
                        return {
                            'success': True,
                            'data': {'db_path': db_path, 'indexed_files': indexed_files, 'indexed_symbols': indexed_symbols, 'indexed_relations': indexed_relations, 'mode': mode, 'scope': target_root},
                            'tool': 'code_index_build'
                        }
                    elif tool_name == 'code_symbol_search':
                        # ä»£ç ç¬¦å·æ£€ç´¢ï¼ˆåªè¯»ï¼‰
                        import sqlite3
                        base_dir = str(rag_system_path) if 'rag_system_path' in globals() else '.'
                        db_path = os.path.join(base_dir, 'data', 'code_index_db.sqlite')
                        if not os.path.exists(db_path):
                            return {'success': False, 'error': 'ç´¢å¼•åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ„å»º', 'tool': 'code_symbol_search'}
                        conn = sqlite3.connect(db_path)
                        cur = conn.cursor()
                        query = parameters.get('query', '')
                        filters = parameters.get('filters', {})
                        symbol_type = filters.get('symbol_type')
                        file_filter = filters.get('file_path')
                        limit = int(parameters.get('limit', 50))
                        # åŸºç¡€æŸ¥è¯¢
                        conditions = ["symbol_name LIKE ?"]
                        params = [f"%{query}%"]
                        if symbol_type:
                            conditions.append("symbol_type = ?")
                            params.append(symbol_type)
                        if file_filter:
                            conditions.append("file_path LIKE ?")
                            params.append(f"%{file_filter}%")
                        where_clause = ' AND '.join(conditions)
                        cur.execute(f"SELECT symbol_id,file_path,symbol_name,symbol_type,start_line,end_line FROM symbols WHERE {where_clause} LIMIT ?", (*params, limit))
                        rows = cur.fetchall()
                        results = [{
                            'symbol_id': r[0], 'file_path': r[1], 'symbol_name': r[2], 'symbol_type': r[3], 'start_line': r[4], 'end_line': r[5]
                        } for r in rows]
                        # å…³ç³»å±•å¼€
                        relation = filters.get('relation')
                        if relation in ('calls', 'called_by', 'extends', 'implemented_by') and results:
                            expanded = []
                            for item in results:
                                sid = item['symbol_id']
                                if relation == 'called_by':
                                    cur.execute("SELECT source_symbol_id FROM relations WHERE relation_type='calls' AND target_symbol_id LIKE ?", (sid.split(':')[0] + ':%',))
                                    callers = [row[0] for row in cur.fetchall()]
                                    item['called_by'] = callers
                                else:
                                    cur.execute("SELECT target_symbol_id FROM relations WHERE relation_type=? AND source_symbol_id=?", (relation, sid))
                                    targets = [row[0] for row in cur.fetchall()]
                                    item[relation] = targets
                                expanded.append(item)
                            results = expanded
                        return {'success': True, 'data': {'results': results, 'count': len(results)}, 'tool': 'code_symbol_search'}
                    # å¦‚æœæ²¡æœ‰åŒ¹é…çš„å·¥å…·è°ƒç”¨æ–¹æ³•ï¼Œè¿”å›é”™è¯¯
                    result = {
                        'success': False,
                        'error': f'å·¥å…· {tool_name} çš„è°ƒç”¨å‚æ•°ä¸æ­£ç¡®',
                        'data': {}
                    }
                    duration = (datetime.now() - start_time).total_seconds()
                    self._log_tool_call(tool_name, parameters, result, duration, False, caller_info, usage_intention, active_call)
                    return result
                    
                except Exception as e:
                    result = {
                        'success': False,
                        'error': f'å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}',
                        'data': {}
                    }
                    duration = (datetime.now() - start_time).total_seconds()
                    self._log_tool_call(tool_name, parameters, result, duration, False, caller_info, usage_intention, active_call)
                    return result
        
        # 2. ä»è®¤çŸ¥å¼•æ“å·¥å…·ä¸­æŸ¥æ‰¾
        if tool_name in self.tool_instances:
            try:
                tool_instance = self.tool_instances[tool_name]
                
                # æ ¹æ®å·¥å…·ç±»å‹è°ƒç”¨ç›¸åº”æ–¹æ³•
                if tool_name == 'MeshThoughtEngine':
                    result = self._call_mesh_thought_engine(tool_instance, parameters)
                elif tool_name == 'ReasoningEngine':
                    result = self._call_reasoning_engine(tool_instance, parameters)
                elif tool_name == 'CognitiveBarrierBreakEngine':
                    result = self._call_cognitive_barrier_engine(tool_instance, parameters)
                elif tool_name == 'MemoryReconstructionEngine':
                    result = self._call_memory_reconstruction_engine(tool_instance, parameters)
                elif tool_name == 'MultimodalAlignmentEngine':
                    result = self._call_multimodal_alignment_engine(tool_instance, parameters)
                elif tool_name == 'MultimodalRetrievalEngine':
                    result = self._call_multimodal_retrieval_engine(tool_instance, parameters)
                elif tool_name == 'VisionProcessingEngine':
                    result = self._call_vision_processing_engine(tool_instance, parameters)
                elif tool_name == 'AudioProcessingEngine':
                    result = self._call_audio_processing_engine(tool_instance, parameters)
                elif tool_name == 'MultimodalFusionEngine':
                    result = self._call_multimodal_fusion_engine(tool_instance, parameters)
                elif tool_name == 'AbductiveReasoningEngine':
                    result = self._call_abductive_reasoning_engine(tool_instance, parameters)
                elif tool_name == 'HierarchicalLearningEngine':
                    result = self._call_hierarchical_learning_engine(tool_instance, parameters)
                else:
                    result = {'success': False, 'error': 'æœªçŸ¥å·¥å…·ç±»å‹', 'data': {}}
                
                duration = (datetime.now() - start_time).total_seconds()
                self._log_tool_call(tool_name, parameters, result, duration, result.get('success', False), caller_info, usage_intention, active_call)
                return result
                
            except Exception as e:
                result = {
                    'success': False,
                    'error': f'å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}',
                    'data': {}
                }
                duration = (datetime.now() - start_time).total_seconds()
                self._log_tool_call(tool_name, parameters, result, duration, False, caller_info, usage_intention, active_call)
                return result
        
        # 3. å·¥å…·æœªæ‰¾åˆ°
        result = {
            'success': False,
            'error': f'å·¥å…· {tool_name} æœªæ‰¾åˆ°æˆ–æœªåˆå§‹åŒ–',
            'data': {}
        }
        duration = (datetime.now() - start_time).total_seconds()
        self._log_tool_call(tool_name, parameters, result, duration, False, caller_info, usage_intention, active_call)
        return result
    
    def _call_mesh_thought_engine(self, engine, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨ç½‘çŠ¶æ€ç»´å¼•æ“"""
        operation = parameters.get('operation', 'analyze')
        input_text = parameters.get('input_text', '')
        context = parameters.get('context', {})
        
        if operation == 'analyze':
            # åˆ†ææ–‡æœ¬å¹¶æ„å»ºæ€ç»´ç½‘ç»œ
            result = engine.analyze_text(input_text, context)
        elif operation == 'search':
            # æœç´¢ç›¸å…³æ€ç»´èŠ‚ç‚¹
            result = engine.search_related_thoughts(input_text, context)
        elif operation == 'associate':
            # æ„å»ºæ€ç»´å…³è”
            result = engine.build_associations(input_text, context)
        else:
            result = {'error': f'æœªçŸ¥æ“ä½œ: {operation}'}
        
        return {
            'success': True,
            'data': result,
            'tool': 'MeshThoughtEngine',
            'operation': operation
        }
    
    def _call_reasoning_engine(self, engine, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨ç†æ€§è®¤çŸ¥å¼•æ“"""
        premise = parameters.get('premise', {})
        rules = parameters.get('rules', ['contradiction', 'identity', 'excluded_middle', 'sufficient_reason'])
        
        # åº”ç”¨æ¨ç†è§„åˆ™
        reasoning_results = {}
        for rule_name in rules:
            if hasattr(engine, f'apply_{rule_name}_rule'):
                rule_method = getattr(engine, f'apply_{rule_name}_rule')
                satisfaction, explanation = rule_method(premise, {})
                reasoning_results[rule_name] = {
                    'satisfaction': satisfaction,
                    'explanation': explanation
                }
        
        # è®¡ç®—æ€»ä½“ç½®ä¿¡åº¦
        overall_confidence = sum(r['satisfaction'] for r in reasoning_results.values()) / len(reasoning_results) if reasoning_results else 0
        
        return {
            'success': True,
            'data': {
                'reasoning_results': reasoning_results,
                'overall_confidence': overall_confidence
            },
            'tool': 'ReasoningEngine'
        }
    
    def _call_cognitive_barrier_engine(self, engine, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨è®¤çŸ¥ç ´éšœå¼•æ“"""
        problem = parameters.get('problem', '')
        barrier_type = parameters.get('barrier_type', 'conceptual')
        
        # åˆ†æè®¤çŸ¥éšœç¢
        barrier_analysis = engine.analyze_barrier(problem, barrier_type)
        
        # ç”Ÿæˆçªç ´æ–¹æ¡ˆ
        breakthrough_ideas = engine.generate_breakthrough_ideas(barrier_analysis)
        
        return {
            'success': True,
            'data': {
                'barrier_analysis': barrier_analysis,
                'breakthrough_ideas': breakthrough_ideas
            },
            'tool': 'CognitiveBarrierBreakEngine'
        }
    
    def _call_memory_reconstruction_engine(self, engine, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨è®°å¿†é‡æ„å¼•æ“"""
        memory_data = parameters.get('memory_data', {})
        reconstruction_type = parameters.get('reconstruction_type', 'hierarchical')
        
        # é‡æ„è®°å¿†
        reconstructed_memory = engine.reconstruct_memory(memory_data, reconstruction_type)
        
        return {
            'success': True,
            'data': {
                'reconstructed_memory': reconstructed_memory,
                'reconstruction_type': reconstruction_type
            },
            'tool': 'MemoryReconstructionEngine'
        }

    def _call_abductive_reasoning_engine(self, engine, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨æº¯å› æ¨ç†å¼•æ“"""
        operation = parameters.get('operation', 'generate_hypotheses')
        
        if operation == 'generate_hypotheses':
            # ç”Ÿæˆå‡è®¾
            observations = parameters.get('observations', [])
            background_knowledge = parameters.get('background_knowledge', {})
            
            result = engine.call('generate_hypotheses', {
                'observations': observations,
                'background_knowledge': background_knowledge
            })
        elif operation == 'evaluate_hypotheses':
            # è¯„ä¼°å‡è®¾
            hypotheses = parameters.get('hypotheses', [])
            observations = parameters.get('observations', [])
            
            result = engine.call('evaluate_hypotheses', {
                'hypotheses': hypotheses,
                'observations': observations
            })
        elif operation == 'select_best_hypothesis':
            # é€‰æ‹©æœ€ä½³å‡è®¾
            hypotheses_evaluations = parameters.get('hypotheses_evaluations', [])
            
            result = engine.call('select_best_hypothesis', {
                'hypotheses_evaluations': hypotheses_evaluations
            })
        else:
            result = {'success': False, 'error': f'æœªçŸ¥æ“ä½œ: {operation}'}
        
        return result

    def _call_hierarchical_learning_engine(self, engine, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨åˆ†å±‚å­¦ä¹ å¼•æ“"""
        operation = parameters.get('operation', 'learn')
        
        if operation == 'learn':
            # é€šç”¨å­¦ä¹ æ–¹æ³•
            learning_mode = parameters.get('learning_mode', 'supervised')
            data = parameters.get('data')
            experience = parameters.get('experience')
            
            result = engine.call('learn', {
                'learning_mode': learning_mode,
                'data': data,
                'experience': experience
            })
        elif operation == 'supervised_learning':
            # ç›‘ç£å­¦ä¹ 
            training_data = parameters.get('training_data', [])
            result = engine.call('supervised_learning', {
                'training_data': training_data
            })
        elif operation == 'unsupervised_learning':
            # æ— ç›‘ç£å­¦ä¹ 
            unlabeled_data = parameters.get('unlabeled_data', [])
            result = engine.call('unsupervised_learning', {
                'unlabeled_data': unlabeled_data
            })
        elif operation == 'reinforcement_learning':
            # å¼ºåŒ–å­¦ä¹ 
            experience_data = parameters.get('experience', {})
            result = engine.call('reinforcement_learning', {
                'experience': experience_data
            })
        elif operation == 'build_hierarchy':
            # æ„å»ºçŸ¥è¯†å±‚æ¬¡
            result = engine.call('build_hierarchy', {})
        elif operation == 'consolidate_knowledge':
            # çŸ¥è¯†å·©å›º
            result = engine.call('consolidate_knowledge', {})
        else:
            result = {'success': False, 'error': f'æœªçŸ¥æ“ä½œ: {operation}'}
        
        return result

    def register_tool(self, tool_name: str, tool_description: str = "", tool_usage: str = "") -> bool:
        """æ³¨å†Œå·¥å…·(å…¼å®¹æ€§æ–¹æ³•,å®é™…å·¥å…·åœ¨åˆå§‹åŒ–æ—¶å·²æ³¨å†Œ)"""
        # è¿™ä¸ªæ–¹æ³•ä¸»è¦æ˜¯ä¸ºäº†å…¼å®¹base_agent.pyä¸­çš„è°ƒç”¨
        # å®é™…å·¥å…·åœ¨_initialize_toolsæ–¹æ³•ä¸­å·²ç»åˆå§‹åŒ–
        # âœ… æ˜ç¡®æ—¥å¿—å«ä¹‰ï¼šè¿™æ˜¯æ³¨å†Œè¯·æ±‚ï¼Œå®é™…å·¥å…·å·²åœ¨åˆå§‹åŒ–æ—¶åŠ è½½
        logger.info(f"âœ… å·¥å…·æ³¨å†ŒæˆåŠŸ: {tool_name} - {tool_description}")
        return True
    
    def get_available_tools(self) -> List[str]:
        """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
        return list(self.tool_instances.keys())
    
    def get_tool(self, tool_name: str):
        """è·å–æŒ‡å®šå·¥å…·å®ä¾‹ï¼ˆæ”¯æŒæ‡’åŠ è½½ï¼‰"""
        # ä¼˜å…ˆä»èŠå¤©å·¥å…·ç®¡ç†å™¨ä¸­è·å–é«˜é¢‘æ ¸å¿ƒå·¥å…·
        if self.chat_tool_manager:
            chat_tool = self.chat_tool_manager.get_tool(tool_name)
            if chat_tool:
                return chat_tool
        
        # ğŸ”¥ ä»å·²åŠ è½½çš„å·¥å…·ä¸­æŸ¥æ‰¾
        if tool_name in self.tool_instances:
            return self.tool_instances[tool_name]
        
        # ğŸ”¥ å°è¯•æ‡’åŠ è½½é«˜çº§å·¥å…·
        if self._lazy_load_tool(tool_name):
            return self.tool_instances[tool_name]
        
        # å°è¯•æ˜ å°„å·¥å…·åç§°
        tool_mapping = {
            'memory_retrieval': 'memory_retrieval',
            'file_reading': 'file_reading', 
            'file_writing': 'file_writing',
            'web_search': 'web_search',
            'memory_iteration': 'memory_iteration',
            'command_line': 'command_line',
            'equality_assessment': 'equality_assessment',
            'memory_slicer': 'memory_slicer',
            'networked_thinking': 'networked_thinking',
            'reasoning_engine': 'reasoning_engine',
            'cognitive_barrier_break': 'cognitive_barrier_break'
        }
        
        mapped_name = tool_mapping.get(tool_name)
        if mapped_name and self.chat_tool_manager:
            return self.chat_tool_manager.get_tool(mapped_name)
        
        return None
    
    def get_tool_status(self) -> Dict[str, Dict[str, Any]]:
        """è·å–å·¥å…·çŠ¶æ€ä¿¡æ¯"""
        tool_status = {}
        
        # ä»èŠå¤©å·¥å…·ç®¡ç†å™¨è·å–é«˜é¢‘æ ¸å¿ƒå·¥å…·çŠ¶æ€
        if self.chat_tool_manager:
            core_tools = ['file_reading', 'file_writing', 'command_line', 'memory_retrieval', 
                         'web_search', 'memory_iteration', 'equality_assessment', 'memory_slicer',
                         'networked_thinking', 'reasoning_engine', 'cognitive_barrier_break']
            
            for tool_name in core_tools:
                tool_status[tool_name] = {
                    'available': True,
                    'type': 'core_tool',
                    'module': 'tools.chat_tools',
                    'description': f'{tool_name}å·¥å…·'
                }
        
        # æ·»åŠ è®¤çŸ¥å¼•æ“å·¥å…·çŠ¶æ€
        for tool_name in self.tool_instances:
            tool_status[tool_name] = {
                'available': True,
                'type': 'cognitive_engine',
                'module': 'src',
                'description': f'{tool_name}è®¤çŸ¥å¼•æ“'
            }
        
        return tool_status
    
    def _call_multimodal_alignment_engine(self, engine, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨å¤šæ¨¡æ€å¯¹é½å¼•æ“"""
        operation = parameters.get('operation', 'align')
        
        if operation == 'align':
            # å¤šæ¨¡æ€å¯¹é½
            modality1 = parameters.get('modality1', {})
            modality2 = parameters.get('modality2', {})
            alignment_type = parameters.get('alignment_type', 'semantic')
            
            result = engine.call('align', {
                'modality1': modality1,
                'modality2': modality2,
                'alignment_type': alignment_type
            })
        elif operation == 'analyze_alignment':
            # åˆ†æå¯¹é½è´¨é‡
            alignment_result = parameters.get('alignment_result', {})
            result = engine.call('analyze_alignment', {
                'alignment_result': alignment_result
            })
        else:
            result = {'success': False, 'error': f'æœªçŸ¥æ“ä½œ: {operation}'}
        
        return result
    
    def _call_multimodal_retrieval_engine(self, engine, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨å¤šæ¨¡æ€æ£€ç´¢å¼•æ“"""
        operation = parameters.get('operation', 'retrieve')
        
        if operation == 'retrieve':
            # è·¨æ¨¡æ€æ£€ç´¢
            query = parameters.get('query', {})
            modality = parameters.get('modality', 'text')
            top_k = parameters.get('top_k', 10)
            
            result = engine.call('retrieve', {
                'query': query,
                'modality': modality,
                'top_k': top_k
            })
        elif operation == 'index':
            # ç´¢å¼•å¤šæ¨¡æ€æ•°æ®
            data = parameters.get('data', {})
            modality = parameters.get('modality', 'text')
            
            result = engine.call('index', {
                'data': data,
                'modality': modality
            })
        else:
            result = {'success': False, 'error': f'æœªçŸ¥æ“ä½œ: {operation}'}
        
        return result
    
    def _call_vision_processing_engine(self, engine, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨è§†è§‰å¤„ç†å¼•æ“"""
        operation = parameters.get('operation', 'analyze_image')
        
        if operation == 'analyze_image':
            # åˆ†æå›¾åƒ
            image_path = parameters.get('image_path')
            base64_data = parameters.get('base64_data')
            
            result = engine.call('analyze_image', {
                'image_path': image_path,
                'base64_data': base64_data
            })
        elif operation == 'extract_features':
            # æå–å›¾åƒç‰¹å¾
            image_path = parameters.get('image_path')
            base64_data = parameters.get('base64_data')
            
            result = engine.call('extract_features', {
                'image_path': image_path,
                'base64_data': base64_data
            })
        elif operation == 'detect_objects':
            # æ£€æµ‹å›¾åƒå¯¹è±¡
            image_path = parameters.get('image_path')
            base64_data = parameters.get('base64_data')
            
            result = engine.call('detect_objects', {
                'image_path': image_path,
                'base64_data': base64_data
            })
        else:
            result = {'success': False, 'error': f'æœªçŸ¥æ“ä½œ: {operation}'}
        
        return result
    
    def _call_audio_processing_engine(self, engine, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨éŸ³é¢‘å¤„ç†å¼•æ“"""
        operation = parameters.get('operation', 'analyze_audio')
        
        if operation == 'analyze_audio':
            # åˆ†æéŸ³é¢‘
            audio_path = parameters.get('audio_path')
            base64_data = parameters.get('base64_data')
            
            result = engine.call('analyze_audio', {
                'audio_path': audio_path,
                'base64_data': base64_data
            })
        elif operation == 'extract_features':
            # æå–éŸ³é¢‘ç‰¹å¾
            audio_path = parameters.get('audio_path')
            base64_data = parameters.get('base64_data')
            
            result = engine.call('extract_features', {
                'audio_path': audio_path,
                'base64_data': base64_data
            })
        else:
            result = {'success': False, 'error': f'æœªçŸ¥æ“ä½œ: {operation}'}
        
        return result

