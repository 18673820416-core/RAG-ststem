# @self-expose: {"id": "llm_client_enhanced", "name": "Llm Client Enhanced", "type": "component", "version": "1.0.0", "needs": {"deps": [], "resources": []}, "provides": {"capabilities": ["Llm Client EnhancedåŠŸèƒ½"]}}
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆLLMå®¢æˆ·ç«¯
é›†æˆAPIå¯†é’¥ç®¡ç†åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†
æ¥æºï¼šç”¨æˆ·å¯¹è¯ä¸­æåˆ°çš„APIå¯†é’¥ä¿å­˜éœ€æ±‚ï¼Œç”¨äºæœªæ¥èŠå¤©æœºå™¨äººè°ƒç”¨LLM
"""

import json
import requests
import time
from typing import List, Dict, Optional, Any
from pathlib import Path

# å¯¼å…¥é…ç½®
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.system_config import api_key_manager, API_ENDPOINTS, LLM_CONFIG

class LLMClientEnhanced:
    """å¢å¼ºç‰ˆLLMå®¢æˆ·ç«¯ï¼ˆæ”¯æŒå¤šæœåŠ¡å•†è‡ªåŠ¨å›é€€ï¼‰
    
    å½“å‰é…ç½®ï¼šåƒé—®ï¼ˆä¸»ï¼‰+ DEEPSEEK-V3.2ï¼ˆå¤‡ï¼‰
    æœ€æ–°ï¼šDeepSeek-V3.2-Speciale (2025.12.01å‘å¸ƒ)
    - æ¨ç†èƒ½åŠ›ï¼šè¶…è¶ŠGPT-5ï¼ŒæŒå¹³Gemini-3.0-Pro
    - ç«èµ›æˆç»©ï¼šIMOé‡‘ç‰Œã€IOIé‡‘ç‰Œ
    - å¼€æºåè®®ï¼šMITï¼ˆå›½äº§AIç¬¬ä¸€æ¬¡å…¨é¢è¶…è¶Šå›½å¤–é—­æºæ¨¡å‹ï¼‰
    
    æ¶æ„æ”¯æŒï¼šå¯æ‰©å±•åˆ°ä»»æ„LLMæœåŠ¡å•†ï¼ˆå›½å†…å¤–ä¸é™ï¼‰
    æœªæ¥æ¼”åŒ–ï¼šæ ¹æ®æŠ€æœ¯è¶‹åŠ¿è°ƒæ•´ï¼Œä¸é¢„è®¾å›ºå®šé€‰å‹
    """
    
    def __init__(self, provider: str = None, enable_fallback: bool = True):
        self.provider = provider or LLM_CONFIG["default_provider"]
        self.timeout = LLM_CONFIG["timeout"]
        self.max_retries = LLM_CONFIG["max_retries"]
        self.temperature = LLM_CONFIG["temperature"]
        self.max_tokens = LLM_CONFIG["max_tokens"]
        self.enable_fallback = enable_fallback
        
        # è·å–APIå¯†é’¥
        self.api_key = api_key_manager.get_key(self.provider)
        if not self.api_key:
            # å¦‚æœå¯ç”¨å›é€€ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–å¯ç”¨æœåŠ¡å•†
            if self.enable_fallback:
                available = self.get_available_providers()
                if available:
                    self.provider = available[0]
                    self.api_key = api_key_manager.get_key(self.provider)
                    print(f"âš ï¸ é»˜è®¤æœåŠ¡å•† {provider or LLM_CONFIG['default_provider']} æœªé…ç½®ï¼Œåˆ‡æ¢åˆ° {self.provider}")
                else:
                    raise ValueError(f"æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„APIå¯†é’¥ï¼Œè¯·å…ˆä½¿ç”¨api_key_tool.pyæ·»åŠ ")
            else:
                raise ValueError(f"æœªæ‰¾åˆ° {self.provider} çš„APIå¯†é’¥ï¼Œè¯·å…ˆä½¿ç”¨api_key_tool.pyæ·»åŠ ")
        
        # è®¾ç½®APIç«¯ç‚¹
        self.endpoint = API_ENDPOINTS.get(self.provider)
        if not self.endpoint:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {self.provider}")
    
    def chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Optional[str]:
        """èŠå¤©è¡¥å…¨æ¥å£ï¼ˆæ”¯æŒè‡ªåŠ¨å›é€€ï¼‰"""
        
        # åˆå¹¶é…ç½®å‚æ•°
        temperature = kwargs.get('temperature', self.temperature)
        max_tokens = kwargs.get('max_tokens', self.max_tokens)
        
        # å°è¯•ä½¿ç”¨å½“å‰provider
        result = self._try_provider(self.provider, messages, temperature, max_tokens)
        
        # å¦‚æœå¤±è´¥ä¸”å¯ç”¨å›é€€ï¼Œå°è¯•å…¶ä»–æœåŠ¡å•†
        if result is None and self.enable_fallback:
            available_providers = self.get_available_providers()
            for fallback_provider in available_providers:
                if fallback_provider == self.provider:
                    continue  # è·³è¿‡å½“å‰provider
                
                print(f"âš ï¸ {self.provider} è°ƒç”¨å¤±è´¥ï¼Œåˆ‡æ¢åˆ° {fallback_provider}...")
                result = self._try_provider(fallback_provider, messages, temperature, max_tokens)
                
                if result is not None:
                    print(f"âœ… {fallback_provider} è°ƒç”¨æˆåŠŸ")
                    # æ›´æ–°å½“å‰providerä¸ºæˆåŠŸçš„fallback
                    self.provider = fallback_provider
                    self.api_key = api_key_manager.get_key(fallback_provider)
                    self.endpoint = API_ENDPOINTS.get(fallback_provider)
                    break
        
        return result
    
    def _try_provider(self, provider: str, messages: List[Dict[str, str]], 
                      temperature: float, max_tokens: int) -> Optional[str]:
        """å°è¯•ä½¿ç”¨æŒ‡å®šproviderè°ƒç”¨LLM"""
        try:
            # ä¸´æ—¶è®¾ç½®providerç›¸å…³é…ç½®
            original_provider = self.provider
            original_key = self.api_key
            original_endpoint = self.endpoint
            
            self.provider = provider
            self.api_key = api_key_manager.get_key(provider)
            self.endpoint = API_ENDPOINTS.get(provider)
            
            if not self.api_key or not self.endpoint:
                return None
            
            # æ ¹æ®æä¾›å•†æ„å»ºè¯·æ±‚
            if provider == "deepseek":
                result = self._deepseek_chat(messages, temperature, max_tokens)
            elif provider == "openai":
                result = self._openai_chat(messages, temperature, max_tokens)
            elif provider == "anthropic":
                result = self._anthropic_chat(messages, temperature, max_tokens)
            elif provider == "google":
                result = self._google_chat(messages, temperature, max_tokens)
            elif provider == "qianwen":
                result = self._qianwen_chat(messages, temperature, max_tokens)
            else:
                result = None
            
            # æ¢å¤åŸå§‹é…ç½®ï¼ˆå¦‚æœå¤±è´¥ï¼‰
            if result is None:
                self.provider = original_provider
                self.api_key = original_key
                self.endpoint = original_endpoint
            
            return result
            
        except Exception as e:
            print(f"Provider {provider} è°ƒç”¨å¼‚å¸¸: {e}")
            return None
    
    def _deepseek_chat(self, messages: List[Dict[str, str]], temperature: float, max_tokens: int) -> Optional[str]:
        """DeepSeekèŠå¤©æ¥å£"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        return self._make_request(headers, data)
    
    def _openai_chat(self, messages: List[Dict[str, str]], temperature: float, max_tokens: int) -> Optional[str]:
        """OpenAIèŠå¤©æ¥å£"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        data = {
            "model": "gpt-3.5-turbo",
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        
        return self._make_request(headers, data)
    
    def _anthropic_chat(self, messages: List[Dict[str, str]], temperature: float, max_tokens: int) -> Optional[str]:
        """Anthropic ClaudeèŠå¤©æ¥å£"""
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01"
        }
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºClaudeæ ¼å¼
        system_message = ""
        claude_messages = []
        
        for msg in messages:
            if msg["role"] == "system":
                system_message = msg["content"]
            else:
                claude_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
        
        data = {
            "model": "claude-3-sonnet-20240229",
            "messages": claude_messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        if system_message:
            data["system"] = system_message
        
        return self._make_request(headers, data, response_key="content")
    
    def _google_chat(self, messages: List[Dict[str, str]], temperature: float, max_tokens: int) -> Optional[str]:
        """Google AIèŠå¤©æ¥å£"""
        headers = {
            "Content-Type": "application/json"
        }
        
        # æ„å»ºGoogle AIæ ¼å¼
        contents = []
        for msg in messages:
            if msg["role"] == "user":
                contents.append({
                    "parts": [{"text": msg["content"]}],
                    "role": "user"
                })
            elif msg["role"] == "assistant":
                contents.append({
                    "parts": [{"text": msg["content"]}],
                    "role": "model"
                })
        
        data = {
            "contents": contents,
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens
            }
        }
        
        # Google AIä½¿ç”¨APIå¯†é’¥ä½œä¸ºæŸ¥è¯¢å‚æ•°
        endpoint = f"{self.endpoint}?key={self.api_key}"
        return self._make_request(headers, data, endpoint=endpoint, response_key="candidates")
    
    def _qianwen_chat(self, messages: List[Dict[str, str]], temperature: float, max_tokens: int) -> Optional[str]:
        """å­—èŠ‚è·³åŠ¨åƒé—®èŠå¤©æ¥å£"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        data = {
            "model": "qwen-turbo",
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        return self._make_request(headers, data)
    
    def _make_request(self, headers: Dict[str, str], data: Dict[str, Any], 
                     endpoint: str = None, response_key: str = "choices") -> Optional[str]:
        """å‘é€HTTPè¯·æ±‚"""
        
        url = endpoint or self.endpoint
        
        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    url,
                    headers=headers,
                    json=data,
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # æ ¹æ®ä¸åŒæä¾›å•†çš„å“åº”æ ¼å¼æå–æ–‡æœ¬
                    if self.provider == "anthropic":
                        return result.get("content", [{}])[0].get("text", "")
                    elif self.provider == "google":
                        candidates = result.get(response_key, [])
                        if candidates:
                            return candidates[0].get("content", {}).get("parts", [{}])[0].get("text", "")
                        return ""
                    else:
                        choices = result.get(response_key, [])
                        if choices:
                            return choices[0].get("message", {}).get("content", "")
                        return ""
                
                elif response.status_code == 429:  # é™æµ
                    wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                    print(f"è¯·æ±‚è¢«é™æµï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                
                else:
                    error_text = response.text[:200] if len(response.text) > 200 else response.text
                    print(f"âŒ {self.provider} APIè¯·æ±‚å¤±è´¥: {response.status_code} - {error_text}")
                    if attempt == self.max_retries - 1:
                        return None
                    time.sleep(1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
                    
            except requests.exceptions.Timeout:
                print(f"â±ï¸ {self.provider} è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries})")
                if attempt == self.max_retries - 1:
                    return None
                time.sleep(1)
                
            except requests.exceptions.ConnectionError as e:
                print(f"ğŸ”Œ {self.provider} è¿æ¥é”™è¯¯: {e}")
                if attempt == self.max_retries - 1:
                    return None
                time.sleep(2)
                
            except Exception as e:
                print(f"âŒ {self.provider} è¯·æ±‚å¼‚å¸¸: {e}")
                if attempt == self.max_retries - 1:
                    return None
                time.sleep(1)
        
        return None
    
    def get_available_providers(self) -> List[str]:
        """è·å–å·²é…ç½®APIå¯†é’¥çš„å¯ç”¨æä¾›å•†"""
        available = []
        for provider in API_ENDPOINTS.keys():
            if api_key_manager.get_key(provider):
                available.append(provider)
        return available
    
    def slice_text_with_llm(self, text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ–‡æœ¬åˆ‡ç‰‡"""
        import json
        import logging
        logger = logging.getLogger(__name__)
        
        # æ„å»ºåˆ‡ç‰‡æç¤ºè¯
        source_info = metadata.get('source', 'æœªçŸ¥æ¥æº')
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ™ºèƒ½è¯­ä¹‰åˆ‡ç‰‡ï¼Œç¡®ä¿æ¯ä¸ªåˆ‡ç‰‡è¯­ä¹‰å®Œæ•´ä¸”å¤§å°é€‚ä¸­ï¼š

æ–‡æœ¬æ¥æºï¼š{source_info}
æ–‡æœ¬å†…å®¹ï¼š
{text}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚è¿›è¡Œåˆ‡ç‰‡ï¼š
1. ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Œä¸è¦åœ¨å¥å­ä¸­é—´åˆ‡æ–­
2. æ¯ä¸ªåˆ‡ç‰‡å¤§å°å»ºè®®åœ¨100-2000å­—ç¬¦ä¹‹é—´
3. è¯†åˆ«è¯é¢˜è½¬æ¢ç‚¹ä½œä¸ºåˆ‡ç‰‡çš„è¾¹ç•Œ
4. å¯¹äºå¯¹è¯æ–‡æœ¬ï¼ŒæŒ‰å¯¹è¯è½®æ¬¡è¿›è¡Œåˆ‡ç‰‡
5. è¾“å‡ºæ ¼å¼ä¸ºJSONåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
   - content: åˆ‡ç‰‡å†…å®¹
   - boundary_reason: è¾¹ç•Œåˆ’åˆ†ç†ç”±
   - quality_score: åˆ‡ç‰‡è´¨é‡è¯„åˆ†(0-1)

è¯·è¾“å‡ºJSONæ ¼å¼çš„åˆ‡ç‰‡ç»“æœï¼š"""
        
        try:
            # è°ƒç”¨LLM API
            response = self.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000
            )
            
            # è§£æLLMå“åº”
            slices_data = json.loads(response)
            
            slices = []
            for i, slice_info in enumerate(slices_data):
                slice_content = slice_info.get('content', '').strip()
                if not slice_content:
                    continue
                    
                slices.append({
                    "content": slice_content,
                    "slice_id": f"llm_slice_{i}",
                    "slice_size": len(slice_content),
                    "semantic_quality": slice_info.get('quality_score', 0.5),
                    "metadata": metadata.copy(),
                    "slice_method": "llm",
                    "boundary_reason": slice_info.get('boundary_reason', 'LLMæ™ºèƒ½åˆ’åˆ†')
                })
            
            logger.info(f"LLMåˆ‡ç‰‡æˆåŠŸï¼Œç”Ÿæˆ {len(slices)} ä¸ªåˆ‡ç‰‡")
            return slices
            
        except json.JSONDecodeError as e:
            logger.error(f"è§£æLLMå“åº”JSONå¤±è´¥: {e}")
            return []
        except Exception as e:
            logger.error(f"LLMåˆ‡ç‰‡å¤±è´¥: {e}")
            return []

def test_llm_client():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯"""
    try:
        # è·å–å¯ç”¨çš„æä¾›å•†
        available_providers = []
        for provider in API_ENDPOINTS.keys():
            if api_key_manager.get_key(provider):
                available_providers.append(provider)
        
        if not available_providers:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„APIå¯†é’¥")
            print("è¯·å…ˆä½¿ç”¨ tools/api_key_tool.py æ·»åŠ APIå¯†é’¥")
            return
        
        print("å¯ç”¨çš„LLMæä¾›å•†:")
        for provider in available_providers:
            print(f"  - {provider}")
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æä¾›å•†è¿›è¡Œæµ‹è¯•
        provider = available_providers[0]
        print(f"\nä½¿ç”¨ {provider} è¿›è¡Œæµ‹è¯•...")
        
        client = LLMClientEnhanced(provider)
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
        ]
        
        response = client.chat_completion(messages)
        if response:
            print(f"âœ… æµ‹è¯•æˆåŠŸï¼å“åº”: {response}")
        else:
            print("âŒ æµ‹è¯•å¤±è´¥")
            
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    test_llm_client()