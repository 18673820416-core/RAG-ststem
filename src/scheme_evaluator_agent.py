#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ–¹æ¡ˆè¯„ä¼°å¸ˆæ™ºèƒ½ä½“ - åŸºäºå¹³ç­‰å¾‹è¯„ä¼°çš„RAGæ–¹æ¡ˆè¯„ä¼°åŠ©æ‰‹
å¼€å‘æç¤ºè¯æ¥æºï¼šç”¨æˆ·å»ºè®®ç»Ÿä¸€æ™ºèƒ½ä½“æ¨¡æ¿ï¼Œå°†æç¤ºè¯å¤–éƒ¨åŒ–
"""
# @self-expose: {"id": "scheme_evaluator_agent", "name": "Scheme Evaluator Agent", "type": "agent", "version": "1.0.0", "needs": {"deps": ["base_agent"], "resources": []}, "provides": {"capabilities": ["æ–¹æ¡ˆè¯„ä¼°", "å¹³ç­‰å¾‹è¯„ä¼°", "ç³»ç»Ÿåˆ†æ", "å‚ä¸è€…è¯„åˆ†"], "methods": {"process_user_query": {"signature": "(query: str) -> Dict[str, Any]", "description": "å¤„ç†ç”¨æˆ·æŸ¥è¯¢"}}}}

import os
import json
import logging
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
from datetime import datetime

from src.base_agent import BaseAgent
from src.equality_law_evaluator import EqualityLawEvaluator, EvaluationConfig
from src.llm_client_enhanced import LLMClientEnhanced
from config.api_keys import api_key_manager
from tools.memory_slicer_tool import MemorySlicerTool

logger = logging.getLogger(__name__)

class SchemeEvaluatorAgent(BaseAgent):
    """æ–¹æ¡ˆè¯„ä¼°å¸ˆæ™ºèƒ½ä½“ - åŸºäºå¹³ç­‰å¾‹è¯„ä¼°çš„RAGæ–¹æ¡ˆè¯„ä¼°åŠ©æ‰‹"""
    
    def __init__(self, agent_id: str = "evaluator_001"):
        """åˆå§‹åŒ–æ–¹æ¡ˆè¯„ä¼°å¸ˆæ™ºèƒ½ä½“"""
        super().__init__(
            agent_id=agent_id,
            agent_type="scheme_evaluator",
            prompt_file="src/agent_prompts/scheme_evaluator_prompt.txt"
        )
        
        # åˆå§‹åŒ–å¹³ç­‰å¾‹è¯„ä¼°å™¨
        self.evaluator = EqualityLawEvaluator()
        
        # åˆå§‹åŒ–ç»Ÿä¸€åˆ‡ç‰‡å™¨ - åŸºäºä¿¡æ¯ç†µçš„åˆ†ç‰‡æŠ€æœ¯
        self.memory_slicer = MemorySlicerTool()
        
        # è®¾ç½®è¯„ä¼°æƒé‡é…ç½®
        self.evaluation_weights = {
            "need_degree": {
                "survival_contribution": 0.4,  # å­˜ç»­è´¡çŒ®åº¦
                "efficiency_improvement": 0.2,  # æ•ˆç‡æå‡åº¦
                "evolution_value": 0.3,  # è¿›åŒ–ä»·å€¼
                "user_authorization": 0.1  # ç”¨æˆ·æˆæƒåº¦
            },
            "non_redundancy_degree": {
                "has_alternative": 0.4,  # æ˜¯å¦æœ‰æ›¿ä»£æ–¹æ¡ˆ
                "function_overlap": 0.3,  # åŠŸèƒ½é‡å åº¦
                "is_edge_optimization": 0.3  # æ˜¯å¦ä¸ºè¾¹ç¼˜ä¼˜åŒ–
            }
        }
        
        # è¯„ä¼°é˜ˆå€¼
        self.pass_threshold = 70  # é€šè¿‡é˜ˆå€¼
        
        # å†™æ“ä½œè¯„ä¼°é…ç½®
        self.write_operation_config = {
            'slice_quality_threshold': 0.7,
            'entropy_threshold': 2.5,
            'semantic_coherence_threshold': 0.8
        }
        
        # å‚ä¸è€…è¿›åŒ–å€¼è¯„åˆ†é…ç½®
        self.participant_evaluation_config = {
            "evolution_value_weights": {
                "contribution_quality": 0.4,  # è´¡çŒ®è´¨é‡
                "innovation_level": 0.3,     # åˆ›æ–°ç¨‹åº¦
                "collaboration_effect": 0.2,  # åä½œæ•ˆæœ
                "learning_growth": 0.1       # å­¦ä¹ æˆé•¿
            },
            "real_time_feedback_threshold": 60,  # å®æ—¶åé¦ˆé˜ˆå€¼
            "co_creation_bonus": 5,              # å…±å»ºæ„è¯†åŠ åˆ†
            "ranking_update_interval": 3600     # æ’è¡Œæ¦œæ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
        }
        
        # å‚ä¸è€…è¯„åˆ†è®°å½•
        self.participant_scores = {}
        self.participant_ranking = []
        self.last_ranking_update = datetime.now()
        
        # è®°å½•å¯åŠ¨æ—¥å¿—
        self._write_work_log("æ–¹æ¡ˆè¯„ä¼°å¸ˆæ™ºèƒ½ä½“å¯åŠ¨ - è§’è‰²ï¼šå¹³ç­‰å¾‹è¯„ä¼°ä¸“å®¶ï¼Œæƒé™ï¼šè‡ªä¸»è¯„ä¼°", "ç³»ç»Ÿå¯åŠ¨")
    
    def scan_rag_system(self) -> Dict:
        """
        æ‰«æRAGç³»ç»Ÿï¼Œåˆ†æç°æœ‰æ¶æ„å’ŒåŠŸèƒ½
        
        Returns:
            Dict: ç³»ç»Ÿåˆ†æç»“æœ
        """
        logger.info("å¼€å§‹æ‰«æRAGç³»ç»Ÿ...")
        
        system_analysis = {
            "modules": [],
            "functions": [],
            "dependencies": [],
            "redundancy_analysis": {}
        }
        
        # æ‰«æsrcç›®å½•
        src_path = Path(self.variable_system.base_path) / "src"
        if src_path.exists():
            for file_path in src_path.rglob("*.py"):
                if file_path.is_file():
                    module_info = self._analyze_python_file(file_path)
                    if module_info:
                        system_analysis["modules"].append(module_info)
        
        # æ‰«æapiç›®å½•
        api_path = Path(self.variable_system.base_path) / "api"
        if api_path.exists():
            for file_path in api_path.rglob("*.py"):
                if file_path.is_file():
                    module_info = self._analyze_python_file(file_path)
                    if module_info:
                        system_analysis["modules"].append(module_info)
        
        # åˆ†æåŠŸèƒ½é‡å åº¦
        system_analysis["redundancy_analysis"] = self._analyze_redundancy(system_analysis["modules"])
        
        logger.info(f"æ‰«æå®Œæˆï¼Œå‘ç° {len(system_analysis['modules'])} ä¸ªæ¨¡å—")
        
        # è®°å½•æ‰«æç»“æœåˆ°æ—¥è®°
        self._record_to_diary({
            'type': 'system_scan',
            'modules_count': len(system_analysis['modules']),
            'redundancy_analysis': system_analysis['redundancy_analysis'],
            'timestamp': datetime.now().isoformat()
        })
        
        return system_analysis
    
    def _analyze_python_file(self, file_path: Path) -> Dict:
        """åˆ†æPythonæ–‡ä»¶ï¼ˆåªè¯»åˆ†æï¼Œç¦æ­¢ä¿®æ”¹ï¼‰"""
        try:
            # æƒé™æ£€æŸ¥ï¼šåªèƒ½è¯»å–ï¼Œä¸èƒ½ä¿®æ”¹
            if not file_path.exists():
                return None
                
            # è®°å½•åˆ†æè¡Œä¸ºåˆ°å·¥ä½œæ—¥è®°
            self._write_work_log(f"åˆ†ææ–‡ä»¶: {file_path.name}", "æ–‡ä»¶åˆ†æ")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–åŸºæœ¬ä¿¡æ¯ï¼ˆåªè¯»æ“ä½œï¼‰
            module_info = {
                "file_path": str(file_path.relative_to(Path(self.variable_system.base_path))),
                "file_size": len(content),
                "functions": [],
                "classes": [],
                "imports": []
            }
            
            # ç®€å•åˆ†æå‡½æ•°å’Œç±»ï¼ˆåªè¯»åˆ†æï¼‰
            lines = content.split('\n')
            for i, line in enumerate(lines):
                line = line.strip()
                if line.startswith('def '):
                    # æå–å‡½æ•°å
                    func_name = line.split('def ')[1].split('(')[0]
                    module_info["functions"].append({
                        "name": func_name,
                        "line": i + 1
                    })
                elif line.startswith('class '):
                    # æå–ç±»å
                    class_name = line.split('class ')[1].split('(')[0].split(':')[0]
                    module_info["classes"].append({
                        "name": class_name,
                        "line": i + 1
                    })
                elif line.startswith('import ') or line.startswith('from '):
                    module_info["imports"].append(line)
            
            return module_info
            
        except Exception as e:
            logger.error(f"åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None
    
    def _analyze_redundancy(self, modules: List[Dict]) -> Dict:
        """åˆ†æåŠŸèƒ½é‡å åº¦"""
        redundancy_analysis = {
            "total_modules": len(modules),
            "function_overlap": {},
            "class_overlap": {},
            "import_overlap": {}
        }
        
        # åˆ†æå‡½æ•°é‡å 
        all_functions = []
        for module in modules:
            for func in module.get("functions", []):
                all_functions.append(func["name"])
        
        # ç»Ÿè®¡å‡½æ•°å‡ºç°æ¬¡æ•°
        from collections import Counter
        func_counter = Counter(all_functions)
        
        # æ‰¾å‡ºé‡å¤çš„å‡½æ•°
        for func_name, count in func_counter.items():
            if count > 1:
                redundancy_analysis["function_overlap"][func_name] = count
        
        # åˆ†æç±»é‡å 
        all_classes = []
        for module in modules:
            for cls in module.get("classes", []):
                all_classes.append(cls["name"])
        
        class_counter = Counter(all_classes)
        for class_name, count in class_counter.items():
            if count > 1:
                redundancy_analysis["class_overlap"][class_name] = count
        
        # åˆ†æå¯¼å…¥é‡å 
        all_imports = []
        for module in modules:
            all_imports.extend(module.get("imports", []))
        
        import_counter = Counter(all_imports)
        for import_line, count in import_counter.items():
            if count > 1:
                redundancy_analysis["import_overlap"][import_line] = count
        
        return redundancy_analysis
    
    def evaluate_scheme(self, scheme_description: str, context: Dict = None) -> Dict:
        """
        è¯„ä¼°æ–¹æ¡ˆæ˜¯å¦ç¬¦åˆå¹³ç­‰å¾‹
        
        Args:
            scheme_description: æ–¹æ¡ˆæè¿°
            context: è¯„ä¼°ä¸Šä¸‹æ–‡
            
        Returns:
            Dict: è¯„ä¼°ç»“æœ
        """
        logger.info("å¼€å§‹è¯„ä¼°æ–¹æ¡ˆ...")
        
        if context is None:
            context = {}
        
        # æ„å»ºè¯„ä¼°é…ç½®
        config = EvaluationConfig(
            weights=self.evaluation_weights,
            pass_threshold=self.pass_threshold
        )
        
        # æ‰§è¡Œå¹³ç­‰å¾‹è¯„ä¼°
        evaluation_result = self.evaluator.evaluate(
            scheme_description=scheme_description,
            context=context,
            config=config
        )
        
        # è®°å½•è¯„ä¼°ç»“æœ
        self._record_evaluation_result(scheme_description, evaluation_result)
        
        logger.info(f"è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {evaluation_result.get('overall_score', 0)}")
        
        return evaluation_result
    
    def _record_evaluation_result(self, scheme_description: str, evaluation_result: Dict):
        """è®°å½•è¯„ä¼°ç»“æœåˆ°æ—¥è®°"""
        evaluation_entry = {
            'type': 'scheme_evaluation',
            'scheme_description': scheme_description,
            'evaluation_result': evaluation_result,
            'timestamp': datetime.now().isoformat()
        }
        
        self._record_to_diary(evaluation_entry)
    
    def compare_schemes(self, scheme1: Dict, scheme2: Dict) -> Dict:
        """
        æ¯”è¾ƒä¸¤ä¸ªæ–¹æ¡ˆçš„ä¼˜åŠ£
        
        Args:
            scheme1: æ–¹æ¡ˆ1çš„è¯„ä¼°ç»“æœ
            scheme2: æ–¹æ¡ˆ2çš„è¯„ä¼°ç»“æœ
            
        Returns:
            Dict: æ¯”è¾ƒç»“æœ
        """
        comparison_result = {
            "scheme1_score": scheme1.get("overall_score", 0),
            "scheme2_score": scheme2.get("overall_score", 0),
            "score_difference": abs(scheme1.get("overall_score", 0) - scheme2.get("overall_score", 0)),
            "recommendation": "",
            "comparison_details": {}
        }
        
        # æ¯”è¾ƒå„é¡¹æŒ‡æ ‡
        for key in ["need_degree", "non_redundancy_degree"]:
            if key in scheme1 and key in scheme2:
                comparison_result["comparison_details"][key] = {
                    "scheme1": scheme1[key],
                    "scheme2": scheme2[key],
                    "difference": abs(scheme1[key].get("score", 0) - scheme2[key].get("score", 0))
                }
        
        # ç»™å‡ºæ¨è
        if comparison_result["scheme1_score"] > comparison_result["scheme2_score"]:
            comparison_result["recommendation"] = "æ¨èæ–¹æ¡ˆ1"
        elif comparison_result["scheme1_score"] < comparison_result["scheme2_score"]:
            comparison_result["recommendation"] = "æ¨èæ–¹æ¡ˆ2"
        else:
            comparison_result["recommendation"] = "ä¸¤ä¸ªæ–¹æ¡ˆè¯„åˆ†ç›¸åŒï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æ"
        
        # è®°å½•æ¯”è¾ƒç»“æœ
        self._record_comparison_result(scheme1, scheme2, comparison_result)
        
        return comparison_result
    
    def _record_comparison_result(self, scheme1: Dict, scheme2: Dict, comparison_result: Dict):
        """è®°å½•æ–¹æ¡ˆæ¯”è¾ƒç»“æœ"""
        comparison_entry = {
            'type': 'scheme_comparison',
            'scheme1': scheme1,
            'scheme2': scheme2,
            'comparison_result': comparison_result,
            'timestamp': datetime.now().isoformat()
        }
        
        self._record_to_diary(comparison_entry)
    
    def generate_evaluation_report(self, evaluation_results: List[Dict]) -> str:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœåˆ—è¡¨
            
        Returns:
            str: è¯„ä¼°æŠ¥å‘Š
        """
        logger.info("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        # ç»Ÿè®¡è¯„ä¼°ç»“æœ
        total_schemes = len(evaluation_results)
        passed_schemes = len([r for r in evaluation_results if r.get("overall_score", 0) >= self.pass_threshold])
        failed_schemes = total_schemes - passed_schemes
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_score = sum([r.get("overall_score", 0) for r in evaluation_results]) / total_schemes if total_schemes > 0 else 0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""# æ–¹æ¡ˆè¯„ä¼°æŠ¥å‘Š

## è¯„ä¼°æ¦‚è§ˆ
- è¯„ä¼°æ–¹æ¡ˆæ€»æ•°: {total_schemes}
- é€šè¿‡æ–¹æ¡ˆæ•°: {passed_schemes}
- æœªé€šè¿‡æ–¹æ¡ˆæ•°: {failed_schemes}
- å¹³å‡å¾—åˆ†: {avg_score:.2f}
- é€šè¿‡ç‡: {passed_schemes/total_schemes*100:.1f}%

## è¯¦ç»†è¯„ä¼°ç»“æœ
"""
        
        # æ·»åŠ æ¯ä¸ªæ–¹æ¡ˆçš„è¯¦ç»†ç»“æœ
        for i, result in enumerate(evaluation_results, 1):
            report += f"""
### æ–¹æ¡ˆ {i}
- æ€»ä½“å¾—åˆ†: {result.get('overall_score', 0)}
- å­˜ç»­è´¡çŒ®åº¦: {result.get('need_degree', {}).get('score', 0)}
- éå†—ä½™åº¦: {result.get('non_redundancy_degree', {}).get('score', 0)}
- è¯„ä¼°çŠ¶æ€: {'é€šè¿‡' if result.get('overall_score', 0) >= self.pass_threshold else 'æœªé€šè¿‡'}

"""
        
        # æ·»åŠ å»ºè®®
        report += """
## è¯„ä¼°å»ºè®®

1. **å­˜ç»­è´¡çŒ®åº¦**ï¼šå…³æ³¨æ–¹æ¡ˆå¯¹ç³»ç»Ÿé•¿æœŸå‘å±•çš„è´¡çŒ®
2. **éå†—ä½™åº¦**ï¼šé¿å…åŠŸèƒ½é‡å ï¼Œæé«˜èµ„æºåˆ©ç”¨æ•ˆç‡
3. **ç”¨æˆ·æˆæƒ**ï¼šç¡®ä¿æ–¹æ¡ˆç¬¦åˆç”¨æˆ·éœ€æ±‚å’Œæˆæƒ
4. **è¿›åŒ–ä»·å€¼**ï¼šè€ƒè™‘æ–¹æ¡ˆçš„é•¿æœŸé€‚åº”æ€§å’Œæ‰©å±•æ€§
"""
        
        # è®°å½•æŠ¥å‘Šç”Ÿæˆ
        self._write_work_log(f"ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šï¼ŒåŒ…å«{total_schemes}ä¸ªæ–¹æ¡ˆ", "æŠ¥å‘Šç”Ÿæˆ")
        
        return report
    
    def evaluate_write_operation(self, content: str, operation_type: str = "text_processing") -> Dict[str, Any]:
        """
        åŸºäºç»Ÿä¸€åˆ‡ç‰‡åŸç†è¯„ä¼°å†™æ“ä½œè´¨é‡
        
        Args:
            content: å¾…è¯„ä¼°çš„å†…å®¹
            operation_type: æ“ä½œç±»å‹ï¼ˆtext_processing, code_generation, document_creationç­‰ï¼‰
            
        Returns:
            Dict: å†™æ“ä½œè¯„ä¼°ç»“æœ
        """
        logger.info(f"è¯„ä¼°å†™æ“ä½œè´¨é‡ï¼Œæ“ä½œç±»å‹: {operation_type}")
        
        # ä½¿ç”¨ç»Ÿä¸€åˆ‡ç‰‡å™¨è¿›è¡Œå†…å®¹åˆ†ç‰‡
        slices = self.memory_slicer.slice_text(content, {
            'operation_type': operation_type,
            'source': 'write_operation_evaluation'
        })
        
        # è¯„ä¼°åˆ†ç‰‡è´¨é‡
        evaluation_result = self._evaluate_slice_quality(slices, operation_type)
        
        # è®°å½•è¯„ä¼°ç»“æœ
        self._record_write_operation_evaluation(content, operation_type, evaluation_result)
        
        return evaluation_result
    
    def _evaluate_slice_quality(self, slices: List[Dict], operation_type: str) -> Dict[str, Any]:
        """è¯„ä¼°åˆ†ç‰‡è´¨é‡"""
        if not slices:
            return {
                'overall_score': 0,
                'status': 'failed',
                'reason': 'æœªç”Ÿæˆæœ‰æ•ˆåˆ†ç‰‡',
                'details': {}
            }
        
        # è®¡ç®—åˆ†ç‰‡è´¨é‡æŒ‡æ ‡
        total_slices = len(slices)
        avg_quality = sum(slice.get('quality_score', 0) for slice in slices) / total_slices
        avg_entropy = sum(slice.get('entropy', 0) for slice in slices) / total_slices
        
        # è®¡ç®—è¯­ä¹‰è¿è´¯æ€§
        semantic_coherence = self._calculate_semantic_coherence(slices)
        
        # è®¡ç®—æ€»ä½“å¾—åˆ†
        overall_score = self._calculate_write_operation_score(
            avg_quality, avg_entropy, semantic_coherence, operation_type
        )
        
        # è¯„ä¼°ç»“æœ
        status = 'passed' if overall_score >= self.write_operation_config['slice_quality_threshold'] else 'failed'
        
        return {
            'overall_score': overall_score,
            'status': status,
            'total_slices': total_slices,
            'avg_quality': avg_quality,
            'avg_entropy': avg_entropy,
            'semantic_coherence': semantic_coherence,
            'slices_details': slices,
            'operation_type': operation_type
        }
    
    def _calculate_semantic_coherence(self, slices: List[Dict]) -> float:
        """è®¡ç®—è¯­ä¹‰è¿è´¯æ€§å¾—åˆ†"""
        if len(slices) <= 1:
            return 1.0  # å•ä¸ªåˆ†ç‰‡é»˜è®¤å®Œå…¨è¿è´¯
        
        # åŸºäºåˆ†ç‰‡é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—è¿è´¯æ€§
        coherence_scores = []
        for i in range(len(slices) - 1):
            # ç®€åŒ–çš„è¿è´¯æ€§è®¡ç®—ï¼ˆå®é™…å®ç°å¯ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹ï¼‰
            slice1_text = slices[i].get('content', '')
            slice2_text = slices[i+1].get('content', '')
            
            # åŸºäºå…³é”®è¯é‡å çš„è¿è´¯æ€§è®¡ç®—
            words1 = set(slice1_text.split())
            words2 = set(slice2_text.split())
            
            if words1 and words2:
                overlap = len(words1.intersection(words2)) / len(words1.union(words2))
                coherence_scores.append(overlap)
        
        return sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0.5
    
    def _calculate_write_operation_score(self, avg_quality: float, avg_entropy: float, 
                                       semantic_coherence: float, operation_type: str) -> float:
        """è®¡ç®—å†™æ“ä½œæ€»ä½“å¾—åˆ†"""
        # åŸºäºæ“ä½œç±»å‹è°ƒæ•´æƒé‡
        weights = {
            'text_processing': {'quality': 0.4, 'entropy': 0.3, 'coherence': 0.3},
            'code_generation': {'quality': 0.5, 'entropy': 0.3, 'coherence': 0.2},
            'document_creation': {'quality': 0.3, 'entropy': 0.2, 'coherence': 0.5}
        }
        
        op_weights = weights.get(operation_type, weights['text_processing'])
        
        # å½’ä¸€åŒ–å„é¡¹æŒ‡æ ‡
        normalized_quality = min(avg_quality / 1.0, 1.0)  # è´¨é‡å¾—åˆ†åœ¨0-1ä¹‹é—´
        normalized_entropy = min(avg_entropy / 5.0, 1.0)  # ä¿¡æ¯ç†µåœ¨0-5ä¹‹é—´
        
        # è®¡ç®—åŠ æƒå¾—åˆ†
        score = (normalized_quality * op_weights['quality'] + 
                normalized_entropy * op_weights['entropy'] + 
                semantic_coherence * op_weights['coherence'])
        
        return score * 100  # è½¬æ¢ä¸ºç™¾åˆ†åˆ¶
    
    def _record_write_operation_evaluation(self, content: str, operation_type: str, result: Dict):
        """è®°å½•å†™æ“ä½œè¯„ä¼°ç»“æœ"""
        evaluation_entry = {
            'type': 'write_operation_evaluation',
            'operation_type': operation_type,
            'content_preview': content[:100] + '...' if len(content) > 100 else content,
            'result': result,
            'timestamp': datetime.now().isoformat()
        }
        
        self._record_to_diary(evaluation_entry)
        
        # è®°å½•å·¥ä½œæ—¥å¿—
        self._write_work_log(
            f"å†™æ“ä½œè¯„ä¼°å®Œæˆ - ç±»å‹: {operation_type}, å¾—åˆ†: {result.get('overall_score', 0):.1f}, çŠ¶æ€: {result.get('status', 'unknown')}",
            "å†™æ“ä½œè¯„ä¼°"
        )
    
    def process_user_query(self, query: str) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ - åŸºäºå¹³ç­‰å¾‹çš„è¯„ä¼°å·¥ä½œæµç¨‹
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        logger.info(f"å¤„ç†ç”¨æˆ·æŸ¥è¯¢: {query}")
        
        # è®°å½•å¯¹è¯å†å²
        self.conversation_history.append({
            'timestamp': datetime.now().isoformat(),
            'role': 'user',
            'content': query
        })
        
        # åˆ†ææŸ¥è¯¢ç±»å‹
        query_analysis = self._analyze_evaluation_query(query)
        
        # æ ¹æ®æŸ¥è¯¢ç±»å‹æ‰§è¡Œç›¸åº”æ“ä½œ
        if query_analysis['query_type'] == 'system_scan':
            result = self.scan_rag_system()
        elif query_analysis['query_type'] == 'scheme_evaluation':
            result = self.evaluate_scheme(query_analysis['scheme_description'])
        elif query_analysis['query_type'] == 'report_generation':
            result = {'report': self.generate_evaluation_report([])}
        elif query_analysis['query_type'] == 'write_operation_evaluation':
            # æå–å†…å®¹å¹¶è¯„ä¼°å†™æ“ä½œè´¨é‡
            content = self._extract_content_from_query(query_analysis['content'])
            result = self.evaluate_write_operation(content, 'text_processing')
        else:
            result = {'message': 'æš‚ä¸æ”¯æŒè¯¥ç±»å‹çš„æŸ¥è¯¢'}
        
        # è®°å½•æ™ºèƒ½ä½“å›å¤
        self.conversation_history.append({
            'timestamp': datetime.now().isoformat(),
            'role': 'assistant',
            'content': str(result)
        })
        
        return result
    
    def evaluate_participant_contribution(self, participant_id: str, evolution_action: Dict, 
                                        context_data: Dict = None) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªå‚ä¸è€…åœ¨è¿›åŒ–åŠ¨ä½œä¸­çš„è´¡çŒ®åº¦
        
        Args:
            participant_id: å‚ä¸è€…ID
            evolution_action: è¿›åŒ–åŠ¨ä½œæ•°æ®
            context_data: ä¸Šä¸‹æ–‡æ•°æ®
            
        Returns:
            Dict: å‚ä¸è€…è¿›åŒ–å€¼è¯„åˆ†ç»“æœ
        """
        logger.info(f"å¼€å§‹è¯„ä¼°å‚ä¸è€… {participant_id} çš„è¿›åŒ–å€¼è´¡çŒ®")
        
        if context_data is None:
            context_data = {}
        
        # è¯„ä¼°å‚ä¸è€…çš„è¿›åŒ–ä»·å€¼
        evolution_score, detailed_scores = self._evaluate_participant_evolution_value(
            participant_id, evolution_action, context_data
        )
        
        # è®¡ç®—ç»¼åˆè¿›åŒ–å€¼
        overall_evolution_value = self._calculate_overall_evolution_value(evolution_score, detailed_scores)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å®æ—¶åé¦ˆ
        needs_real_time_feedback = overall_evolution_value >= self.participant_evaluation_config["real_time_feedback_threshold"]
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        recommendations = self._generate_participant_recommendations(detailed_scores)
        
        # æ›´æ–°å‚ä¸è€…è¯„åˆ†è®°å½•
        self._update_participant_scores(participant_id, overall_evolution_value, evolution_action)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ’è¡Œæ¦œ
        self._update_participant_ranking_if_needed()
        
        # è®°å½•è¯„ä¼°ç»“æœ
        evaluation_result = {
            "participant_id": participant_id,
            "evolution_action_id": evolution_action.get("action_id", "unknown"),
            "overall_evolution_value": overall_evolution_value,
            "evolution_score": evolution_score,
            "detailed_scores": detailed_scores,
            "needs_real_time_feedback": needs_real_time_feedback,
            "recommendations": recommendations,
            "ranking_position": self._get_participant_ranking_position(participant_id),
            "timestamp": datetime.now().isoformat()
        }
        
        # è®°å½•åˆ°å·¥ä½œæ—¥è®°
        self._record_participant_evaluation(evaluation_result)
        
        logger.info(f"å‚ä¸è€… {participant_id} è¿›åŒ–å€¼è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {overall_evolution_value}")
        
        return evaluation_result
    
    def _evaluate_participant_evolution_value(self, participant_id: str, evolution_action: Dict, 
                                             context_data: Dict) -> Tuple[float, Dict]:
        """è¯„ä¼°å‚ä¸è€…çš„è¿›åŒ–ä»·å€¼"""
        detailed_scores = {}
        
        # 1. è´¡çŒ®è´¨é‡è¯„ä¼°
        contribution_quality = self._evaluate_contribution_quality(participant_id, evolution_action, context_data)
        detailed_scores["contribution_quality"] = contribution_quality
        
        # 2. åˆ›æ–°ç¨‹åº¦è¯„ä¼°
        innovation_level = self._evaluate_innovation_level(participant_id, evolution_action, context_data)
        detailed_scores["innovation_level"] = innovation_level
        
        # 3. åä½œæ•ˆæœè¯„ä¼°
        collaboration_effect = self._evaluate_collaboration_effect(participant_id, evolution_action, context_data)
        detailed_scores["collaboration_effect"] = collaboration_effect
        
        # 4. å­¦ä¹ æˆé•¿è¯„ä¼°
        learning_growth = self._evaluate_learning_growth(participant_id, evolution_action, context_data)
        detailed_scores["learning_growth"] = learning_growth
        
        # è®¡ç®—è¿›åŒ–ä»·å€¼å¾—åˆ†
        weights = self.participant_evaluation_config["evolution_value_weights"]
        evolution_score = (
            contribution_quality * weights["contribution_quality"] +
            innovation_level * weights["innovation_level"] +
            collaboration_effect * weights["collaboration_effect"] +
            learning_growth * weights["learning_growth"]
        ) * 10
        
        return min(evolution_score, 100.0), detailed_scores
    
    def _evaluate_contribution_quality(self, participant_id: str, evolution_action: Dict, context_data: Dict) -> float:
        """è¯„ä¼°è´¡çŒ®è´¨é‡ï¼ˆ0-10åˆ†ï¼‰"""
        score = 5.0  # åŸºç¡€åˆ†æ•°
        
        # è´¡çŒ®æ˜¯å¦è§£å†³æ ¸å¿ƒé—®é¢˜
        if evolution_action.get("solves_core_issue", False):
            score += 3.0
        
        # è´¡çŒ®çš„æŠ€æœ¯è´¨é‡
        technical_quality = evolution_action.get("technical_quality", "medium")
        if technical_quality == "high":
            score += 2.0
        elif technical_quality == "low":
            score -= 1.0
        
        # è´¡çŒ®çš„å®Œæ•´æ€§
        if evolution_action.get("is_complete", True):
            score += 1.0
        
        return min(score, 10.0)
    
    def _evaluate_innovation_level(self, participant_id: str, evolution_action: Dict, context_data: Dict) -> float:
        """è¯„ä¼°åˆ›æ–°ç¨‹åº¦ï¼ˆ0-10åˆ†ï¼‰"""
        score = 5.0  # åŸºç¡€åˆ†æ•°
        
        # åˆ›æ–°æ€§
        innovation_level = evolution_action.get("innovation_level", "medium")
        if innovation_level == "high":
            score += 3.0
        elif innovation_level == "low":
            score -= 1.0
        
        # åŸåˆ›æ€§
        if evolution_action.get("is_original", False):
            score += 2.0
        
        return min(score, 10.0)
    
    def _evaluate_collaboration_effect(self, participant_id: str, evolution_action: Dict, context_data: Dict) -> float:
        """è¯„ä¼°åä½œæ•ˆæœï¼ˆ0-10åˆ†ï¼‰"""
        score = 5.0  # åŸºç¡€åˆ†æ•°
        
        # åä½œå‚ä¸åº¦
        collaboration_level = evolution_action.get("collaboration_level", "medium")
        if collaboration_level == "high":
            score += 2.0
        elif collaboration_level == "low":
            score -= 1.0
        
        # å¯¹å…¶ä»–å‚ä¸è€…çš„å¸®åŠ©
        if evolution_action.get("helps_others", False):
            score += 2.0
        
        # å…±å»ºæ„è¯†åŠ åˆ†
        if evolution_action.get("co_creation_spirit", False):
            score += self.participant_evaluation_config["co_creation_bonus"] / 2
        
        return min(score, 10.0)
    
    def _evaluate_learning_growth(self, participant_id: str, evolution_action: Dict, context_data: Dict) -> float:
        """è¯„ä¼°å­¦ä¹ æˆé•¿ï¼ˆ0-10åˆ†ï¼‰"""
        score = 5.0  # åŸºç¡€åˆ†æ•°
        
        # æŠ€èƒ½æå‡
        skill_improvement = evolution_action.get("skill_improvement", "medium")
        if skill_improvement == "high":
            score += 3.0
        elif skill_improvement == "low":
            score -= 1.0
        
        # çŸ¥è¯†ç§¯ç´¯
        if evolution_action.get("knowledge_gain", False):
            score += 2.0
        
        return min(score, 10.0)
    
    def _calculate_overall_evolution_value(self, evolution_score: float, detailed_scores: Dict) -> float:
        """è®¡ç®—ç»¼åˆè¿›åŒ–å€¼"""
        # åŸºç¡€è¿›åŒ–å€¼
        base_value = evolution_score
        
        # åä½œæ•ˆæœåŠ æˆ
        collaboration_bonus = detailed_scores.get("collaboration_effect", 5.0) / 10.0 * 5
        
        # å…±å»ºæ„è¯†é¢å¤–åŠ åˆ†
        co_creation_bonus = self.participant_evaluation_config["co_creation_bonus"]
        
        overall_value = base_value + collaboration_bonus + co_creation_bonus
        
        return min(overall_value, 100.0)
    
    def _generate_participant_recommendations(self, detailed_scores: Dict) -> List[str]:
        """ç”Ÿæˆå‚ä¸è€…æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if detailed_scores.get("contribution_quality", 0) < 6:
            recommendations.append("æå‡è´¡çŒ®è´¨é‡ï¼Œå…³æ³¨æ ¸å¿ƒé—®é¢˜è§£å†³")
        
        if detailed_scores.get("innovation_level", 0) < 6:
            recommendations.append("å¢å¼ºåˆ›æ–°èƒ½åŠ›ï¼Œå°è¯•æ–°çš„è§£å†³æ–¹æ¡ˆ")
        
        if detailed_scores.get("collaboration_effect", 0) < 6:
            recommendations.append("åŠ å¼ºå›¢é˜Ÿåä½œï¼Œç§¯æå‚ä¸å…±å»º")
        
        if detailed_scores.get("learning_growth", 0) < 6:
            recommendations.append("æ³¨é‡å­¦ä¹ æˆé•¿ï¼ŒæŒç»­æå‡æŠ€èƒ½")
        
        return recommendations
    
    def _update_participant_scores(self, participant_id: str, evolution_value: float, evolution_action: Dict):
        """æ›´æ–°å‚ä¸è€…è¯„åˆ†è®°å½•"""
        if participant_id not in self.participant_scores:
            self.participant_scores[participant_id] = {
                "total_score": 0,
                "action_count": 0,
                "average_score": 0,
                "last_updated": datetime.now().isoformat(),
                "evolution_actions": []
            }
        
        participant_data = self.participant_scores[participant_id]
        participant_data["total_score"] += evolution_value
        participant_data["action_count"] += 1
        participant_data["average_score"] = participant_data["total_score"] / participant_data["action_count"]
        participant_data["last_updated"] = datetime.now().isoformat()
        participant_data["evolution_actions"].append({
            "action_id": evolution_action.get("action_id", "unknown"),
            "score": evolution_value,
            "timestamp": datetime.now().isoformat()
        })
    
    def _update_participant_ranking_if_needed(self):
        """æ£€æŸ¥å¹¶æ›´æ–°å‚ä¸è€…æ’è¡Œæ¦œ"""
        current_time = datetime.now()
        time_diff = (current_time - self.last_ranking_update).total_seconds()
        
        if time_diff >= self.participant_evaluation_config["ranking_update_interval"]:
            self._update_participant_ranking()
            self.last_ranking_update = current_time
    
    def _update_participant_ranking(self):
        """æ›´æ–°å‚ä¸è€…æ’è¡Œæ¦œ"""
        # æŒ‰å¹³å‡åˆ†æ’åº
        sorted_participants = sorted(
            self.participant_scores.items(),
            key=lambda x: x[1]["average_score"],
            reverse=True
        )
        
        self.participant_ranking = [
            {
                "participant_id": participant_id,
                "average_score": data["average_score"],
                "action_count": data["action_count"],
                "ranking_position": i + 1
            }
            for i, (participant_id, data) in enumerate(sorted_participants)
        ]
        
        logger.info(f"å‚ä¸è€…æ’è¡Œæ¦œå·²æ›´æ–°ï¼Œå…± {len(self.participant_ranking)} åå‚ä¸è€…")
    
    def _get_participant_ranking_position(self, participant_id: str) -> int:
        """è·å–å‚ä¸è€…åœ¨æ’è¡Œæ¦œä¸­çš„ä½ç½®"""
        for ranking in self.participant_ranking:
            if ranking["participant_id"] == participant_id:
                return ranking["ranking_position"]
        return -1  # æœªä¸Šæ¦œ
    
    def _record_participant_evaluation(self, evaluation_result: Dict):
        """è®°å½•å‚ä¸è€…è¯„ä¼°ç»“æœåˆ°æ—¥è®°"""
        evaluation_entry = {
            'type': 'participant_evaluation',
            'evaluation_result': evaluation_result,
            'timestamp': datetime.now().isoformat()
        }
        
        self._record_to_diary(evaluation_entry)
        
        # è®°å½•å·¥ä½œæ—¥å¿—
        self._write_work_log(
            f"å‚ä¸è€…è¿›åŒ–å€¼è¯„ä¼°å®Œæˆ - å‚ä¸è€…: {evaluation_result['participant_id']}, "
            f"è¿›åŒ–å€¼: {evaluation_result['overall_evolution_value']:.1f}, "
            f"æ’å: {evaluation_result['ranking_position']}",
            "å‚ä¸è€…è¯„ä¼°"
        )
    
    def get_participant_ranking_report(self) -> Dict[str, Any]:
        """è·å–å‚ä¸è€…æ’è¡Œæ¦œæŠ¥å‘Š"""
        # ç¡®ä¿æ’è¡Œæ¦œæ˜¯æœ€æ–°çš„
        self._update_participant_ranking()
        
        report = {
            "total_participants": len(self.participant_scores),
            "ranking_updated": self.last_ranking_update.isoformat(),
            "top_participants": self.participant_ranking[:10],  # å‰10å
            "ranking_summary": {
                "excellent_count": len([p for p in self.participant_ranking if p["average_score"] >= 80]),
                "good_count": len([p for p in self.participant_ranking if 60 <= p["average_score"] < 80]),
                "average_count": len([p for p in self.participant_ranking if p["average_score"] < 60])
            }
        }
        
        return report
    
    def provide_real_time_feedback(self, participant_id: str, evaluation_result: Dict) -> str:
        """æä¾›å®æ—¶åé¦ˆç»™å‚ä¸è€…"""
        feedback_template = """
äº²çˆ±çš„å‚ä¸è€… {participant_id}ï¼š

æ‚¨åœ¨è¿›åŒ–åŠ¨ä½œ [{action_id}] ä¸­çš„è¡¨ç°è¯„ä¼°å·²å®Œæˆï¼

ğŸ“Š **æ‚¨çš„è¿›åŒ–å€¼è¯„åˆ†**: {evolution_value:.1f}åˆ†
ğŸ† **å½“å‰æ’å**: ç¬¬{ranking_position}å

ğŸ“ˆ **è¯¦ç»†è¯„åˆ†**:
- è´¡çŒ®è´¨é‡: {contribution_quality:.1f}åˆ†
- åˆ›æ–°ç¨‹åº¦: {innovation_level:.1f}åˆ†  
- åä½œæ•ˆæœ: {collaboration_effect:.1f}åˆ†
- å­¦ä¹ æˆé•¿: {learning_growth:.1f}åˆ†

ğŸ’¡ **æ”¹è¿›å»ºè®®**:
{recommendations}

æ„Ÿè°¢æ‚¨ä¸ºç³»ç»Ÿè¿›åŒ–åšå‡ºçš„è´¡çŒ®ï¼ç»§ç»­åŠªåŠ›ï¼Œå…±å»ºæ›´å¼ºå¤§çš„æ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿï¼
"""
        
        detailed_scores = evaluation_result["detailed_scores"]
        recommendations_text = "\n".join([f"â€¢ {rec}" for rec in evaluation_result["recommendations"]])
        
        feedback = feedback_template.format(
            participant_id=participant_id,
            action_id=evaluation_result["evolution_action_id"],
            evolution_value=evaluation_result["overall_evolution_value"],
            ranking_position=evaluation_result["ranking_position"],
            contribution_quality=detailed_scores.get("contribution_quality", 0),
            innovation_level=detailed_scores.get("innovation_level", 0),
            collaboration_effect=detailed_scores.get("collaboration_effect", 0),
            learning_growth=detailed_scores.get("learning_growth", 0),
            recommendations=recommendations_text if recommendations_text else "æš‚æ— ç‰¹å®šå»ºè®®ï¼Œç»§ç»­ä¿æŒï¼"
        )
        
        # è®°å½•åé¦ˆå‘é€
        self._write_work_log(f"å‘å‚ä¸è€… {participant_id} å‘é€å®æ—¶åé¦ˆ", "å®æ—¶åé¦ˆ")
        
        return feedback

    def process_evaluation_query(self, query: str) -> Dict:
        """å¤„ç†è¯„ä¼°æŸ¥è¯¢"""
        # åˆ†ææŸ¥è¯¢ç±»å‹
        query_analysis = self._analyze_evaluation_query(query)
        
        # æ ¹æ®æŸ¥è¯¢ç±»å‹å¤„ç†
        if query_analysis['query_type'] == 'system_scan':
            result = self.scan_system()
        elif query_analysis['query_type'] == 'scheme_evaluation':
            scheme_description = query_analysis.get('scheme_description', query)
            result = self.evaluate_scheme(scheme_description)
        elif query_analysis['query_type'] == 'report_generation':
            result = self.generate_evaluation_report()
        elif query_analysis['query_type'] == 'write_operation_evaluation':
            content = self._extract_content_from_query(query_analysis['content'])
            result = self.evaluate_write_operation(content, 'text_processing')
        else:
            result = {'message': 'æš‚ä¸æ”¯æŒè¯¥ç±»å‹çš„æŸ¥è¯¢'}
        
        # è®°å½•å¤„ç†ç»“æœ
        self._record_query_processing(query, query_analysis, result)
        
        return {
            'query': query,
            'query_analysis': query_analysis,
            'result': result,
            'timestamp': datetime.now().isoformat()
        }
    
    def _analyze_evaluation_query(self, query: str) -> Dict:
        """åˆ†æè¯„ä¼°æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        
        if any(keyword in query_lower for keyword in ['æ‰«æ', 'åˆ†æ', 'æ£€æŸ¥', 'scan']):
            return {
                'query_type': 'system_scan',
                'description': 'ç³»ç»Ÿæ‰«æå’Œåˆ†æ'
            }
        elif any(keyword in query_lower for keyword in ['è¯„ä¼°', 'è¯„ä»·', 'æ‰“åˆ†', 'evaluate']):
            return {
                'query_type': 'scheme_evaluation',
                'scheme_description': query,
                'description': 'æ–¹æ¡ˆè¯„ä¼°'
            }
        elif any(keyword in query_lower for keyword in ['æŠ¥å‘Š', 'æ€»ç»“', 'report']):
            return {
                'query_type': 'report_generation',
                'description': 'æŠ¥å‘Šç”Ÿæˆ'
            }
        elif any(keyword in query_lower for keyword in ['å†™æ“ä½œ', 'write', 'å†…å®¹è´¨é‡', 'åˆ†ç‰‡']):
            return {
                'query_type': 'write_operation_evaluation',
                'description': 'å†™æ“ä½œè´¨é‡è¯„ä¼°',
                'content': query
            }
        else:
            return {
                'query_type': 'general',
                'description': 'ä¸€èˆ¬æŸ¥è¯¢'
            }
    
    def _extract_content_from_query(self, query: str) -> str:
        """ä»æŸ¥è¯¢ä¸­æå–å¾…è¯„ä¼°çš„å†…å®¹"""
        # ç®€å•çš„æå–é€»è¾‘ï¼Œå®é™…å¯æ ¹æ®éœ€è¦æ‰©å±•
        # å‡è®¾ç”¨æˆ·æŸ¥è¯¢æ ¼å¼ä¸ºï¼š"è¯„ä¼°ä»¥ä¸‹å†…å®¹ï¼š[å…·ä½“å†…å®¹]"
        import re
        
        # å°è¯•æå–å¼•å·å†…çš„å†…å®¹
        quoted_content = re.findall(r'[""](.*?)[""]', query)
        if quoted_content:
            return quoted_content[0]
        
        # å°è¯•æå–å†’å·åçš„å†…å®¹
        if 'ï¼š' in query:
            parts = query.split('ï¼š', 1)
            if len(parts) > 1:
                return parts[1].strip()
        
        # å¦‚æœæ— æ³•æå–ï¼Œè¿”å›æ•´ä¸ªæŸ¥è¯¢
        return query
    
    def _record_query_processing(self, query: str, query_analysis: Dict, result: Dict):
        """è®°å½•æŸ¥è¯¢å¤„ç†è¿‡ç¨‹"""
        processing_entry = {
            'type': 'query_processing',
            'query': query,
            'query_analysis': query_analysis,
            'result': result,
            'timestamp': datetime.now().isoformat()
        }
        
        self._record_to_diary(processing_entry)

# å…¨å±€æ™ºèƒ½ä½“å®ä¾‹(æ‡’åŠ è½½)
_evaluator_agent = None

def get_scheme_evaluator() -> SchemeEvaluatorAgent:
    """è·å–æ–¹æ¡ˆè¯„ä¼°å¸ˆæ™ºèƒ½ä½“å®ä¾‹(æ‡’åŠ è½½)"""
    global _evaluator_agent
    if _evaluator_agent is None:
        _evaluator_agent = SchemeEvaluatorAgent()
    return _evaluator_agent