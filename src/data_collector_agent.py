# @self-expose: {"id": "data_collector_agent", "name": "Data Collector Agent", "type": "agent", "version": "2.0.0", "needs": {"deps": ["base_agent", "data_collector", "tool_discovery_engine", "llm_client_enhanced", "vision_processing_engine", "audio_processing_engine", "multimodal_fusion_engine"], "resources": ["agent_prompts/data_collector_prompt.txt"]}, "provides": {"capabilities": ["æ•°æ®æ”¶é›†", "æ•°æ®æºæ‰«æ", "æ•°æ®è´¨é‡éªŒè¯", "æ•°æ®æŠ¥å‘Šç”Ÿæˆ", "å¤šæ¨¡æ€å†…å®¹è§£æ", "ç½‘é¡µå¤šåª’ä½“çˆ¬å–"], "methods": {"process_user_query": {"signature": "(query: str) -> Dict[str, Any]", "description": "å¤„ç†ç”¨æˆ·æŸ¥è¯¢"}, "_register_multimodal_tools": {"signature": "() -> None", "description": "æ³¨å†Œå¤šæ¨¡æ€å¼•æ“ï¼ˆä»…æ•°æ®æ”¶é›†å¸ˆä¸“å±ï¼‰"}}, "exclusive_tools": ["VisionProcessingEngine", "AudioProcessingEngine", "MultimodalFusionEngine"], "tool_usage_scenarios": ["çˆ¬å–ç½‘é¡µæ—¶è§£æå›¾ç‰‡/æˆªå›¾", "çˆ¬å–ç½‘é¡µæ—¶è§£æéŸ³é¢‘/è§†é¢‘", "èåˆå¤šæ¨¡æ€ä¿¡æ¯æå–ç»“æ„åŒ–æ•°æ®"]}}
# æ•°æ®æ”¶é›†æ™ºèƒ½ä½“ - åŸºäºç»Ÿä¸€æ™ºèƒ½ä½“æ¨¡æ¿
# å¼€å‘æç¤ºè¯æ¥æºï¼šç”¨æˆ·è¦æ±‚è®¾è®¡æ•°æ®æ”¶é›†æ™ºèƒ½ä½“ï¼Œè§£å†³RAGç³»ç»Ÿ"åƒé¥­"é—®é¢˜

import os
import json
import logging
from typing import Dict, Any, List
from pathlib import Path
from datetime import datetime

# å¯¼å…¥æ™ºèƒ½ä½“åŸºç±»
from base_agent import BaseAgent

# å¯¼å…¥æ•°æ®æ”¶é›†å·¥å…·
from data_collector import DataCollector

# å¯¼å…¥å·¥å…·å‘ç°å¼•æ“
from tool_discovery_engine import ToolDiscoveryEngine

# å¯¼å…¥LLMå®¢æˆ·ç«¯
from llm_client_enhanced import LLMClientEnhanced

logger = logging.getLogger(__name__)

class DataCollectorAgent(BaseAgent):
    """æ•°æ®æ”¶é›†æ™ºèƒ½ä½“ - è´Ÿè´£RAGç³»ç»Ÿçš„æ•°æ®åŸºç¡€å»ºè®¾"""
    
    def __init__(self, agent_id: str = "data_collector_001"):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            agent_id=agent_id,
            agent_type="data_collector",
            prompt_file="src/agent_prompts/data_collector_prompt.txt"
        )
        
        # è®¾ç½®æ™ºèƒ½ä½“ç›®çš„ï¼ˆè§’è‰²ç”±ç³»ç»Ÿæç¤ºè¯å®šä¹‰ï¼‰
        self.purpose = "ä¸ºRAGç³»ç»Ÿæ”¶é›†ã€æ•´ç†å’Œå‡†å¤‡åŸºç¡€æ•°æ®ï¼Œç¡®ä¿ç³»ç»Ÿæœ‰å……è¶³çš„çŸ¥è¯†æ¥æº"
        
        # åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨
        self.data_collector = DataCollector()
        
        # åˆå§‹åŒ–å·¥å…·å‘ç°å¼•æ“
        self.tool_discovery_engine = ToolDiscoveryEngine()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ - æ™ºèƒ½ä½“æ ¸å¿ƒå¤§è„‘
        self.llm_client = LLMClientEnhanced()
        
        # æ³¨å†Œä¸“ç”¨å·¥å…·
        self._register_data_collection_tools()
        
        logger.info(f"æ•°æ®æ”¶é›†æ™ºèƒ½ä½“ {agent_id} åˆå§‹åŒ–å®Œæˆ")
    
    def _register_data_collection_tools(self):
        """æ³¨å†Œæ•°æ®æ”¶é›†ä¸“ç”¨å·¥å…·"""
        # æ•°æ®æ”¶é›†æ™ºèƒ½ä½“ä¸“ç”¨å·¥å…· - ä½¿ç”¨tool_integratoræ³¨å†Œ
        self.tool_integrator.register_tool(
            tool_name="scan_file_system",
            tool_description="æ‰«ææ–‡ä»¶ç³»ç»Ÿï¼Œå‘ç°å¯æ”¶é›†çš„æ•°æ®æº",
            tool_usage="ç”¨äºæ‰«ææ–‡ä»¶ç³»ç»Ÿï¼Œå‘ç°å¯æ”¶é›†çš„æ•°æ®æº"
        )
        
        self.tool_integrator.register_tool(
            tool_name="collect_from_path",
            tool_description="ä»æŒ‡å®šè·¯å¾„æ”¶é›†æ•°æ®",
            tool_usage="ç”¨äºä»æŒ‡å®šè·¯å¾„æ”¶é›†æ•°æ®"
        )
        
        self.tool_integrator.register_tool(
            tool_name="batch_collect_sources",
            tool_description="æ‰¹é‡æ”¶é›†æ‰€æœ‰é…ç½®çš„æ•°æ®æº",
            tool_usage="ç”¨äºæ‰¹é‡æ”¶é›†æ‰€æœ‰é…ç½®çš„æ•°æ®æº"
        )
        
        self.tool_integrator.register_tool(
            tool_name="validate_data_quality",
            tool_description="éªŒè¯æ”¶é›†æ•°æ®çš„è´¨é‡",
            tool_usage="ç”¨äºéªŒè¯æ”¶é›†æ•°æ®çš„è´¨é‡"
        )
        
        self.tool_integrator.register_tool(
            tool_name="generate_collection_report",
            tool_description="ç”Ÿæˆæ•°æ®æ”¶é›†æŠ¥å‘Š",
            tool_usage="ç”¨äºç”Ÿæˆæ•°æ®æ”¶é›†æŠ¥å‘Š"
        )
        
        # ğŸ”¥ æ•°æ®æ”¶é›†å¸ˆä¸“å±ï¼šå¤šæ¨¡æ€å¼•æ“æ³¨å†Œ
        # åœºæ™¯ï¼šçˆ¬å–ç½‘é¡µæ—¶é‡åˆ°å›¾ç‰‡/è§†é¢‘ï¼Œéœ€è¦è§£æå¤šåª’ä½“å†…å®¹
        self._register_multimodal_tools()
        
        logger.info("æ•°æ®æ”¶é›†ä¸“ç”¨å·¥å…·æ³¨å†Œå®Œæˆï¼ˆå«å¤šæ¨¡æ€å¼•æ“ï¼‰")
    
    def _register_multimodal_tools(self):
        """æ³¨å†Œå¤šæ¨¡æ€å¼•æ“ï¼ˆä»…æ•°æ®æ”¶é›†å¸ˆå¯ç”¨ï¼‰"""
        try:
            # ğŸ”¥ ç›´æ¥å®ä¾‹åŒ–å¹¶æ³¨å†Œåˆ° tool_instancesï¼ˆè€Œéè°ƒç”¨ç©ºå£³çš„ register_toolï¼‰
            
            # è§†è§‰å¤„ç†å¼•æ“ï¼šè§£æç½‘é¡µæˆªå›¾/å›¾ç‰‡
            from src.vision_processing_engine import VisionProcessingTool
            self.tool_integrator.tool_instances['VisionProcessingEngine'] = VisionProcessingTool()
            logger.info("âœ… è§†è§‰å¤„ç†å¼•æ“å®ä¾‹åŒ–æˆåŠŸ")
            
            # éŸ³é¢‘å¤„ç†å¼•æ“ï¼šè§£æç½‘é¡µéŸ³é¢‘/è§†é¢‘
            from src.audio_processing_engine import AudioProcessingTool
            self.tool_integrator.tool_instances['AudioProcessingEngine'] = AudioProcessingTool()
            logger.info("âœ… éŸ³é¢‘å¤„ç†å¼•æ“å®ä¾‹åŒ–æˆåŠŸ")
            
            # å¤šæ¨¡æ€èåˆå¼•æ“ï¼šèåˆå¤šç§æ¨¡æ€ä¿¡æ¯
            from src.multimodal_fusion_engine import MultimodalFusionTool
            self.tool_integrator.tool_instances['MultimodalFusionEngine'] = MultimodalFusionTool()
            logger.info("âœ… å¤šæ¨¡æ€èåˆå¼•æ“å®ä¾‹åŒ–æˆåŠŸ")
            
            logger.info("ğŸ¨ å¤šæ¨¡æ€å¼•æ“æ³¨å†ŒæˆåŠŸï¼ˆä»…æ•°æ®æ”¶é›†å¸ˆå¯ç”¨ï¼‰")
        except Exception as e:
            logger.warning(f"å¤šæ¨¡æ€å¼•æ“æ³¨å†Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def process_user_query(self, user_query: str) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ - æ™ºèƒ½ä½“è‡ªä¸»å†³ç­–èƒ½åŠ›çš„æ ¸å¿ƒæ–¹æ³•
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢å†…å®¹
            
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        # è®°å½•å·¥ä½œæ—¥å¿—
        self._write_work_log(f"å¤„ç†ç”¨æˆ·æŸ¥è¯¢: {user_query}", "QUERY_PROCESSING")
        
        try:
            # ä½¿ç”¨LLMåˆ†æç”¨æˆ·æ„å›¾
            analysis_result = self._analyze_user_intent(user_query)
            
            # æ ¹æ®æ„å›¾é€‰æ‹©é€‚å½“çš„å·¥å…·
            tool_selection = self._select_tools_for_query(analysis_result)
            
            # æ‰§è¡Œå·¥å…·é“¾
            execution_result = self._execute_tool_chain(tool_selection, user_query)
            
            # ç”Ÿæˆå“åº”
            response = self._generate_response(execution_result, user_query)
            
            return {
                "success": True,
                "user_query": user_query,
                "intent_analysis": analysis_result,
                "tool_selection": tool_selection,
                "execution_result": execution_result,
                "response": response,
                "message": "æŸ¥è¯¢å¤„ç†å®Œæˆ"
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†ç”¨æˆ·æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "user_query": user_query,
                "error": str(e),
                "message": "æŸ¥è¯¢å¤„ç†å¤±è´¥"
            }
    
    def _analyze_user_intent(self, user_query: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMåˆ†æç”¨æˆ·æ„å›¾"""
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæ•°æ®æ”¶é›†æ™ºèƒ½ä½“ï¼Œéœ€è¦åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾ã€‚
        
        ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}
        
        è¯·åˆ†æç”¨æˆ·çš„æ„å›¾ï¼Œå¹¶è¿”å›ä»¥ä¸‹ä¿¡æ¯ï¼š
        1. æ„å›¾åˆ†ç±»ï¼ˆæ•°æ®æ‰«æã€æ•°æ®æ”¶é›†ã€å·¥å…·é›†æˆã€æŠ¥å‘Šç”Ÿæˆç­‰ï¼‰
        2. å…³é”®éœ€æ±‚
        3. å»ºè®®çš„å¤„ç†æµç¨‹
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚
        """
        
        try:
            response = self.llm_client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model="deepseek-chat",
                temperature=0.3,
                max_tokens=300
            )
            
            # è§£æJSONå“åº”
            import json
            return json.loads(response)
            
        except:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤åˆ†æ
            return {
                "intent_category": "æ•°æ®æ”¶é›†",
                "key_requirements": ["æ”¶é›†æ•°æ®"],
                "suggested_workflow": ["æ‰«ææ•°æ®æº", "æ”¶é›†æ•°æ®", "ç”ŸæˆæŠ¥å‘Š"]
            }
    
    def _select_tools_for_query(self, intent_analysis: Dict[str, Any]) -> List[str]:
        """æ ¹æ®æ„å›¾åˆ†æé€‰æ‹©å·¥å…·"""
        intent_category = intent_analysis.get("intent_category", "æ•°æ®æ”¶é›†")
        
        tool_mapping = {
            "æ•°æ®æ‰«æ": ["scan_file_system"],
            "æ•°æ®æ”¶é›†": ["collect_from_path", "batch_collect_sources"],
            "å·¥å…·é›†æˆ": ["discover_external_tools", "integrate_external_tool"],
            "æŠ¥å‘Šç”Ÿæˆ": ["generate_collection_report"]
        }
        
        return tool_mapping.get(intent_category, ["scan_file_system", "collect_from_path"])
    
    def _execute_tool_chain(self, tools: List[str], user_query: str) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·é“¾"""
        results = {}
        
        for tool_name in tools:
            try:
                # è°ƒç”¨ç›¸åº”çš„å·¥å…·æ–¹æ³•
                if hasattr(self, tool_name):
                    tool_method = getattr(self, tool_name)
                    
                    # æ ¹æ®å·¥å…·ç±»å‹ä¼ é€’é€‚å½“çš„å‚æ•°
                    if tool_name == "scan_file_system":
                        result = tool_method()
                    elif tool_name == "collect_from_path":
                        result = tool_method("./data")
                    elif tool_name == "batch_collect_sources":
                        result = tool_method()
                    elif tool_name == "discover_external_tools":
                        result = tool_method(["data", "collection"])
                    elif tool_name == "integrate_external_tool":
                        result = tool_method("data_collector")
                    elif tool_name == "generate_collection_report":
                        result = tool_method()
                    else:
                        result = tool_method()
                    
                    results[tool_name] = result
                    
            except Exception as e:
                results[tool_name] = {"error": str(e)}
        
        return results
    
    def _generate_response(self, execution_results: Dict[str, Any], user_query: str) -> str:
        """ç”Ÿæˆæœ€ç»ˆå“åº”"""
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæ•°æ®æ”¶é›†æ™ºèƒ½ä½“ï¼Œå·²ç»å¤„ç†äº†ç”¨æˆ·æŸ¥è¯¢å¹¶è·å¾—äº†æ‰§è¡Œç»“æœã€‚
        
        ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}
        æ‰§è¡Œç»“æœï¼š{execution_results}
        
        è¯·åŸºäºæ‰§è¡Œç»“æœï¼Œç”Ÿæˆä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„å“åº”ç»™ç”¨æˆ·ã€‚
        å“åº”åº”è¯¥åŒ…æ‹¬ï¼š
        1. å¯¹ç”¨æˆ·æŸ¥è¯¢çš„ç†è§£
        2. æ‰§è¡Œçš„ä¸»è¦æ“ä½œ
        3. è·å¾—çš„ç»“æœ
        4. ä¸‹ä¸€æ­¥å»ºè®®
        
        è¯·ç”¨ä¸­æ–‡å›å¤ã€‚
        """
        
        try:
            response = self.llm_client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model="deepseek-chat",
                temperature=0.7,
                max_tokens=400
            )
            return response
            
        except Exception as e:
            return f"å¤„ç†å®Œæˆã€‚æ‰§è¡Œç»“æœï¼š{execution_results}"
    
    def discover_external_tools(self, keywords: List[str], category: str = None) -> Dict[str, Any]:
        """å‘ç°å¤–éƒ¨æ•°æ®æ”¶é›†å·¥å…·"""
        
        # è®°å½•å·¥ä½œæ—¥å¿—
        self._write_work_log("å¼€å§‹å‘ç°å¤–éƒ¨å·¥å…·", {
            "keywords": keywords,
            "category": category
        })
        
        try:
            # ä½¿ç”¨å·¥å…·å‘ç°å¼•æ“æœç´¢
            discovered_tools = self.tool_discovery_engine.search_github_tools(keywords, category)
            
            if not discovered_tools:
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_tools = self.tool_discovery_engine.get_cached_tools(category)
                if cached_tools:
                    discovered_tools = cached_tools
                    logger.info(f"ä»ç¼“å­˜è·å–åˆ° {len(discovered_tools)} ä¸ªå·¥å…·")
            
            # åˆ†æå·¥å…·é€‚ç”¨æ€§
            suitable_tools = []
            for tool in discovered_tools:
                if self._assess_tool_suitability(tool, keywords):
                    suitable_tools.append(tool)
            
            return {
                "success": True,
                "discovered_count": len(discovered_tools),
                "suitable_count": len(suitable_tools),
                "tools": suitable_tools,
                "message": f"å‘ç° {len(suitable_tools)} ä¸ªé€‚ç”¨çš„å¤–éƒ¨å·¥å…·"
            }
            
        except Exception as e:
            logger.error(f"å¤–éƒ¨å·¥å…·å‘ç°å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"å·¥å…·å‘ç°å¤±è´¥: {str(e)}"
            }
    
    def _assess_tool_suitability(self, tool_info: Dict, keywords: List[str]) -> bool:
        """è¯„ä¼°å·¥å…·é€‚ç”¨æ€§"""
        
        # è´¨é‡é˜ˆå€¼
        if tool_info.get("quality_score", 0) < 0.6:
            return False
        
        # å…³é”®è¯åŒ¹é…åº¦
        description = tool_info.get("description", "").lower()
        name = tool_info.get("name", "").lower()
        
        for keyword in keywords:
            if keyword.lower() in description or keyword.lower() in name:
                return True
        
        return False
    
    def generate_tool_wrapper(self, tool_info: Dict) -> Dict[str, Any]:
        """ä¸ºå¤–éƒ¨å·¥å…·ç”ŸæˆåŒ…è£…å™¨"""
        
        # è®°å½•å·¥ä½œæ—¥å¿—
        self._write_work_log("ç”Ÿæˆå·¥å…·åŒ…è£…å™¨", {
            "tool_name": tool_info.get("name"),
            "tool_url": tool_info.get("url")
        })
        
        try:
            # ç”ŸæˆåŒ…è£…å™¨ä»£ç 
            wrapper_code = self.tool_discovery_engine.generate_tool_wrapper(tool_info)
            
            if wrapper_code:
                # ä¿å­˜åŒ…è£…å™¨æ–‡ä»¶
                tool_name = tool_info["name"]
                wrapper_file = Path(f"tools/external/{tool_name}_wrapper.py")
                wrapper_file.parent.mkdir(parents=True, exist_ok=True)
                
                with open(wrapper_file, 'w', encoding='utf-8') as f:
                    f.write(wrapper_code)
                
                # æ³¨å†Œæ–°å·¥å…·
                self.tool_integrator.register_tool(
                    tool_name=f"external_{tool_name}",
                    tool_description=f"å¤–éƒ¨å·¥å…·åŒ…è£…å™¨: {tool_info.get('description', '')}",
                    tool_usage=f"ä½¿ç”¨å¤–éƒ¨å·¥å…· {tool_name} è¿›è¡Œæ•°æ®æ”¶é›†"
                )
                
                return {
                    "success": True,
                    "wrapper_file": str(wrapper_file),
                    "tool_name": f"external_{tool_name}",
                    "message": f"å·¥å…·åŒ…è£…å™¨ç”ŸæˆæˆåŠŸ: {tool_name}"
                }
            else:
                return {
                    "success": False,
                    "message": "åŒ…è£…å™¨ä»£ç ç”Ÿæˆå¤±è´¥"
                }
                
        except Exception as e:
            logger.error(f"å·¥å…·åŒ…è£…å™¨ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"åŒ…è£…å™¨ç”Ÿæˆå¤±è´¥: {str(e)}"
            }
    
    def integrate_external_tool(self, tool_name: str, tool_url: str = None) -> Dict[str, Any]:
        """é›†æˆå¤–éƒ¨å·¥å…·åˆ°æ•°æ®æ”¶é›†ç³»ç»Ÿ"""
        
        # è®°å½•å·¥ä½œæ—¥å¿—
        self._write_work_log("é›†æˆå¤–éƒ¨å·¥å…·", {
            "tool_name": tool_name,
            "tool_url": tool_url
        })
        
        try:
            # å¦‚æœæä¾›äº†URLï¼Œå…ˆå‘ç°å·¥å…·ä¿¡æ¯
            tool_info = None
            if tool_url:
                # ä»URLæå–å·¥å…·ä¿¡æ¯
                discovered_tools = self.tool_discovery_engine.search_github_tools([tool_name])
                if discovered_tools:
                    tool_info = discovered_tools[0]
            
            # å¦‚æœæœªæä¾›URLï¼Œå°è¯•ä»ç¼“å­˜è·å–
            if not tool_info:
                cached_tools = self.tool_discovery_engine.get_cached_tools()
                for tool in cached_tools:
                    if tool["name"] == tool_name:
                        tool_info = tool
                        break
            
            if not tool_info:
                return {
                    "success": False,
                    "message": f"æœªæ‰¾åˆ°å·¥å…·ä¿¡æ¯: {tool_name}"
                }
            
            # ç”ŸæˆåŒ…è£…å™¨
            wrapper_result = self.generate_tool_wrapper(tool_info)
            
            if wrapper_result["success"]:
                # æµ‹è¯•å·¥å…·å¯ç”¨æ€§
                test_result = self._test_external_tool(tool_info["name"])
                
                if test_result["success"]:
                    # è®°å½•å·¥å…·é›†æˆç»éªŒ
                    self._record_tool_integration_experience(tool_info, "success")
                    
                    return {
                        "success": True,
                        "tool_name": tool_info["name"],
                        "wrapper_file": wrapper_result["wrapper_file"],
                        "test_result": test_result,
                        "message": f"å¤–éƒ¨å·¥å…·é›†æˆæˆåŠŸ: {tool_info['name']}"
                    }
                else:
                    # è®°å½•å¤±è´¥ç»éªŒ
                    self._record_tool_integration_experience(tool_info, "failed")
                    
                    return {
                        "success": False,
                        "message": f"å·¥å…·æµ‹è¯•å¤±è´¥: {test_result.get('message', 'æœªçŸ¥é”™è¯¯')}"
                    }
            else:
                return wrapper_result
                
        except Exception as e:
            logger.error(f"å¤–éƒ¨å·¥å…·é›†æˆå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"é›†æˆå¤±è´¥: {str(e)}"
            }
    
    def _test_external_tool(self, tool_name: str) -> Dict[str, Any]:
        """æµ‹è¯•å¤–éƒ¨å·¥å…·å¯ç”¨æ€§"""
        # ç®€åŒ–çš„æµ‹è¯•é€»è¾‘
        # å®é™…å®ç°éœ€è¦æ‰§è¡Œå…·ä½“çš„æµ‹è¯•ç”¨ä¾‹
        return {
            "success": True,
            "message": "å·¥å…·æµ‹è¯•é€šè¿‡",
            "test_cases": ["å®‰è£…æµ‹è¯•", "åŸºæœ¬åŠŸèƒ½æµ‹è¯•"]
        }
    
    def _record_tool_integration_experience(self, tool_info: Dict, result: str):
        """è®°å½•å·¥å…·é›†æˆç»éªŒ"""
        # è®°å½•åˆ°è®°å¿†ç³»ç»Ÿï¼Œä¸ºåç»­å·¥å…·é€‰æ‹©æä¾›å‚è€ƒ
        experience = {
            "tool_name": tool_info["name"],
            "integration_result": result,
            "timestamp": self._get_current_timestamp(),
            "tool_quality": tool_info.get("quality_score", 0)
        }
        
        # ä¿å­˜åˆ°ç»éªŒåº“
        experience_file = Path("data/tool_experiences.json")
        experiences = []
        if experience_file.exists():
            with open(experience_file, 'r', encoding='utf-8') as f:
                experiences = json.load(f)
        
        experiences.append(experience)
        
        with open(experience_file, 'w', encoding='utf-8') as f:
            json.dump(experiences, f, ensure_ascii=False, indent=2)
    
    def scan_file_system(self, target_path: str = None) -> Dict[str, Any]:
        """æ‰«ææ–‡ä»¶ç³»ç»Ÿï¼Œå‘ç°å¯æ”¶é›†çš„æ•°æ®æº"""
        
        # è®°å½•å·¥ä½œæ—¥å¿—
        self._write_work_log("å¼€å§‹æ‰«ææ–‡ä»¶ç³»ç»Ÿ", {"target_path": target_path})
        
        try:
            if target_path is None:
                # ä½¿ç”¨é…ç½®çš„æ•°æ®æºè·¯å¾„
                from config.system_config import DATA_SOURCES
                
                scan_results = {
                    "scanned_sources": [],
                    "available_paths": [],
                    "unavailable_paths": []
                }
                
                for source_name, config in DATA_SOURCES.items():
                    if config.get("enabled", False):
                        paths = config.get("paths", [])
                        for path_template in paths:
                            path = path_template.replace("{username}", "current_user")
                            
                            if Path(path).exists():
                                scan_results["available_paths"].append({
                                    "source": source_name,
                                    "path": path,
                                    "status": "available"
                                })
                            else:
                                scan_results["unavailable_paths"].append({
                                    "source": source_name,
                                    "path": path,
                                    "status": "unavailable"
                                })
                        
                        scan_results["scanned_sources"].append(source_name)
                
                logger.info(f"æ‰«æå®Œæˆ: {len(scan_results['available_paths'])} ä¸ªå¯ç”¨è·¯å¾„")
                return {
                    "success": True,
                    "scan_results": scan_results,
                    "message": f"å‘ç° {len(scan_results['available_paths'])} ä¸ªå¯ç”¨æ•°æ®æº"
                }
            
            else:
                # æ‰«ææŒ‡å®šè·¯å¾„
                target_path = Path(target_path)
                if not target_path.exists():
                    return {
                        "success": False,
                        "message": f"ç›®æ ‡è·¯å¾„ä¸å­˜åœ¨: {target_path}"
                    }
                
                # ç»Ÿè®¡æ–‡ä»¶ä¿¡æ¯
                file_count = 0
                total_size = 0
                supported_extensions = ['.txt', '.md', '.json', '.log']
                
                for file_path in target_path.rglob('*'):
                    if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                        file_count += 1
                        total_size += file_path.stat().st_size
                
                return {
                    "success": True,
                    "scan_results": {
                        "target_path": str(target_path),
                        "file_count": file_count,
                        "total_size": total_size,
                        "supported_files": file_count > 0
                    },
                    "message": f"å‘ç° {file_count} ä¸ªæ”¯æŒçš„æ–‡ä»¶ï¼Œæ€»å¤§å°: {total_size} å­—èŠ‚"
                }
        
        except Exception as e:
            logger.error(f"æ–‡ä»¶ç³»ç»Ÿæ‰«æå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ‰«æå¤±è´¥: {str(e)}"
            }
    
    def collect_from_path(self, path: str, use_intelligent_slicing: bool = True) -> Dict[str, Any]:
        """ä»æŒ‡å®šè·¯å¾„æ”¶é›†æ•°æ®"""
        
        # é£é™©è¯„ä¼°ï¼šæ£€æŸ¥è·¯å¾„å®‰å…¨æ€§
        risk_assessment = self._assess_collection_risk(path)
        if risk_assessment["risk_level"] == "high":
            return {
                "success": False,
                "message": f"é«˜é£é™©æ“ä½œè¢«é˜»æ­¢: {risk_assessment['reason']}"
            }
        
        # è®°å½•å·¥ä½œæ—¥å¿—
        self._write_work_log("å¼€å§‹æ•°æ®æ”¶é›†", {
            "path": path,
            "use_intelligent_slicing": use_intelligent_slicing,
            "risk_assessment": risk_assessment
        })
        
        try:
            # ä½¿ç”¨æ•°æ®æ”¶é›†å™¨è¿›è¡Œæ”¶é›†
            raw_data = self.data_collector.collect_from_file_system(path)
            
            if not raw_data:
                return {
                    "success": False,
                    "message": "æœªæ”¶é›†åˆ°ä»»ä½•æ•°æ®"
                }
            
            # åº”ç”¨æ™ºèƒ½åˆ‡ç‰‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if use_intelligent_slicing:
                sliced_data = []
                for item in raw_data:
                    content = item.get('content', '')
                    if content:
                        slices = self.data_collector._intelligent_slice_text(
                            content, item.get('file_path', '')
                        )
                        sliced_data.extend(slices)
                
                final_data = sliced_data
                slicing_info = f"ï¼Œæ™ºèƒ½åˆ‡ç‰‡åå¾—åˆ° {len(sliced_data)} æ¡æ•°æ®"
            else:
                final_data = raw_data
                slicing_info = ""
            
            # ä¿å­˜æ”¶é›†çš„æ•°æ®
            self.data_collector._save_collected_data(final_data)
            
            # åˆ›å»ºè®°å¿†æ¡ç›®
            memory_content = f"ä»è·¯å¾„ {path} æ”¶é›†æ•°æ®ï¼Œè·å¾— {len(final_data)} æ¡è®°å½•"
            self.create_memory(
                content=memory_content,
                importance=0.7,
                tags=["data_collection", path]
            )
            
            return {
                "success": True,
                "collected_count": len(final_data),
                "raw_count": len(raw_data),
                "used_intelligent_slicing": use_intelligent_slicing,
                "message": f"æˆåŠŸæ”¶é›† {len(raw_data)} æ¡åŸå§‹æ•°æ®{slicing_info}"
            }
        
        except Exception as e:
            logger.error(f"æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ”¶é›†å¤±è´¥: {str(e)}"
            }
    
    def batch_collect_sources(self) -> Dict[str, Any]:
        """æ‰¹é‡æ”¶é›†æ‰€æœ‰é…ç½®çš„æ•°æ®æº"""
        
        self._write_work_log("å¼€å§‹æ‰¹é‡æ•°æ®æ”¶é›†", {})
        
        try:
            # ä½¿ç”¨æ•°æ®æ”¶é›†å™¨çš„æ‰¹é‡æ”¶é›†åŠŸèƒ½
            all_data = self.data_collector.collect_all_sources()
            
            if not all_data:
                return {
                    "success": False,
                    "message": "æ‰¹é‡æ”¶é›†æœªè·å¾—ä»»ä½•æ•°æ®"
                }
            
            # åˆ›å»ºæ‰¹é‡æ”¶é›†è®°å¿†
            memory_content = f"æ‰¹é‡æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å¾— {len(all_data)} æ¡é«˜è´¨é‡æ•°æ®åˆ‡ç‰‡"
            self.create_memory(
                content=memory_content,
                importance=0.8,
                tags=["batch_collection", "data_foundation"]
            )
            
            return {
                "success": True,
                "total_collected": len(all_data),
                "message": f"æ‰¹é‡æ”¶é›†å®Œæˆï¼Œè·å¾— {len(all_data)} æ¡æ•°æ®"
            }
        
        except Exception as e:
            logger.error(f"æ‰¹é‡æ”¶é›†å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ‰¹é‡æ”¶é›†å¤±è´¥: {str(e)}"
            }
    
    def validate_data_quality(self, sample_size: int = 10) -> Dict[str, Any]:
        """éªŒè¯æ”¶é›†æ•°æ®çš„è´¨é‡"""
        
        self._write_work_log("å¼€å§‹æ•°æ®è´¨é‡éªŒè¯", {"sample_size": sample_size})
        
        try:
            # è·å–æœ€è¿‘æ”¶é›†çš„æ•°æ®æ–‡ä»¶
            data_dir = Path("e:/RAGç³»ç»Ÿ/data")
            data_files = list(data_dir.glob("collected_data_*.json"))
            
            if not data_files:
                return {
                    "success": False,
                    "message": "æœªæ‰¾åˆ°æ”¶é›†çš„æ•°æ®æ–‡ä»¶"
                }
            
            # ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶
            latest_file = max(data_files, key=lambda x: x.stat().st_mtime)
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è´¨é‡è¯„ä¼°æŒ‡æ ‡
            quality_metrics = {
                "total_records": len(data),
                "avg_content_length": 0,
                "importance_distribution": {"high": 0, "medium": 0, "low": 0},
                "source_diversity": {},
                "completeness_score": 0.0
            }
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            total_length = 0
            for item in data[:sample_size]:  # æŠ½æ ·æ£€æŸ¥
                content = item.get('content', '')
                total_length += len(content)
                
                importance = item.get('importance', 0.5)
                if importance >= 0.8:
                    quality_metrics["importance_distribution"]["high"] += 1
                elif importance >= 0.5:
                    quality_metrics["importance_distribution"]["medium"] += 1
                else:
                    quality_metrics["importance_distribution"]["low"] += 1
                
                source = item.get('source', 'unknown')
                quality_metrics["source_diversity"][source] = quality_metrics["source_diversity"].get(source, 0) + 1
            
            if sample_size > 0:
                quality_metrics["avg_content_length"] = total_length / sample_size
                
                # è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
                completeness_factors = [
                    1.0 if len(data) > 0 else 0.0,  # æ˜¯å¦æœ‰æ•°æ®
                    0.8 if quality_metrics["avg_content_length"] > 100 else 0.3,  # å†…å®¹é•¿åº¦
                    0.7 if len(quality_metrics["source_diversity"]) > 1 else 0.4,  # æ¥æºå¤šæ ·æ€§
                    0.6 if quality_metrics["importance_distribution"]["high"] > 0 else 0.2  # é«˜è´¨é‡å†…å®¹
                ]
                quality_metrics["completeness_score"] = sum(completeness_factors) / len(completeness_factors)
            
            return {
                "success": True,
                "quality_metrics": quality_metrics,
                "data_file": str(latest_file),
                "message": f"æ•°æ®è´¨é‡è¯„ä¼°å®Œæˆï¼Œå®Œæ•´æ€§åˆ†æ•°: {quality_metrics['completeness_score']:.2f}"
            }
        
        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"è´¨é‡éªŒè¯å¤±è´¥: {str(e)}"
            }
    
    def generate_collection_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æ”¶é›†æŠ¥å‘Š"""
        
        self._write_work_log("ç”Ÿæˆæ•°æ®æ”¶é›†æŠ¥å‘Š", {})
        
        try:
            # è·å–æ•°æ®ç›®å½•ä¿¡æ¯
            data_dir = Path("e:/RAGç³»ç»Ÿ/data")
            data_files = list(data_dir.glob("collected_data_*.json"))
            
            if not data_files:
                return {
                    "success": False,
                    "message": "æœªæ‰¾åˆ°æ”¶é›†çš„æ•°æ®æ–‡ä»¶"
                }
            
            # ç»Ÿè®¡æŠ¥å‘Šä¿¡æ¯
            report = {
                "total_collection_files": len(data_files),
                "latest_collection_time": None,
                "total_data_records": 0,
                "file_size_distribution": {},
                "collection_timeline": []
            }
            
            for data_file in data_files:
                file_size = data_file.stat().st_size
                file_time = datetime.fromtimestamp(data_file.stat().st_mtime)
                
                # è¯»å–æ–‡ä»¶ç»Ÿè®¡è®°å½•æ•°
                try:
                    with open(data_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    record_count = len(data)
                except:
                    record_count = 0
                
                report["total_data_records"] += record_count
                
                # æ–‡ä»¶å¤§å°åˆ†ç±»
                size_category = "small"
                if file_size > 1024 * 1024:  # 1MB
                    size_category = "large"
                elif file_size > 1024 * 100:  # 100KB
                    size_category = "medium"
                
                report["file_size_distribution"][size_category] = report["file_size_distribution"].get(size_category, 0) + 1
                
                report["collection_timeline"].append({
                    "file": data_file.name,
                    "timestamp": file_time.isoformat(),
                    "records": record_count,
                    "size": file_size
                })
            
            # æŒ‰æ—¶é—´æ’åº
            report["collection_timeline"].sort(key=lambda x: x["timestamp"], reverse=True)
            
            if report["collection_timeline"]:
                report["latest_collection_time"] = report["collection_timeline"][0]["timestamp"]
            
            return {
                "success": True,
                "report": report,
                "message": f"æ•°æ®æ”¶é›†æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œå…± {report['total_data_records']} æ¡è®°å½•"
            }
        
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
            }
    
    def _assess_collection_risk(self, path: str) -> Dict[str, Any]:
        """è¯„ä¼°æ•°æ®æ”¶é›†é£é™©"""
        
        path_obj = Path(path)
        
        # é«˜é£é™©è·¯å¾„æ£€æŸ¥
        system_paths = [
            "C:\\Windows", "C:\\Program Files", "C:\\ProgramData",
            "/etc", "/usr", "/bin", "/sbin"
        ]
        
        for system_path in system_paths:
            if str(path_obj).startswith(system_path):
                return {
                    "risk_level": "high",
                    "reason": f"å°è¯•è®¿é—®ç³»ç»Ÿä¿æŠ¤è·¯å¾„: {system_path}"
                }
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not path_obj.exists():
            return {
                "risk_level": "medium",
                "reason": "ç›®æ ‡è·¯å¾„ä¸å­˜åœ¨"
            }
        
        # æ£€æŸ¥æƒé™
        try:
            # å°è¯•è¯»å–è·¯å¾„ä¿¡æ¯
            path_obj.stat()
        except PermissionError:
            return {
                "risk_level": "high",
                "reason": "æ²¡æœ‰è®¿é—®è¯¥è·¯å¾„çš„æƒé™"
            }
        
        return {
            "risk_level": "low",
            "reason": "è·¯å¾„å®‰å…¨æ£€æŸ¥é€šè¿‡"
        }

# æµ‹è¯•å‡½æ•°
def test_data_collector_agent():
    """æµ‹è¯•æ•°æ®æ”¶é›†æ™ºèƒ½ä½“"""
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæ™ºèƒ½ä½“å®ä¾‹
    agent = DataCollectorAgent()
    
    print("=== æ•°æ®æ”¶é›†æ™ºèƒ½ä½“æµ‹è¯• ===")
    
    # æµ‹è¯•æ‰«ææ–‡ä»¶ç³»ç»Ÿ
    print("\n1. æ‰«ææ–‡ä»¶ç³»ç»Ÿ...")
    scan_result = agent.scan_file_system()
    print(f"æ‰«æç»“æœ: {scan_result}")
    
    # æµ‹è¯•æ•°æ®æ”¶é›†
    print("\n2. æµ‹è¯•æ•°æ®æ”¶é›†...")
    test_path = "E:\\AI"  # ä½¿ç”¨ä½ ä¹‹å‰æåˆ°çš„AIç›®å½•
    if Path(test_path).exists():
        collect_result = agent.collect_from_path(test_path)
        print(f"æ”¶é›†ç»“æœ: {collect_result}")
    else:
        print(f"æµ‹è¯•è·¯å¾„ä¸å­˜åœ¨: {test_path}")
    
    # æµ‹è¯•è´¨é‡éªŒè¯
    print("\n3. æµ‹è¯•æ•°æ®è´¨é‡éªŒè¯...")
    quality_result = agent.validate_data_quality()
    print(f"è´¨é‡éªŒè¯ç»“æœ: {quality_result}")
    
    print("\n=== æµ‹è¯•å®Œæˆ ===")

if __name__ == "__main__":
    test_data_collector_agent()

# å…¨å±€æ™ºèƒ½ä½“å®ä¾‹(æ‡’åŠ è½½)
_data_collector_agent = None

def get_data_collector() -> DataCollectorAgent:
    """è·å–æ•°æ®æ”¶é›†è€…æ™ºèƒ½ä½“å®ä¾‹(æ‡’åŠ è½½)"""
    global _data_collector_agent
    if _data_collector_agent is None:
        _data_collector_agent = DataCollectorAgent()
    return _data_collector_agent