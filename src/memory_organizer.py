#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMé©±åŠ¨è®°å¿†æ¢³ç†ç³»ç»Ÿ
ä½¿ç”¨DEEPSEEK APIå®ç°è®°å¿†æ•°æ®çš„æ™ºèƒ½æ¢³ç†ã€å»é‡ã€çŸ¥è¯†å›¾è°±æ„å»ºå’Œè¯­ä¹‰å…³è”ç»´è¡¥å…¨

å¼€å‘æç¤ºè¯æ¥æºï¼šç”¨æˆ·å»ºè®®ä½¿ç”¨DEEPSEEK APIå¯†é’¥è°ƒç”¨LLMå®Œæˆè®°å¿†æ•°æ®å…¨é¢æ¢³ç†
"""
# @self-expose: {"id": "memory_organizer", "name": "Memory Organizer", "type": "component", "version": "1.0.0", "needs": {"deps": [], "resources": []}, "provides": {"capabilities": ["Memory OrganizeråŠŸèƒ½"]}}

import time
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import requests

from .mesh_database_interface import MeshDatabaseInterface
from .llm_client_enhanced import LLMClientEnhanced
from config.api_keys import api_key_manager

logger = logging.getLogger(__name__)

class MemoryOrganizer:
    """LLMé©±åŠ¨è®°å¿†æ¢³ç†å™¨"""
    
    def __init__(self, mesh_interface: MeshDatabaseInterface, deepseek_api_key: str = None):
        self.mesh_interface = mesh_interface
        self.deepseek_api_key = deepseek_api_key or api_key_manager.get_key("deepseek")
        
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        try:
            self.llm_client = LLMClientEnhanced(provider="deepseek")
            logger.info("âœ… ä½¿ç”¨DEEPSEEK APIè¿›è¡Œè®°å¿†æ¢³ç†")
        except ValueError as e:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°DEEPSEEK APIå¯†é’¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼: {e}")
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿå®¢æˆ·ç«¯
            class MockLLMClient:
                def slice_text_with_llm(self, text, metadata):
                    return []
                def chat_completion(self, messages, **kwargs):
                    return "[]"
            self.llm_client = MockLLMClient()
        
        # æ¢³ç†é…ç½®
        self.batch_size = 20  # æ¯æ‰¹å¤„ç†æ•°é‡ï¼ˆé¿å…APIé™åˆ¶ï¼‰
        self.max_retries = 3   # æœ€å¤§é‡è¯•æ¬¡æ•°
        
    def comprehensive_memory_organization(self, max_memories: int = 100) -> Dict[str, Any]:
        """å…¨é¢è®°å¿†æ¢³ç†ï¼šå»é‡ã€çŸ¥è¯†å›¾è°±æ„å»ºã€è¯­ä¹‰å…³è”ç»´è¡¥å…¨"""
        print("ğŸš€ å¼€å§‹å…¨é¢è®°å¿†æ¢³ç†...")
        
        start_time = time.time()
        
        # è·å–è®°å¿†æ•°æ®
        memories = self.mesh_interface.vector_db.search_memories(limit=max_memories)
        
        if not memories:
            return {'error': 'æ²¡æœ‰æ‰¾åˆ°è®°å¿†æ•°æ®'}
        
        print(f"ğŸ“š æ‰¾åˆ° {len(memories)} æ¡è®°å¿†ï¼Œå¼€å§‹å…¨é¢æ¢³ç†...")
        
        # æ‰§è¡Œæ¢³ç†æµç¨‹
        results = {
            'deduplication': self._deduplicate_memories(memories),
            'semantic_analysis': self._semantic_analysis_memories(memories),
            'knowledge_graph': self._build_enhanced_knowledge_graph(memories),
            'mesh_association': self._build_mesh_associations(memories)
        }
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        
        return {
            'overview': {
                'total_memories_processed': len(memories),
                'processing_time': processing_time,
                'completion_time': datetime.now().isoformat(),
                'llm_used': 'deepseek' if self.deepseek_api_key else 'mock'
            },
            'results': results
        }
    
    def _deduplicate_memories(self, memories: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å»é‡"""
        print("ğŸ” å¼€å§‹è®°å¿†å»é‡åˆ†æ...")
        
        # åˆ†æ‰¹å¤„ç†é¿å…APIé™åˆ¶
        duplicate_groups = []
        processed_count = 0
        
        for i in range(0, len(memories), self.batch_size):
            batch = memories[i:i + self.batch_size]
            batch_duplicates = self._analyze_duplicates_with_llm(batch)
            duplicate_groups.extend(batch_duplicates)
            processed_count += len(batch)
            print(f"  è¿›åº¦: {processed_count}/{len(memories)}")
        
        return {
            'duplicate_groups': duplicate_groups,
            'total_groups': len(duplicate_groups),
            'analysis_method': 'llm_semantic_deduplication'
        }
    
    def _analyze_duplicates_with_llm(self, memories: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMåˆ†æé‡å¤è®°å¿†"""
        if not self.deepseek_api_key:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šç®€å•åŸºäºå†…å®¹çš„å»é‡
            return self._simple_deduplication(memories)
        
        try:
            # æ„å»ºå»é‡åˆ†ææç¤ºè¯
            memory_contents = [
                f"{i+1}. {mem['content'][:200]}..." 
                for i, mem in enumerate(memories)
            ]
            
            prompt = f"""è¯·åˆ†æä»¥ä¸‹è®°å¿†å†…å®¹ï¼Œè¯†åˆ«è¯­ä¹‰é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„è®°å¿†å¯¹ï¼š

è®°å¿†åˆ—è¡¨ï¼š
{"\n".join(memory_contents)}

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
1. duplicate_groups: é‡å¤è®°å¿†ç»„åˆ—è¡¨ï¼Œæ¯ç»„åŒ…å«ç›¸ä¼¼è®°å¿†çš„ç´¢å¼•
2. similarity_reason: ç›¸ä¼¼æ€§åŸå› è¯´æ˜
3. confidence: ç›¸ä¼¼åº¦ç½®ä¿¡åº¦(0-1)

è¯·ç¡®ä¿åˆ†æåŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯è¡¨é¢æ–‡å­—é‡å¤ã€‚"""
            
            # è°ƒç”¨LLM API
            response = self._call_llm_api(prompt)
            
            # è§£æå“åº”
            analysis = self._parse_llm_response(response)
            return analysis.get('duplicate_groups', [])
            
        except Exception as e:
            logger.error(f"LLMå»é‡åˆ†æå¤±è´¥: {e}")
            return self._simple_deduplication(memories)
    
    def _semantic_analysis_memories(self, memories: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦è¯­ä¹‰åˆ†æ"""
        print("ğŸ§  å¼€å§‹æ·±åº¦è¯­ä¹‰åˆ†æ...")
        
        semantic_categories = {}
        key_concepts = []
        
        for memory in memories:
            analysis = self._analyze_single_memory(memory)
            
            # ç»Ÿè®¡åˆ†ç±»
            category = analysis.get('semantic_category', 'æœªçŸ¥')
            semantic_categories[category] = semantic_categories.get(category, 0) + 1
            
            # æ”¶é›†å…³é”®æ¦‚å¿µ
            key_concepts.extend(analysis.get('key_concepts', []))
        
        return {
            'semantic_categories': semantic_categories,
            'top_key_concepts': list(set(key_concepts))[:20],  # å»é‡å¹¶é™åˆ¶æ•°é‡
            'total_memories_analyzed': len(memories)
        }
    
    def _analyze_single_memory(self, memory: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå•æ¡è®°å¿†"""
        content = memory.get('content', '')[:500]
        
        if not self.deepseek_api_key:
            # æ¨¡æ‹Ÿåˆ†æ
            return self._simple_semantic_analysis(content)
        
        try:
            prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬å†…å®¹çš„è¯­ä¹‰ä¿¡æ¯ï¼š

æ–‡æœ¬å†…å®¹ï¼š{content}

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
1. main_topics: ä¸»è¦ä¸»é¢˜ï¼ˆ1-3ä¸ªå…³é”®è¯ï¼‰
2. key_concepts: å…³é”®æ¦‚å¿µï¼ˆ3-5ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼‰
3. semantic_category: è¯­ä¹‰ç±»åˆ«ï¼ˆæŠ€æœ¯ã€å­¦æœ¯ã€å•†ä¸šã€æ—¥å¸¸ç­‰ï¼‰
4. sentiment: æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æã€æ¶ˆæã€ä¸­æ€§ï¼‰
5. complexity: å†…å®¹å¤æ‚åº¦ï¼ˆç®€å•ã€ä¸­ç­‰ã€å¤æ‚ï¼‰"""
            
            response = self._call_llm_api(prompt)
            return self._parse_llm_response(response)
            
        except Exception as e:
            logger.error(f"LLMè¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            return self._simple_semantic_analysis(content)
    
    def _build_enhanced_knowledge_graph(self, memories: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ„å»ºå¢å¼ºç‰ˆçŸ¥è¯†å›¾è°±ï¼ˆLLMä¼˜åŒ–ï¼‰"""
        print("ğŸ—ºï¸ æ„å»ºå¢å¼ºç‰ˆçŸ¥è¯†å›¾è°±...")
        
        # ä½¿ç”¨ç½‘çŠ¶æ€ç»´å¼•æ“çš„åŸºç¡€çŸ¥è¯†å›¾è°±
        base_graph = self.mesh_interface.build_knowledge_graph()
        
        # ä½¿ç”¨LLMä¼˜åŒ–çŸ¥è¯†å›¾è°±ç»“æ„
        if self.deepseek_api_key:
            enhanced_graph = self._enhance_graph_with_llm(base_graph, memories)
        else:
            enhanced_graph = base_graph
        
        return {
            'base_graph': {
                'nodes': len(base_graph['nodes']),
                'edges': len(base_graph['edges'])
            },
            'enhanced_graph': enhanced_graph,
            'enhancement_method': 'llm_optimized' if self.deepseek_api_key else 'base_only'
        }
    
    def _enhance_graph_with_llm(self, base_graph: Dict[str, Any], memories: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä½¿ç”¨LLMä¼˜åŒ–çŸ¥è¯†å›¾è°±"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„LLMä¼˜åŒ–é€»è¾‘
        # æš‚æ—¶è¿”å›åŸºç¡€å›¾è°±
        return base_graph
    
    def _build_mesh_associations(self, memories: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ„å»ºç½‘çŠ¶å…³è”ç½‘ç»œ"""
        print("ğŸ”— æ„å»ºç½‘çŠ¶è¯­ä¹‰å…³è”...")
        
        associations_created = 0
        
        for memory in memories:
            try:
                # ä½¿ç”¨ç½‘çŠ¶æ€ç»´å¼•æ“å­˜å‚¨è®°å¿†ï¼ˆè‡ªåŠ¨åˆ›å»ºå…³è”ï¼‰
                result = self.mesh_interface.store_memory_with_mesh({
                    'content': memory['content'],
                    'topic': memory.get('topic', 'æœªåˆ†ç±»'),
                    'importance': memory.get('importance', 0.5),
                    'source_type': 'memory_organizer'
                })
                
                if result.get('mesh_enhanced'):
                    associations_created += result.get('connections_created', 0)
                    
            except Exception as e:
                logger.error(f"æ„å»ºè®°å¿†å…³è”å¤±è´¥: {e}")
                continue
        
        return {
            'associations_created': associations_created,
            'memories_processed': len(memories)
        }
    
    def _call_llm_api(self, prompt: str, max_retries: int = None) -> str:
        """è°ƒç”¨LLM API"""
        max_retries = max_retries or self.max_retries
        
        for attempt in range(max_retries):
            try:
                # ä½¿ç”¨ç°æœ‰çš„LLMå®¢æˆ·ç«¯
                metadata = {'source': 'memory_organizer', 'purpose': 'semantic_analysis'}
                slices = self.llm_client.slice_text_with_llm(prompt, metadata)
                
                if slices:
                    return slices[0].get('content', '')
                else:
                    return ""
                    
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                time.sleep(1)  # é‡è¯•å‰ç­‰å¾…
        
        return ""
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æLLMå“åº”"""
        try:
            # å°è¯•è§£æJSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æ„
        return {
            'main_topics': ['æœªçŸ¥'],
            'key_concepts': ['æœªçŸ¥'],
            'semantic_category': 'æœªçŸ¥',
            'sentiment': 'ä¸­æ€§',
            'complexity': 'ä¸­ç­‰'
        }
    
    def _simple_deduplication(self, memories: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç®€åŒ–å»é‡ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰"""
        # åŸºäºå†…å®¹å“ˆå¸Œçš„ç®€å•å»é‡
        seen = set()
        duplicates = []
        
        for i, memory in enumerate(memories):
            content_hash = hash(memory['content'][:100])  # å–å‰100å­—ç¬¦è®¡ç®—å“ˆå¸Œ
            if content_hash in seen:
                duplicates.append({
                    'memory_indices': [i],
                    'similarity_reason': 'å†…å®¹å“ˆå¸Œé‡å¤',
                    'confidence': 0.8
                })
            else:
                seen.add(content_hash)
        
        return duplicates
    
    def _simple_semantic_analysis(self, content: str) -> Dict[str, Any]:
        """ç®€åŒ–è¯­ä¹‰åˆ†æï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰"""
        # åŸºäºå…³é”®è¯çš„ç®€å•åˆ†æ
        words = content.split()
        keywords = [word for word in words if len(word) >= 2][:5]
        
        return {
            'main_topics': keywords[:2],
            'key_concepts': keywords,
            'semantic_category': 'æ—¥å¸¸',
            'sentiment': 'ä¸­æ€§',
            'complexity': 'ä¸­ç­‰'
        }

# æµ‹è¯•å‡½æ•°
def test_memory_organizer():
    """æµ‹è¯•è®°å¿†æ¢³ç†å™¨"""
    print("=== LLMé©±åŠ¨è®°å¿†æ¢³ç†ç³»ç»Ÿæµ‹è¯• ===")
    
    from mesh_database_interface import MeshDatabaseInterface
    
    # åˆ›å»ºæ¥å£å®ä¾‹
    interface = MeshDatabaseInterface()
    
    # åˆ›å»ºè®°å¿†æ¢³ç†å™¨
    organizer = MemoryOrganizer(interface)
    
    # æµ‹è¯•å…¨é¢æ¢³ç†ï¼ˆå°è§„æ¨¡ï¼‰
    result = organizer.comprehensive_memory_organization(max_memories=30)
    
    print(f"\nğŸ“Š æ¢³ç†ç»“æœæ¦‚è§ˆ:")
    overview = result['overview']
    print(f"   å¤„ç†è®°å¿†æ•°: {overview['total_memories_processed']}")
    print(f"   å¤„ç†æ—¶é—´: {overview['processing_time']:.2f}ç§’")
    print(f"   LLMæ¨¡å¼: {overview['llm_used']}")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
    results = result['results']
    
    print(f"\nğŸ” å»é‡åˆ†æ:")
    dup_result = results['deduplication']
    print(f"   å‘ç°é‡å¤ç»„: {dup_result['total_groups']}ç»„")
    
    print(f"\nğŸ§  è¯­ä¹‰åˆ†æ:")
    semantic_result = results['semantic_analysis']
    print(f"   è¯­ä¹‰åˆ†ç±»: {semantic_result['semantic_categories']}")
    print(f"   å…³é”®æ¦‚å¿µ: {semantic_result['top_key_concepts'][:5]}...")
    
    print(f"\nğŸ—ºï¸ çŸ¥è¯†å›¾è°±:")
    graph_result = results['knowledge_graph']
    print(f"   åŸºç¡€å›¾è°±: {graph_result['base_graph']['nodes']}èŠ‚ç‚¹, {graph_result['base_graph']['edges']}è¾¹")
    print(f"   ä¼˜åŒ–æ–¹æ³•: {graph_result['enhancement_method']}")
    
    print(f"\nğŸ”— ç½‘çŠ¶å…³è”:")
    mesh_result = results['mesh_association']
    print(f"   åˆ›å»ºå…³è”: {mesh_result['associations_created']}ä¸ª")

if __name__ == "__main__":
    test_memory_organizer()