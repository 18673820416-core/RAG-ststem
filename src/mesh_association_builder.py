#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘çŠ¶æ€ç»´å¼•æ“å…³è”æ„å»ºå™¨
é€šè¿‡LLMåä½œå®ç°è®°å¿†æ•°æ®åº“çš„è¯­ä¹‰å…³è”ç»´è¡¥å…¨

å¼€å‘æç¤ºè¯æ¥æºï¼šç”¨æˆ·å»ºè®®ä½¿ç”¨LLMåä½œéå†è®°å¿†æ•°æ®åº“å®ç°è¯­ä¹‰å…³è”ç»´è¡¥å…¨
"""
# @self-expose: {"id": "mesh_association_builder", "name": "Mesh Association Builder", "type": "component", "version": "1.0.0", "needs": {"deps": [], "resources": []}, "provides": {"capabilities": ["Mesh Association BuilderåŠŸèƒ½"]}}

import time
import json
from typing import List, Dict, Any, Optional
from datetime import datetime

from .mesh_database_interface import MeshDatabaseInterface
from .llm_client_enhanced import LLMClientEnhanced

class MeshAssociationBuilder:
    """ç½‘çŠ¶æ€ç»´å…³è”æ„å»ºå™¨ - LLMåä½œç‰ˆæœ¬"""
    
    def __init__(self, mesh_interface: MeshDatabaseInterface, llm_client: LLMClientEnhanced = None):
        self.mesh_interface = mesh_interface
        
        # åˆ›å»ºæˆ–ä½¿ç”¨æä¾›çš„LLMå®¢æˆ·ç«¯
        if llm_client is None:
            # ä½¿ç”¨æ¨¡æ‹Ÿå®¢æˆ·ç«¯è¿›è¡Œæµ‹è¯•
            try:
                self.llm_client = LLMClientEnhanced(provider="deepseek")
            except ValueError:
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿå®¢æˆ·ç«¯
                class MockLLMClient:
                    def slice_text_with_llm(self, text, metadata):
                        return []
                    def chat_completion(self, messages, **kwargs):
                        return "[]"
                self.llm_client = MockLLMClient()
        else:
            self.llm_client = llm_client
            
        self.batch_size = 100  # æ¯æ‰¹å¤„ç†æ•°é‡
        
    def build_complete_association_network(self, max_memories: int = 1000) -> Dict[str, Any]:
        """æ„å»ºå®Œæ•´çš„å…³è”ç½‘ç»œï¼ˆLLMåä½œï¼‰"""
        print("ğŸš€ å¼€å§‹æ„å»ºå®Œæ•´çš„è¯­ä¹‰å…³è”ç½‘ç»œ...")
        
        # è·å–è®°å¿†æ•°æ®
        memories = self.mesh_interface.vector_db.search_memories(limit=max_memories)
        
        if not memories:
            return {'error': 'æ²¡æœ‰æ‰¾åˆ°è®°å¿†æ•°æ®'}
        
        print(f"ğŸ“š æ‰¾åˆ° {len(memories)} æ¡è®°å¿†ï¼Œå¼€å§‹è¯­ä¹‰å…³è”åˆ†æ...")
        
        stats = {
            'total_processed': 0,
            'thought_nodes_created': 0,
            'thought_nodes_reused': 0,
            'associations_created': 0,
            'processing_time': 0
        }
        
        start_time = time.time()
        
        # åˆ†æ‰¹å¤„ç†è®°å¿†
        for i in range(0, len(memories), self.batch_size):
            batch = memories[i:i + self.batch_size]
            batch_stats = self._process_memory_batch(batch, i // self.batch_size + 1)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            stats['total_processed'] += batch_stats['processed']
            stats['thought_nodes_created'] += batch_stats['nodes_created']
            stats['thought_nodes_reused'] += batch_stats['nodes_reused']
            stats['associations_created'] += batch_stats['associations']
            
            print(f"âœ… æ‰¹æ¬¡ {i//self.batch_size + 1} å®Œæˆ: "
                  f"å¤„ç† {batch_stats['processed']} æ¡è®°å¿†, "
                  f"åˆ›å»º {batch_stats['nodes_created']} ä¸ªæ€ç»´èŠ‚ç‚¹")
        
        stats['processing_time'] = time.time() - start_time
        
        # æ„å»ºçŸ¥è¯†å›¾è°±
        knowledge_graph = self.mesh_interface.build_knowledge_graph()
        
        return {
            'association_stats': stats,
            'knowledge_graph': {
                'nodes': len(knowledge_graph['nodes']),
                'edges': len(knowledge_graph['edges'])
            },
            'completion_time': datetime.now().isoformat()
        }
    
    def _process_memory_batch(self, memories: List[Dict[str, Any]], batch_num: int) -> Dict[str, Any]:
        """å¤„ç†ä¸€æ‰¹è®°å¿†æ•°æ®"""
        batch_stats = {
            'processed': 0,
            'nodes_created': 0,
            'nodes_reused': 0,
            'associations': 0
        }
        
        for memory in memories:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ€ç»´èŠ‚ç‚¹å…³è”
                if memory.get('thought_node_id'):
                    batch_stats['nodes_reused'] += 1
                    continue
                
                # ä½¿ç”¨LLMåˆ†æè®°å¿†å†…å®¹
                analysis_result = self._analyze_memory_with_llm(memory)
                
                # åŸºäºåˆ†æç»“æœåˆ›å»ºæˆ–å¤ç”¨æ€ç»´èŠ‚ç‚¹
                thought_result = self.mesh_interface.store_memory_with_mesh({
                    'content': memory['content'],
                    'topic': memory.get('topic', 'æœªåˆ†ç±»'),
                    'source_type': 'association_builder',
                    'importance': memory.get('importance', 0.5),
                    'llm_analysis': analysis_result
                })
                
                if thought_result.get('mesh_enhanced'):
                    if 'thought_node_id' in thought_result:
                        batch_stats['nodes_created'] += 1
                    batch_stats['associations'] += thought_result.get('connections_created', 0)
                
                batch_stats['processed'] += 1
                
                # è¿›åº¦æ˜¾ç¤º
                if batch_stats['processed'] % 10 == 0:
                    print(f"  è¿›åº¦: {batch_stats['processed']}/{len(memories)}")
                
            except Exception as e:
                print(f"âŒ å¤„ç†è®°å¿†å¤±è´¥: {e}")
                continue
        
        return batch_stats
    
    def _analyze_memory_with_llm(self, memory: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨LLMåˆ†æè®°å¿†å†…å®¹ï¼ˆè¯­ä¹‰ç†è§£ï¼‰"""
        content = memory.get('content', '')[:500]  # é™åˆ¶é•¿åº¦
        
        if not self.llm_client or not hasattr(self.llm_client, 'slice_text_with_llm'):
            # å¦‚æœæ²¡æœ‰LLMå®¢æˆ·ç«¯æˆ–å®¢æˆ·ç«¯ä¸æ”¯æŒè¯­ä¹‰åˆ†æï¼Œä½¿ç”¨ç®€åŒ–åˆ†æ
            return self._simple_semantic_analysis(content)
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„LLMåˆ‡ç‰‡åŠŸèƒ½è¿›è¡Œè¯­ä¹‰åˆ†æ
            metadata = {
                'source': 'association_builder',
                'purpose': 'semantic_analysis'
            }
            
            # è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†æ
            slices = self.llm_client.slice_text_with_llm(content, metadata)
            
            if slices:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ‡ç‰‡çš„å†…å®¹è¿›è¡Œåˆ†æ
                slice_content = slices[0].get('content', content)
                return self._analyze_slice_content(slice_content)
            else:
                # å¦‚æœåˆ‡ç‰‡å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†æ
                return self._simple_semantic_analysis(content)
            
        except Exception as e:
            print(f"LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†æ: {e}")
            return self._simple_semantic_analysis(content)
    
    def _analyze_slice_content(self, content: str) -> Dict[str, Any]:
        """åˆ†æåˆ‡ç‰‡å†…å®¹ï¼ˆåŸºäºLLMåˆ‡ç‰‡ç»“æœï¼‰"""
        # åŸºäºåˆ‡ç‰‡å†…å®¹è¿›è¡Œè¯­ä¹‰åˆ†æ
        keywords = self._extract_keywords(content)
        
        return {
            'main_topics': keywords[:3],
            'key_concepts': keywords[:5],
            'semantic_category': self._categorize_content(content),
            'related_suggestions': keywords[3:6] if len(keywords) > 6 else [],
            'analysis_method': 'llm_slice_based'
        }
    
    def _simple_semantic_analysis(self, content: str) -> Dict[str, Any]:
        """ç®€åŒ–è¯­ä¹‰åˆ†æï¼ˆæ— LLMæ—¶ä½¿ç”¨ï¼‰"""
        # åŸºäºå…³é”®è¯çš„ç®€å•åˆ†æ
        keywords = self._extract_keywords(content)
        
        return {
            'main_topics': keywords[:3],
            'key_concepts': keywords[:5],
            'semantic_category': self._categorize_content(content),
            'related_suggestions': keywords[3:6] if len(keywords) > 6 else [],
            'analysis_method': 'keyword_based'
        }
    
    def _extract_keywords(self, content: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # ä¸­æ–‡åœç”¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'ç»™', 'å¯ä»¥', 'é€šè¿‡', 'è¿™ä¸ª', 'è¿™æ ·', 'å·²ç»', 'ç°åœ¨', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'å¦‚æœ', 'ç„¶å', 'è€Œä¸”', 'æˆ–è€…', 'è™½ç„¶', 'å°½ç®¡', 'å³ä½¿', 'ä¸ºäº†', 'ç”±äº', 'å› æ­¤', 'ç„¶è€Œ', 'ä¸è¿‡', 'æ€»ä¹‹', 'ä¾‹å¦‚', 'æ¯”å¦‚', 'ç‰¹åˆ«', 'å°¤å…¶', 'éå¸¸', 'æ¯”è¾ƒ', 'ç›¸å¯¹', 'ç»å¯¹', 'å®Œå…¨', 'å½»åº•', 'åŸºæœ¬', 'ä¸»è¦', 'é‡è¦', 'å…³é”®', 'æ ¸å¿ƒ', 'æ ¹æœ¬', 'æœ¬è´¨', 'å®è´¨', 'å®é™…', 'çœŸæ­£', 'ç¡®å®', 'çš„ç¡®', 'è‚¯å®š', 'ä¸€å®š', 'å¿…é¡»', 'éœ€è¦', 'åº”è¯¥', 'åº”å½“', 'å¯ä»¥', 'èƒ½å¤Ÿ', 'å¯èƒ½', 'ä¹Ÿè®¸', 'å¤§æ¦‚', 'å¤§çº¦', 'å·¦å³', 'ä¸Šä¸‹', 'å‰å', 'å…ˆå', 'å…ˆåé¡ºåº', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å', 'æ€»ä¹‹', 'ç»¼ä¸Šæ‰€è¿°', 'æ€»çš„æ¥è¯´', 'æ€»è€Œè¨€ä¹‹', 'ç®€è€Œè¨€ä¹‹', 'æ¢å¥è¯è¯´', 'ä¹Ÿå°±æ˜¯è¯´', 'å®é™…ä¸Š', 'äº‹å®ä¸Š', 'æœ¬è´¨ä¸Š', 'ä»æœ¬è´¨ä¸Šè®²', 'ä»æ ¹æœ¬ä¸Šè¯´', 'æ€»çš„æ¥è¯´', 'æ€»ä½“è€Œè¨€', 'ä¸€èˆ¬è€Œè¨€', 'é€šå¸¸æƒ…å†µä¸‹', 'ä¸€èˆ¬æ¥è¯´', 'å¤§å¤šæ•°æƒ…å†µä¸‹', 'å°‘æ•°æƒ…å†µä¸‹', 'ä¸ªåˆ«æƒ…å†µä¸‹', 'ç‰¹æ®Šæƒ…å†µä¸‹', 'æ­£å¸¸æƒ…å†µä¸‹', 'å¼‚å¸¸æƒ…å†µä¸‹', 'ç´§æ€¥æƒ…å†µä¸‹', 'å±é™©æƒ…å†µä¸‹', 'å®‰å…¨æƒ…å†µä¸‹', 'ç¨³å®šæƒ…å†µä¸‹', 'ä¸ç¨³å®šæƒ…å†µä¸‹', 'å¹³è¡¡çŠ¶æ€ä¸‹', 'ä¸å¹³è¡¡çŠ¶æ€ä¸‹', 'å¯¹ç§°çŠ¶æ€ä¸‹', 'ä¸å¯¹ç§°çŠ¶æ€ä¸‹', 'å‡åŒ€çŠ¶æ€ä¸‹', 'ä¸å‡åŒ€çŠ¶æ€ä¸‹', 'è¿ç»­çŠ¶æ€ä¸‹', 'ä¸è¿ç»­çŠ¶æ€ä¸‹', 'ç¦»æ•£çŠ¶æ€ä¸‹', 'è¿ç»­ç¦»æ•£çŠ¶æ€ä¸‹'}
        
        # ç®€å•åˆ†è¯å’Œå…³é”®è¯æå–
        words = content.split()
        keywords = []
        
        for word in words:
            if (len(word) >= 2 and 
                word not in stop_words and 
                not word.isdigit() and
                word not in keywords):
                keywords.append(word)
        
        return keywords[:10]  # é™åˆ¶æ•°é‡
    
    def _categorize_content(self, content: str) -> str:
        """å†…å®¹åˆ†ç±»ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        tech_keywords = {'æŠ€æœ¯', 'ç§‘æŠ€', 'äººå·¥æ™ºèƒ½', 'AI', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç®—æ³•', 'ç¼–ç¨‹', 'ä»£ç ', 'è½¯ä»¶', 'ç¡¬ä»¶', 'ç½‘ç»œ', 'æ•°æ®', 'æ•°æ®åº“'}
        academic_keywords = {'ç ”ç©¶', 'å­¦æœ¯', 'è®ºæ–‡', 'ç§‘å­¦', 'ç†è®º', 'å®éªŒ', 'åˆ†æ', 'æ–¹æ³•', 'æ¨¡å‹', 'æ¡†æ¶', 'æ¦‚å¿µ', 'å®šä¹‰'}
        
        content_words = set(content.split())
        
        if tech_keywords.intersection(content_words):
            return 'æŠ€æœ¯'
        elif academic_keywords.intersection(content_words):
            return 'å­¦æœ¯'
        else:
            return 'æ—¥å¸¸'
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æLLMå“åº”"""
        try:
            # å°è¯•è§£æJSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æ„
        return {
            'main_topics': ['æœªçŸ¥'],
            'key_concepts': ['æœªçŸ¥'],
            'semantic_category': 'æœªçŸ¥',
            'related_suggestions': [],
            'analysis_method': 'llm_fallback'
        }

# æµ‹è¯•å‡½æ•°
def test_association_builder():
    """æµ‹è¯•å…³è”æ„å»ºå™¨"""
    print("=== ç½‘çŠ¶æ€ç»´å…³è”æ„å»ºå™¨æµ‹è¯• ===")
    
    from mesh_database_interface import MeshDatabaseInterface
    
    # åˆ›å»ºæ¥å£å®ä¾‹
    interface = MeshDatabaseInterface()
    
    # åˆ›å»ºæ„å»ºå™¨ï¼ˆæ— LLMå®¢æˆ·ç«¯ï¼‰
    builder = MeshAssociationBuilder(interface)
    
    # æµ‹è¯•æ„å»ºå…³è”ç½‘ç»œï¼ˆå°è§„æ¨¡ï¼‰
    result = builder.build_complete_association_network(max_memories=50)
    
    print(f"\nğŸ“Š å…³è”æ„å»ºç»“æœ:")
    print(f"  å¤„ç†è®°å¿†æ€»æ•°: {result['association_stats']['total_processed']}")
    print(f"  åˆ›å»ºæ€ç»´èŠ‚ç‚¹: {result['association_stats']['thought_nodes_created']}")
    print(f"  å¤ç”¨æ€ç»´èŠ‚ç‚¹: {result['association_stats']['thought_nodes_reused']}")
    print(f"  åˆ›å»ºå…³è”æ•°: {result['association_stats']['associations_created']}")
    print(f"  å¤„ç†æ—¶é—´: {result['association_stats']['processing_time']:.2f}ç§’")
    print(f"  çŸ¥è¯†å›¾è°±: {result['knowledge_graph']['nodes']}èŠ‚ç‚¹, {result['knowledge_graph']['edges']}è¾¹")

if __name__ == "__main__":
    test_association_builder()