# @self-expose: {"id": "chat_engine", "name": "Chat Engine", "type": "component", "version": "2.0.0", "needs": {"deps": ["unified_memory_system", "llm_client_enhanced", "chat_tools"], "resources": []}, "provides": {"capabilities": ["Chat EngineåŠŸèƒ½", "èŒè´£åˆ†ç¦»æ¶æ„"]}}
"""
RAGèŠå¤©å¼•æ“ - ä¸‰å±‚å“åº”æœºåˆ¶å®ç°

å¼€å‘æç¤ºè¯æ¥æºï¼šç”¨æˆ·å¯¹è¯ä¸­å…³äºæ™ºèƒ½è·¯ç”±å’Œå·¥å…·åŒ–æ€ç»´çš„è®¨è®º
æ ¸å¿ƒç†å¿µï¼šæœ¬åœ°çŸ¥è¯† â†’ é¢„è®­ç»ƒçŸ¥è¯† â†’ å®æ—¶å·¥å…·
"""

import logging
from typing import Dict, List, Any, Optional
from datetime import datetime

from tools.chat_tools import ChatToolManager
from src.llm_client_enhanced import LLMClientEnhanced

logger = logging.getLogger(__name__)

class ChatEngine:
    """RAGèŠå¤©å¼•æ“ - å®ç°ä¸‰å±‚å“åº”æœºåˆ¶"""
    
    def __init__(self):
        self.tool_manager = ChatToolManager()
        self.llm_client = LLMClientEnhanced()
        self.conversation_history = []
        
        # åˆå§‹åŒ–ç½‘çŠ¶æ€ç»´å¼•æ“
        self.mesh_thought_engine = self._initialize_mesh_thought_engine()
        
    def chat(self, user_input: str, use_tools: bool = True) -> Dict[str, Any]:
        """
        ä¸‰å±‚å“åº”æœºåˆ¶çš„èŠå¤©æµç¨‹
        
        æµç¨‹ï¼š
        1. æœ¬åœ°çŸ¥è¯†å±‚ï¼šæ£€ç´¢ç›¸å…³è®°å¿†æ„å»ºä¸Šä¸‹æ–‡
        2. é¢„è®­ç»ƒçŸ¥è¯†å±‚ï¼šåŸºäºLLMçš„é€šç”¨çŸ¥è¯†
        3. å®æ—¶å·¥å…·å±‚ï¼šè°ƒç”¨æ–‡ä»¶ã€ç½‘ç»œç­‰å·¥å…·
        """
        logger.info(f"å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input}")
        
        # å·¥ä½œæµç¨‹æ•°æ®é‡‡é›†å¼€å§‹
        workflow_data = {
            'thinking_time': 0,
            'tools_used': [],
            'memory_retrieved': 0,
            'risk_assessment': 'æœªè¯„ä¼°',
            'steps_completed': [],
            'step_timings': {},
            'strategy_selection': {},
            'query_analysis': {}
        }
        
        import time
        start_time = time.time()
        
        # ç¬¬ä¸€æ­¥ï¼šæœ¬åœ°çŸ¥è¯†å±‚æ£€ç´¢
        retrieval_start = time.time()
        local_context = self._retrieve_local_knowledge(user_input)
        workflow_data['step_timings']['knowledge_retrieval'] = time.time() - retrieval_start
        workflow_data['memory_retrieved'] = local_context['memory_count']
        workflow_data['steps_completed'].append('toolSelectionStep')
        
        # ç¬¬äºŒæ­¥ï¼šæ„å»ºå“åº”ç­–ç•¥
        strategy_start = time.time()
        response_strategy = self._determine_response_strategy(user_input, local_context)
        workflow_data['step_timings']['strategy_selection'] = time.time() - strategy_start
        workflow_data['strategy_selection'] = {
            'selected_strategy': response_strategy,
            'available_strategies': ['local_enhanced', 'hybrid', 'tool_enhanced', 'llm_only'],
            'selection_reason': self._get_strategy_reason(response_strategy, local_context)
        }
        workflow_data['steps_completed'].append('executionStep')
        
        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå“åº”
        generation_start = time.time()
        response = self._generate_response(user_input, local_context, response_strategy, use_tools)
        workflow_data['step_timings']['response_generation'] = time.time() - generation_start
        workflow_data['steps_completed'].append('validationStep')
        
        # ç¬¬å››æ­¥ï¼šæ›´æ–°å¯¹è¯å†å²
        history_start = time.time()
        self._update_conversation_history(user_input, response)
        workflow_data['step_timings']['history_update'] = time.time() - history_start
        workflow_data['steps_completed'].append('summaryStep')
        
        # è®¡ç®—æ€»æ€è€ƒæ—¶é—´
        workflow_data['thinking_time'] = int((time.time() - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # æ·»åŠ å·¥ä½œæµç¨‹æ•°æ®åˆ°å“åº”
        response['workflow_data'] = workflow_data
        
        return response
    
    def _retrieve_local_knowledge(self, query: str) -> Dict[str, Any]:
        """æœ¬åœ°çŸ¥è¯†å±‚ï¼šæ™ºèƒ½æ£€ç´¢ç›¸å…³è®°å¿†å’ŒçŸ¥è¯†å›¾è°±"""
        memory_tool = self.tool_manager.get_tool('memory_retrieval')
        
        if not memory_tool:
            return {'memories': [], 'context': '', 'knowledge_graph_context': ''}
        
        # ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢ç»„åˆï¼Œæ™ºèƒ½å°è¯•
        query_combinations = self._rewrite_query(query)
        
        memories = []
        best_query = query
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„æŸ¥è¯¢ç»„åˆ
        for query_variant in query_combinations:
            if not query_variant.strip():
                continue
                
            current_memories = memory_tool.search_memories(query_variant, limit=10)
            
            # å¦‚æœæ‰¾åˆ°è®°å¿†ï¼Œä½¿ç”¨è¿™ä¸ªæŸ¥è¯¢ä½œä¸ºæœ€ä½³æŸ¥è¯¢
            if current_memories:
                memories = current_memories
                best_query = query_variant
                print(f"âœ… æŸ¥è¯¢æˆåŠŸ: '{query_variant}' æ‰¾åˆ° {len(memories)} æ¡è®°å¿†")
                break
            else:
                print(f"âŒ æŸ¥è¯¢å¤±è´¥: '{query_variant}' æœªæ‰¾åˆ°è®°å¿†")
        
        # æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡
        memory_context = memory_tool.get_context_from_memories(best_query)
        
        # è·å–çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼ˆä¸ºLLMæä¾›ç»“æ„åŒ–çŸ¥è¯†ï¼‰
        knowledge_graph_context = self._get_knowledge_graph_context(query)
        
        # åˆå¹¶ä¸Šä¸‹æ–‡
        combined_context = memory_context
        if knowledge_graph_context:
            if combined_context:
                combined_context += f"\n\n{knowledge_graph_context}"
            else:
                combined_context = knowledge_graph_context
        
        return {
            'memories': memories,
            'context': combined_context,
            'memory_count': len(memories),
            'best_query': best_query,
            'knowledge_graph_context': knowledge_graph_context
        }
    
    def _get_knowledge_graph_context(self, query: str) -> str:
        """è·å–çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼ˆä¸ºLLMæä¾›ç»“æ„åŒ–çŸ¥è¯†ï¼‰"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç½‘çŠ¶æ€ç»´å¼•æ“
            if not hasattr(self, 'mesh_thought_engine') or not self.mesh_thought_engine:
                return ""
            
            # å‘é‡åŒ–æŸ¥è¯¢
            query_vector = self.mesh_thought_engine.vector_store.embed(query)
            
            # æŸ¥æ‰¾ç›¸ä¼¼çš„æ€ç»´èŠ‚ç‚¹
            similar_nodes = self.mesh_thought_engine.find_similar_thoughts(query_vector, threshold=0.6)
            
            if not similar_nodes:
                return ""
            
            # æ„å»ºLLMå‹å¥½çš„çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡
            context_parts = ["çŸ¥è¯†å›¾è°±å…³è”ä¿¡æ¯ï¼š"]
            
            for i, node in enumerate(similar_nodes[:3]):  # é™åˆ¶ä¸ºå‰3ä¸ªæœ€ç›¸å…³çš„èŠ‚ç‚¹
                # è·å–èŠ‚ç‚¹çš„å…³è”ç½‘ç»œ
                node_network = self.mesh_thought_engine.get_thought_network(node.id, max_depth=1)
                
                # æ„å»ºèŠ‚ç‚¹æè¿°
                node_desc = f"\n{i+1}. æ ¸å¿ƒæ¦‚å¿µ: {node.content}"
                
                # æ·»åŠ å…³è”æ¦‚å¿µ
                if node_network.get('connections'):
                    related_concepts = []
                    for conn in node_network['connections']:
                        if conn['target'] in self.mesh_thought_engine.nodes:
                            target_node = self.mesh_thought_engine.nodes[conn['target']]
                            relation_desc = f"{target_node.content}ï¼ˆ{conn['type']}ï¼‰"
                            related_concepts.append(relation_desc)
                    
                    if related_concepts:
                        node_desc += f"\n   å…³è”æ¦‚å¿µ: {', '.join(related_concepts[:2])}"
                
                context_parts.append(node_desc)
            
            return '\n'.join(context_parts)
            
        except Exception as e:
            print(f"çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡è·å–å¤±è´¥: {e}")
            return ""
    
    def _generate_intelligent_queries(self, query: str) -> list:
        """ç”Ÿæˆæ™ºèƒ½æŸ¥è¯¢ç»„åˆï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº"""
        
        # å…³é”®è¯åˆ—è¡¨
        keywords = ['ç¬¬ä¸€æ€§åŸç†', 'ç¬¬ä¸€æ€§', 'ç³»ç»Ÿ', 'æ„è¯†', 'è®¤çŸ¥', 'è®°å¿†', 'æ„ä¹‰',
                   'RAG', 'çŸ¥è¯†', 'å­¦ä¹ ', 'æ€è€ƒ', 'æ¨ç†', 'é€»è¾‘', 'å“²å­¦']
        
        # ç–‘é—®è¯åˆ—è¡¨
        question_words = ['ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'ä¸ºä½•', 'æ€æ ·', 'å“ªä¸ª', 'å“ªäº›',
                         'æ˜¯ä¸æ˜¯', 'æ˜¯å¦', 'æœ‰æ²¡æœ‰', 'èƒ½å¦', 'å¯å¦', 'å¯ä»¥å—', 'å¥½å—',
                         'è¡Œå—', 'å¯¹ä¸å¯¹', 'å¯¹å—', 'æ˜¯ä¸æ˜¯', 'æ˜¯å—', 'å‘¢', 'å—', 'ï¼Ÿ', '?',
                         'è¯·', 'è§£é‡Š', 'ä¸€ä¸‹', 'æ¦‚å¿µ', 'å®šä¹‰', 'å«ä¹‰', 'æ„æ€', 'æ˜¯ä»€ä¹ˆ',
                         'ä»€ä¹ˆæ˜¯', 'ä»€ä¹ˆå«', 'å•¥æ˜¯', 'å•¥å«', 'å•¥æ„æ€', 'å•¥å«ä¹‰', 'å•¥æ¦‚å¿µ']
        
        query_combinations = []
        
        # 1. åŸå§‹æŸ¥è¯¢ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        query_combinations.append(query)
        
        # 2. å»æ‰ç–‘é—®è¯çš„æŸ¥è¯¢
        clean_query = query
        for word in question_words:
            clean_query = clean_query.replace(word, '')
        
        clean_query = clean_query.strip().strip('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š')
        if clean_query and clean_query != query:
            query_combinations.append(clean_query)
        
        # 3. æå–åŒ…å«çš„å…³é”®è¯
        found_keywords = [kw for kw in keywords if kw in query]
        
        # å•ä¸ªå…³é”®è¯æŸ¥è¯¢
        for kw in found_keywords:
            query_combinations.append(kw)
        
        # å…³é”®è¯ç»„åˆæŸ¥è¯¢
        if len(found_keywords) > 1:
            # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆå°è¯•æ›´é•¿çš„ç»„åˆ
            sorted_keywords = sorted(found_keywords, key=len, reverse=True)
            query_combinations.append(' '.join(sorted_keywords))
            
            # å°è¯•æ‰€æœ‰å…³é”®è¯ç»„åˆ
            for i in range(len(sorted_keywords)):
                if i > 0:  # é¿å…é‡å¤æ·»åŠ å•ä¸ªå…³é”®è¯
                    query_combinations.append(' '.join(sorted_keywords[:i+1]))
        
        # 4. å¦‚æœæŸ¥è¯¢åŒ…å«ç‰¹å®šæ¨¡å¼ï¼Œç”Ÿæˆæ¨¡å¼åŒ–æŸ¥è¯¢
        if 'ç¬¬ä¸€æ€§åŸç†' in query:
            query_combinations.extend(['ç¬¬ä¸€æ€§åŸç†', 'ç¬¬ä¸€æ€§', 'ç³»ç»Ÿç¬¬ä¸€æ€§åŸç†'])
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_queries = []
        for q in query_combinations:
            if q and q not in seen:
                seen.add(q)
                unique_queries.append(q)
        
        return unique_queries
    
    def _preprocess_query(self, query: str) -> str:
        """é¢„å¤„ç†æŸ¥è¯¢ï¼šæå–å…³é”®è¯ï¼Œå»æ‰ç–‘é—®è¯"""
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨ä¸»è¦ç”¨äºå‘åå…¼å®¹
        intelligent_queries = self._generate_intelligent_queries(query)
        return intelligent_queries[0] if intelligent_queries else query
    
    def _rewrite_query(self, query: str) -> List[str]:
        """
        è½»é‡çº§ç”¨æˆ·é—®é¢˜æ”¹å†™ï¼Œç”Ÿæˆä¼˜åŒ–çš„æ£€ç´¢æŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·åŸå§‹é—®é¢˜
            
        Returns:
            List[str]: æ”¹å†™åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        try:
            # 1. å¯¼å…¥å¿…è¦çš„åº“
            import jieba
            from collections import Counter
            
            # 2. åˆ†è¯å¤„ç†
            words = jieba.lcut(query)
            
            # 3. å…³é”®è¯æå–ï¼ˆä½¿ç”¨è¯é¢‘ç»Ÿè®¡ï¼‰
            word_counts = Counter(words)
            # è¿‡æ»¤åœç”¨è¯
            stop_words = set(['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'])
            filtered_words = [word for word in words if word not in stop_words and len(word) > 1]
            
            # 4. ç”Ÿæˆæ”¹å†™æŸ¥è¯¢
            rewrite_queries = []
            
            # åŸå§‹æŸ¥è¯¢
            rewrite_queries.append(query)
            
            # å»æ‰ç–‘é—®è¯çš„æŸ¥è¯¢
            question_words = ['ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'ä¸ºä½•', 'æ€æ ·', 'å“ªä¸ª', 'å“ªäº›',
                             'æ˜¯ä¸æ˜¯', 'æ˜¯å¦', 'æœ‰æ²¡æœ‰', 'èƒ½å¦', 'å¯å¦', 'å¯ä»¥å—', 'å¥½å—',
                             'è¡Œå—', 'å¯¹ä¸å¯¹', 'å¯¹å—', 'æ˜¯ä¸æ˜¯', 'æ˜¯å—', 'å‘¢', 'å—', 'ï¼Ÿ', '?',
                             'è¯·', 'è§£é‡Š', 'ä¸€ä¸‹', 'æ¦‚å¿µ', 'å®šä¹‰', 'å«ä¹‰', 'æ„æ€', 'æ˜¯ä»€ä¹ˆ',
                             'ä»€ä¹ˆæ˜¯', 'ä»€ä¹ˆå«', 'å•¥æ˜¯', 'å•¥å«', 'å•¥æ„æ€', 'å•¥å«ä¹‰', 'å•¥æ¦‚å¿µ']
            clean_query = query
            for word in question_words:
                clean_query = clean_query.replace(word, '')
            clean_query = clean_query.strip().strip('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š')
            if clean_query and clean_query != query:
                rewrite_queries.append(clean_query)
            
            # å…³é”®è¯ç»„åˆæŸ¥è¯¢
            if filtered_words:
                # ç”Ÿæˆä¸åŒé•¿åº¦çš„å…³é”®è¯ç»„åˆ
                for i in range(1, min(len(filtered_words) + 1, 4)):
                    # è·å–å‰iä¸ªæœ€é¢‘ç¹çš„å…³é”®è¯
                    top_words = [word for word, count in word_counts.most_common(i)]
                    if top_words:
                        keyword_query = ' '.join(top_words)
                        if keyword_query not in rewrite_queries:
                            rewrite_queries.append(keyword_query)
            
            # 5. å»é‡å¹¶ä¿æŒé¡ºåº
            seen = set()
            unique_queries = []
            for q in rewrite_queries:
                if q and q not in seen:
                    seen.add(q)
                    unique_queries.append(q)
            
            return unique_queries
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ”¹å†™å¤±è´¥: {e}")
            # é™çº§åˆ°åŸå§‹æŸ¥è¯¢
            return [query]
    
    def _initialize_mesh_thought_engine(self):
        """åˆå§‹åŒ–ç½‘çŠ¶æ€ç»´å¼•æ“"""
        try:
            from src.mesh_thought_engine import MeshThoughtEngine
            
            # åˆ›å»ºç½‘çŠ¶æ€ç»´å¼•æ“å®ä¾‹ï¼ˆä¼šè‡ªåŠ¨è°ƒç”¨_load_from_storage()ï¼‰
            mesh_engine = MeshThoughtEngine()
            
            logger.info("ç½‘çŠ¶æ€ç»´å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            return mesh_engine
            
        except Exception as e:
            logger.error(f"ç½‘çŠ¶æ€ç»´å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            # è¿”å›Noneï¼Œä½†å…è®¸ç³»ç»Ÿç»§ç»­è¿è¡Œ
            return None
    
    def _determine_response_strategy(self, query: str, local_context: Dict) -> str:
        """
        ç¡®å®šå“åº”ç­–ç•¥
        
        ç­–ç•¥ç±»å‹ï¼š
        - local_only: ä»…ä½¿ç”¨æœ¬åœ°çŸ¥è¯†
        - llm_only: ä»…ä½¿ç”¨é¢„è®­ç»ƒçŸ¥è¯†
        - tool_enhanced: éœ€è¦å·¥å…·å¢å¼º
        - hybrid: æ··åˆç­–ç•¥
        """
        
        # å¦‚æœæœ‰ä¸°å¯Œçš„æœ¬åœ°è®°å¿†ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°çŸ¥è¯†
        if local_context['memory_count'] >= 3:
            return 'local_enhanced'
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–‡ä»¶æ“ä½œ
        file_keywords = ['æ–‡ä»¶', 'æ–‡æ¡£', 'è¯»å–', 'æ‰“å¼€', 'æŸ¥çœ‹']
        if any(keyword in query for keyword in file_keywords):
            return 'tool_enhanced'
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
        search_keywords = ['æœç´¢', 'æŸ¥æ‰¾', 'æœ€æ–°', 'å®æ—¶', 'æ–°é—»']
        if any(keyword in query for keyword in search_keywords):
            return 'tool_enhanced'
        
        # é»˜è®¤ä½¿ç”¨æ··åˆç­–ç•¥
        return 'hybrid'
    
    def _generate_response(self, query: str, local_context: Dict, 
                          strategy: str, use_tools: bool) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆå“åº”"""
        
        # æ„å»ºåŸºç¡€æç¤ºè¯
        base_prompt = self._build_base_prompt(query, local_context, strategy)
        
        # æ ¹æ®ç­–ç•¥è°ƒç”¨å·¥å…·
        tool_results = {}
        if use_tools and strategy == 'tool_enhanced':
            tool_results = self._call_tools(query)
            base_prompt += f"\n\nå·¥å…·è°ƒç”¨ç»“æœ:\n{tool_results}"
        
        # è°ƒç”¨LLMç”Ÿæˆå“åº”
        messages = [{"role": "user", "content": base_prompt}]
        
        # å¦‚æœæœ¬åœ°è®°å¿†ä¸ºç©ºï¼Œè°ƒæ•´ç­–ç•¥ä¸ºä½¿ç”¨é¢„è®­ç»ƒçŸ¥è¯†
        if local_context['memory_count'] == 0 and strategy == 'local_enhanced':
            strategy = 'hybrid'
            print("âš ï¸ æœ¬åœ°è®°å¿†ä¸ºç©ºï¼Œåˆ‡æ¢åˆ°æ··åˆç­–ç•¥")
        
        llm_response = self.llm_client.chat_completion(messages)
        
        # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«æ— æ³•å›ç­”çš„æç¤º
        if self._is_unhelpful_response(llm_response):
            print("âš ï¸ LLMè¿”å›äº†æ— æ³•å›ç­”çš„å“åº”ï¼Œé‡æ–°ç”Ÿæˆ")
            # é‡æ–°æ„å»ºæç¤ºè¯ï¼Œå¼ºè°ƒä½¿ç”¨é¢„è®­ç»ƒçŸ¥è¯†
            fallback_prompt = self._build_fallback_prompt(query, local_context)
            messages = [{"role": "user", "content": fallback_prompt}]
            llm_response = self.llm_client.chat_completion(messages)
        
        return {
            'response': llm_response,
            'strategy': strategy,
            'local_memories_used': local_context['memory_count'],
            'tools_used': list(tool_results.keys()) if tool_results else [],
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'query': query,
            'knowledge_sources': self._get_knowledge_sources(local_context, tool_results)
        }
    
    def _rewrite_retrieved_chunks(self, retrieved_text: str, query: str) -> str:
        """
        ä½¿ç”¨LLMé‡å†™æ£€ç´¢åˆ°çš„æ–‡æœ¬å—ï¼Œç§»é™¤æ— å…³å†…å®¹å¹¶æé«˜é€»è¾‘æµç•…æ€§
        
        å¼€å‘æç¤ºè¯æ¥æºï¼šç”¨æˆ·è¦æ±‚ç¡®ä¿æ£€ç´¢åˆ°çš„æ–‡æœ¬å—åœ¨æœ€ç»ˆè¾“å‡ºå‰è¢«LLMé‡å†™
        """
        if not retrieved_text:
            return retrieved_text
        
        try:
            rewrite_prompt = f"""
            è¯·é‡å†™ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æœ¬ï¼Œä½¿å…¶æ›´é€‚åˆå›ç­”å½“å‰ç”¨æˆ·é—®é¢˜ã€‚
            
            è¦æ±‚ï¼š
            1. ç§»é™¤æ— å…³å†…å®¹ï¼Œå¦‚IDEå‘½ä»¤ã€æ€è€ƒè¿‡ç¨‹ã€ä»£ç ç¤ºä¾‹ç­‰
            2. æé«˜æ–‡æœ¬çš„é€»è¾‘æµç•…æ€§å’Œè¿è´¯æ€§
            3. ä¿ç•™ä¸å½“å‰é—®é¢˜ç›¸å…³çš„æ ¸å¿ƒä¿¡æ¯
            4. ä¸è¦æ·»åŠ æ–°çš„ä¿¡æ¯ï¼Œåªä¼˜åŒ–ç°æœ‰å†…å®¹
            
            å½“å‰ç”¨æˆ·é—®é¢˜ï¼š{query}
            
            æ£€ç´¢åˆ°çš„æ–‡æœ¬ï¼š
            {retrieved_text}
            
            é‡å†™åçš„æ–‡æœ¬ï¼š
            """
            
            messages = [{"role": "user", "content": rewrite_prompt}]
            rewritten_text = self.llm_client.chat_completion(messages)
            
            logger.info("æ£€ç´¢æ–‡æœ¬å—é‡å†™æˆåŠŸ")
            return rewritten_text
        except Exception as e:
            logger.error(f"æ£€ç´¢æ–‡æœ¬å—é‡å†™å¤±è´¥: {e}")
            # é™çº§åˆ°åŸå§‹æ–‡æœ¬
            return retrieved_text
    
    def _build_base_prompt(self, query: str, local_context: Dict, strategy: str) -> str:
        """
        æ„å»ºåŸºç¡€æç¤ºè¯
        """
        
        prompt_parts = ["ä½ æ˜¯ä¸€ä¸ªåŸºäºRAGç³»ç»Ÿçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·å¤‡é•¿æœŸè®°å¿†èƒ½åŠ›ã€‚"]
        
        # æ·»åŠ å¯¹è¯å†å²ä¸Šä¸‹æ–‡
        if self.conversation_history:
            recent_history = self.conversation_history[-5:]  # æœ€è¿‘5è½®å¯¹è¯
            history_text = "\n".join([f"ç”¨æˆ·: {h['query']}\nåŠ©æ‰‹: {h['response']}" 
                                    for h in recent_history])
            prompt_parts.append(f"\næœ€è¿‘çš„å¯¹è¯å†å²:\n{history_text}")
        
        # æ ¹æ®ç­–ç•¥æ·»åŠ ä¸åŒå†…å®¹
        if strategy in ['local_enhanced', 'hybrid'] and local_context['context']:
            # é‡å†™æ£€ç´¢åˆ°çš„æ–‡æœ¬å—ï¼Œç§»é™¤æ— å…³å†…å®¹å¹¶æé«˜é€»è¾‘æµç•…æ€§
            rewritten_context = self._rewrite_retrieved_chunks(local_context['context'], query)
            prompt_parts.append(f"\nç›¸å…³è®°å¿†ä¸Šä¸‹æ–‡:\n{rewritten_context}")
        
        prompt_parts.append(f"\nå½“å‰ç”¨æˆ·é—®é¢˜: {query}")
        
        # æ·»åŠ å“åº”æŒ‡å¯¼
        guidance = ""
        if strategy == 'local_enhanced':
            guidance = "è¯·ä¸»è¦åŸºäºæä¾›çš„è®°å¿†ä¸Šä¸‹æ–‡è¿›è¡Œå›ç­”ï¼Œç¡®ä¿å›ç­”ä¸å·²æœ‰è®°å¿†ä¿æŒä¸€è‡´ã€‚"
        elif strategy == 'tool_enhanced':
            guidance = "è¯·ç»“åˆå·¥å…·è°ƒç”¨ç»“æœè¿›è¡Œå›ç­”ï¼Œç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ã€‚"
        else:
            guidance = "è¯·ç»“åˆè®°å¿†ä¸Šä¸‹æ–‡å’Œä½ çš„çŸ¥è¯†è¿›è¡Œå›ç­”ï¼Œç¡®ä¿å›ç­”çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚"
        
        prompt_parts.append(f"\nå›ç­”æŒ‡å¯¼: {guidance}")
        
        return "\n".join(prompt_parts)
    
    def _call_tools(self, query: str) -> Dict[str, Any]:
        """è°ƒç”¨å·¥å…·å±‚"""
        tool_results = {}
        
        # æ–‡ä»¶è¯»å–å·¥å…·
        if 'æ–‡ä»¶' in query or 'æ–‡æ¡£' in query:
            file_tool = self.tool_manager.get_tool('file_reading')
            if file_tool:
                # è¿™é‡Œå¯ä»¥å®ç°æ–‡ä»¶è·¯å¾„æå–å’Œè¯»å–é€»è¾‘
                tool_results['file_reading'] = "æ–‡ä»¶è¯»å–åŠŸèƒ½å·²å‡†å¤‡"
        
        # ç½‘ç»œæœç´¢å·¥å…·
        if 'æœç´¢' in query or 'æŸ¥æ‰¾' in query:
            search_tool = self.tool_manager.get_tool('web_search')
            if search_tool and search_tool.enabled:
                search_results = search_tool.search_web(query)
                tool_results['web_search'] = search_results
        
        return tool_results
    
    def _is_unhelpful_response(self, response: str) -> bool:
        """æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«æ— æ³•å›ç­”çš„æç¤º"""
        unhelpful_phrases = [
            'æŠ±æ­‰', 'æ— æ³•å›ç­”', 'ä¸çŸ¥é“', 'ä¸äº†è§£', 'æ²¡æœ‰ç›¸å…³ä¿¡æ¯',
            'æš‚æ—¶æ— æ³•', 'ç›®å‰æ— æ³•', 'ä¸æ¸…æ¥š', 'ä¸æ˜ç™½', 'ä¸æŒæ¡'
        ]
        
        return any(phrase in response for phrase in unhelpful_phrases)
    
    def _get_strategy_reason(self, strategy: str, local_context: Dict) -> str:
        """è·å–ç­–ç•¥é€‰æ‹©çš„åŸå› è¯´æ˜"""
        memory_count = local_context['memory_count']
        
        if strategy == 'local_enhanced':
            if memory_count > 0:
                return f"æ‰¾åˆ°{memory_count}æ¡ç›¸å…³è®°å¿†ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°çŸ¥è¯†"
            else:
                return "è™½ç„¶ç­–ç•¥ä¸ºæœ¬åœ°å¢å¼ºï¼Œä½†æœªæ‰¾åˆ°ç›¸å…³è®°å¿†"
        elif strategy == 'hybrid':
            if memory_count > 0:
                return f"æ‰¾åˆ°{memory_count}æ¡ç›¸å…³è®°å¿†ï¼Œç»“åˆæœ¬åœ°çŸ¥è¯†å’Œé¢„è®­ç»ƒçŸ¥è¯†"
            else:
                return "æœªæ‰¾åˆ°ç›¸å…³è®°å¿†ï¼Œä¸»è¦ä¾èµ–é¢„è®­ç»ƒçŸ¥è¯†"
        elif strategy == 'tool_enhanced':
            return "æŸ¥è¯¢éœ€è¦å®æ—¶ä¿¡æ¯æˆ–å·¥å…·æ”¯æŒï¼Œä½¿ç”¨å·¥å…·å¢å¼ºç­–ç•¥"
        else:  # llm_only
            return "æŸ¥è¯¢ç®€å•ï¼Œç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçŸ¥è¯†å›ç­”"
    
    def _build_fallback_prompt(self, query: str, local_context: Dict) -> str:
        """æ„å»ºå›é€€æç¤ºè¯ï¼Œå¼ºè°ƒä½¿ç”¨é¢„è®­ç»ƒçŸ¥è¯†"""
        
        prompt_parts = [
            "ä½ æ˜¯ä¸€ä¸ªåŸºäºRAGç³»ç»Ÿçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·å¤‡é•¿æœŸè®°å¿†èƒ½åŠ›ã€‚",
            "è™½ç„¶å½“å‰æŸ¥è¯¢åœ¨æœ¬åœ°è®°å¿†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›´æ¥åŒ¹é…çš„å†…å®¹ï¼Œä½†è¯·åŸºäºä½ çš„é¢„è®­ç»ƒçŸ¥è¯†è¿›è¡Œå›ç­”ã€‚"
        ]
        
        # æ·»åŠ å¯¹è¯å†å²ä¸Šä¸‹æ–‡
        if self.conversation_history:
            recent_history = self.conversation_history[-5:]  # æœ€è¿‘5è½®å¯¹è¯
            history_text = "\n".join([f"ç”¨æˆ·: {h['query']}\nåŠ©æ‰‹: {h['response']}" 
                                    for h in recent_history])
            prompt_parts.append(f"\næœ€è¿‘çš„å¯¹è¯å†å²:\n{history_text}")
        
        # æ·»åŠ æŸ¥è¯¢
        prompt_parts.append(f"\nå½“å‰ç”¨æˆ·é—®é¢˜: {query}")
        
        # æ·»åŠ æŒ‡å¯¼
        guidance = ""
        if local_context['memory_count'] == 0:
            guidance = "è™½ç„¶æœ¬åœ°è®°å¿†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œä½†è¯·åŸºäºä½ çš„é¢„è®­ç»ƒçŸ¥è¯†æä¾›æœ‰ä»·å€¼çš„å›ç­”ã€‚"
        else:
            guidance = "è¯·ç»“åˆè®°å¿†ä¸Šä¸‹æ–‡å’Œä½ çš„çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚"
        
        prompt_parts.append(f"\nå›ç­”æŒ‡å¯¼: {guidance}")
        prompt_parts.append("\né‡è¦æç¤º: è¯·ä¸è¦è¯´'æŠ±æ­‰æ— æ³•å›ç­”'æˆ–ç±»ä¼¼çš„è¯ï¼Œå³ä½¿æ²¡æœ‰æ‰¾åˆ°æœ¬åœ°è®°å¿†ï¼Œä¹Ÿè¦åŸºäºä½ çš„çŸ¥è¯†æä¾›æœ‰ä»·å€¼çš„å›ç­”ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _update_conversation_history(self, query: str, response: Dict):
        """æ›´æ–°å¯¹è¯å†å²"""
        history_entry = {
            'query': query,
            'response': response['response'],
            'timestamp': response['timestamp'],
            'strategy': response['strategy']
        }
        
        self.conversation_history.append(history_entry)
        
        # é™åˆ¶å†å²è®°å½•é•¿åº¦
        if len(self.conversation_history) > 50:
            self.conversation_history = self.conversation_history[-50:]
        
        # ä¿å­˜äº¤äº’ä¿¡æ¯åˆ°å‘é‡æ•°æ®åº“
        self._save_interaction_to_vector_db(query, response)
    
    def trigger_memory_iteration(self, topic: str = None) -> Optional[Dict]:
        """è§¦å‘è®°å¿†è¿­ä»£"""
        iteration_tool = self.tool_manager.get_tool('memory_iteration')
        
        if not iteration_tool:
            return None
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šä¸»é¢˜ï¼Œä½¿ç”¨æœ€è¿‘å¯¹è¯çš„ä¸»é¢˜
        if not topic and self.conversation_history:
            # ä»æœ€è¿‘å¯¹è¯ä¸­æå–ä¸»é¢˜
            recent_queries = [h['query'] for h in self.conversation_history[-5:]]
            topic = " ".join(recent_queries)
        
        if topic:
            return iteration_tool.summarize_related_memories(topic)
        
        return None
    
    def get_conversation_stats(self) -> Dict[str, Any]:
        """è·å–å¯¹è¯ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_conversations': len(self.conversation_history),
            'recent_strategies': [h['strategy'] for h in self.conversation_history[-10:]],
            'memory_usage_stats': self._get_memory_usage_stats()
        }
    
    def _get_knowledge_sources(self, local_context: Dict, tool_results: Dict) -> List[str]:
        """è·å–çŸ¥è¯†æ¥æºä¿¡æ¯"""
        sources = []
        
        # æœ¬åœ°è®°å¿†æ¥æº
        if local_context['memory_count'] > 0:
            sources.append(f"æœ¬åœ°è®°å¿†åº“ ({local_context['memory_count']}æ¡ç›¸å…³è®°å¿†)")
        
        # å·¥å…·æ¥æº
        if tool_results:
            for tool_name in tool_results.keys():
                if tool_name == 'file_reading':
                    sources.append("æ–‡ä»¶è¯»å–å·¥å…·")
                elif tool_name == 'web_search':
                    sources.append("ç½‘ç»œæœç´¢å·¥å…·")
        
        # é¢„è®­ç»ƒçŸ¥è¯†æ¥æºï¼ˆå¦‚æœæ²¡æœ‰å…¶ä»–æ¥æºï¼‰
        if not sources:
            sources.append("é¢„è®­ç»ƒçŸ¥è¯†åº“")
        else:
            sources.append("é¢„è®­ç»ƒçŸ¥è¯†åº“")
        
        return sources
    
    def _get_memory_usage_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ä½¿ç”¨ç»Ÿè®¡"""
        if not self.conversation_history:
            return {
                'recent_memory_usage_rate': 0.0,
                'preferred_strategy': 'hybrid'
            }
        
        # ç»Ÿè®¡æœ€è¿‘å¯¹è¯ä¸­è®°å¿†çš„ä½¿ç”¨æƒ…å†µ
        recent_history = self.conversation_history[-10:]
        recent_with_memory = [h for h in recent_history 
                             if h['strategy'] in ['local_enhanced', 'hybrid']]
        
        # é¿å…é™¤é›¶é”™è¯¯
        denominator = min(10, len(self.conversation_history))
        memory_usage_rate = len(recent_with_memory) / denominator if denominator > 0 else 0.0
        
        # è®¡ç®—æœ€å¸¸ç”¨çš„ç­–ç•¥
        strategy_counts = {}
        for h in recent_history:
            strategy = h['strategy']
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1
        
        preferred_strategy = max(strategy_counts.items(), key=lambda x: x[1])[0] if strategy_counts else 'hybrid'
        
        return {
            'recent_memory_usage_rate': memory_usage_rate,
            'preferred_strategy': preferred_strategy
        }
    
    def _save_interaction_to_vector_db(self, query: str, response: Dict):
        """âœ… å°†äº¤äº’ä¿¡æ¯ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ï¼ˆèŒè´£å½’ä½è‡³UnifiedMemorySystemï¼‰
        
        æµç¨‹ï¼š
        1. è°ƒç”¨UnifiedMemorySystemçš„ç»Ÿä¸€å‘é‡åŒ–å­˜å‚¨æ¥å£
        2. UnifiedMemorySystemè°ƒç”¨MemorySlicerToolè¿›è¡Œåˆ†ç‰‡
        3. UnifiedMemorySystemè°ƒç”¨MeshDatabaseInterfaceå¤„ç†å»é‡
        """
        try:
            # ğŸ” æ£€æŸ¥responseä¸­æ˜¯å¦å·²æœ‰å‘é‡åŒ–æ ‡è®°
            if response.get('vectorized', False):
                logger.debug(f"è·³è¿‡å·²å‘é‡åŒ–çš„äº¤äº’è®°å½•: {response.get('timestamp', 'unknown')}")
                return
            
            # âœ… è°ƒç”¨ç»Ÿä¸€è®°å¿†ç³»ç»Ÿï¼ˆèŒè´£å½’ä½ï¼‰
            from src.unified_memory_system import get_unified_memory_system
            from pathlib import Path
            
            memory_system = get_unified_memory_system(str(Path.cwd()))
            
            # å‡†å¤‡äº¤äº’å†…å®¹
            interaction_content = f"ç”¨æˆ·: {query}\nåŠ©æ‰‹: {response['response']}"
            
            # âœ… è°ƒç”¨ç»Ÿä¸€è®°å¿†ç³»ç»Ÿçš„å‘é‡åŒ–å­˜å‚¨æ¥å£
            result = memory_system.store_interaction_to_vector_db(
                interaction_content=interaction_content,
                metadata={
                    "source": "chat_engine",
                    "source_type": "chat_interaction",
                    "sender": "user_assistant",
                    "timestamp": response['timestamp'],
                    "topic": f"èŠå¤©äº¤äº’ - {response['strategy']}",
                    "tags": ["chat", "interaction", response['strategy']]
                }
            )
            
            saved_count = result.get('saved_count', 0)
            duplicate_count = result.get('duplicate_count', 0)
            
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {saved_count} ä¸ªåˆ‡ç‰‡ï¼Œè·³è¿‡ {duplicate_count} ä¸ªé‡å¤")
            
            # âœ… æ ‡è®°ä¸ºå·²å‘é‡åŒ–
            response['vectorized'] = True
            response['saved_count'] = saved_count
            response['duplicate_count'] = duplicate_count
            
        except Exception as e:
            logger.warning(f"ä¿å­˜äº¤äº’ä¿¡æ¯åˆ°å‘é‡åº“å¤±è´¥: {e}")
    
    def _generate_content_vector(self, text: str) -> list:
        """ç”Ÿæˆæ–‡æœ¬å†…å®¹çš„ç®€å•å‘é‡è¡¨ç¤º"""
        # ç®€åŒ–çš„å‘é‡ç”Ÿæˆæ–¹æ³•ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨ä¸“ä¸šçš„embeddingæ¨¡å‹ï¼‰
        if not text:
            return [0.0] * 12  # 12ç»´å‘é‡
        
        # åŸºäºæ–‡æœ¬é•¿åº¦ã€å…³é”®è¯ç­‰ç”Ÿæˆç®€å•å‘é‡
        vector = []
        
        # 1. æ–‡æœ¬é•¿åº¦ç‰¹å¾
        length_feature = min(len(text) / 1000, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
        vector.append(length_feature)
        
        # 2. å…³é”®è¯ç‰¹å¾ï¼ˆæ¶æ„ç›¸å…³ï¼‰
        arch_keywords = ["æ¶æ„", "è®¾è®¡", "ç³»ç»Ÿ", "æ¨¡å—"]
        arch_score = sum(1 for word in arch_keywords if word in text) / len(arch_keywords)
        vector.append(arch_score)
        
        # 3. å…³é”®è¯ç‰¹å¾ï¼ˆè¯„ä¼°ç›¸å…³ï¼‰
        eval_keywords = ["è¯„ä¼°", "é£é™©", "å¯è¡Œæ€§", "æˆæœ¬"]
        eval_score = sum(1 for word in eval_keywords if word in text) / len(eval_keywords)
        vector.append(eval_score)
        
        # 4. å…³é”®è¯ç‰¹å¾ï¼ˆå®ç°ç›¸å…³ï¼‰
        impl_keywords = ["å®ç°", "ä»£ç ", "æŠ€æœ¯", "å¼€å‘"]
        impl_score = sum(1 for word in impl_keywords if word in text) / len(impl_keywords)
        vector.append(impl_score)
        
        # 5-12. å¡«å……å…¶ä»–ç‰¹å¾
        for i in range(8):
            vector.append(0.1)  # å ä½ç‰¹å¾
        
        # å½’ä¸€åŒ–å‘é‡
        norm = sum(x**2 for x in vector) ** 0.5
        if norm > 0:
            vector = [x / norm for x in vector]
        
        return vector
    
    def close(self):
        """å…³é—­å¼•æ“"""
        self.tool_manager.close()



def create_chat_engine() -> ChatEngine:
    """åˆ›å»ºèŠå¤©å¼•æ“å®ä¾‹"""
    return ChatEngine()