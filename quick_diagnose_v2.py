#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""å¿«é€Ÿè¯Šæ–­v2.0ï¼šåˆ†ææ‰€æœ‰è®°å¿†æ‰¾å‡ºä½è´¨é‡æ¡ˆä¾‹"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from src.mesh_database_interface import MeshDatabaseInterface
from tools.induction_engine import (
    summarize_topic, _split_sentences, _score_sentence, 
    _extract_tfidf_keywords, _calculate_sentence_entropy,
    _calculate_sentence_perplexity
)

def main():
    print("=" * 100)
    print("v2.0è´¨é‡è¯Šæ–­ï¼šæ‰¾å‡ºä½è´¨é‡æ¡ˆä¾‹å¹¶åˆ†æåŸå› ")
    print("=" * 100)
    
    # åŠ è½½æ•°æ®
    db = MeshDatabaseInterface()
    all_memories = db.vector_db.get_all_memories()
    memories = sorted(all_memories, key=lambda m: m.get('timestamp', ''), reverse=True)[:30]
    
    print(f"\nğŸ“Š è·å–åˆ° {len(memories)} æ¡æœ€æ–°è®°å¿†\n")
    
    # æµ‹è¯•æ‰€æœ‰è®°å¿†
    results = []
    for i, mem in enumerate(memories, 1):
        content = mem.get('content', '')
        if not content or len(content) < 50:
            continue
        
        result = summarize_topic(content, max_sentences=3, max_chars=280)
        
        # è´¨é‡è¯„åˆ†
        score = 0
        issues = []
        
        if result['topic_summary']:
            score += 1
        
        keyword_coverage = result['stats'].get('keyword_coverage', 0)
        if keyword_coverage > 0.15:
            score += 2
        else:
            issues.append(f"å…³é”®è¯è¦†ç›–ç‡ä½: {keyword_coverage:.2%}")
        
        if result['key_points']:
            score += 1
        
        compression_ratio = result['stats'].get('compression_ratio', 1.0)
        if 0.1 <= compression_ratio <= 0.5:
            score += 1
        else:
            issues.append(f"å‹ç¼©ç‡å¼‚å¸¸: {compression_ratio:.2%}")
        
        quality = "ä¼˜ç§€" if score >= 4 else ("è‰¯å¥½" if score >= 3 else "éœ€è¦æ”¹è¿›")
        
        results.append({
            'id': mem['id'],
            'content_len': len(content),
            'summary': result['topic_summary'],
            'quality': quality,
            'score': score,
            'keyword_coverage': keyword_coverage,
            'compression_ratio': compression_ratio,
            'issues': issues,
            'content': content
        })
        
        if quality != "ä¼˜ç§€":
            print(f"[{i}] {quality} ({score}/5) | {mem['id'][:30]}... | è¦†ç›–ç‡{keyword_coverage:.1%}")
    
    # ç»Ÿè®¡
    excellent = [r for r in results if r['quality'] == "ä¼˜ç§€"]
    good = [r for r in results if r['quality'] == "è‰¯å¥½"]
    poor = [r for r in results if r['quality'] == "éœ€è¦æ”¹è¿›"]
    
    print(f"\n" + "=" * 100)
    print(f"ğŸ“Š è´¨é‡ç»Ÿè®¡")
    print(f"=" * 100)
    print(f"ä¼˜ç§€: {len(excellent)} ({len(excellent)/len(results)*100:.1f}%)")
    print(f"è‰¯å¥½: {len(good)} ({len(good)/len(results)*100:.1f}%)")
    print(f"éœ€è¦æ”¹è¿›: {len(poor)} ({len(poor)/len(results)*100:.1f}%)")
    
    # åˆ†æä½è´¨é‡æ¡ˆä¾‹
    if poor:
        print(f"\n" + "=" * 100)
        print(f"ğŸ” åˆ†æéœ€è¦æ”¹è¿›çš„æ¡ˆä¾‹ï¼ˆå…±{len(poor)}ä¸ªï¼‰")
        print(f"=" * 100)
        
        for idx, case in enumerate(poor[:3], 1):  # åªåˆ†æå‰3ä¸ª
            print(f"\nã€æ¡ˆä¾‹{idx}ã€‘")
            print(f"ID: {case['id']}")
            print(f"è´¨é‡: {case['quality']} ({case['score']}/5)")
            print(f"å…³é”®è¯è¦†ç›–ç‡: {case['keyword_coverage']:.2%}")
            print(f"å‹ç¼©ç‡: {case['compression_ratio']:.2%}")
            print(f"é—®é¢˜: {', '.join(case['issues'])}")
            print(f"\nåŸæ–‡é•¿åº¦: {case['content_len']} å­—ç¬¦")
            print(f"åŸæ–‡å‰300å­—ç¬¦:")
            print(f"  {case['content'][:300]}...")
            print(f"\næ‘˜è¦: {case['summary']}")
            
            # æ·±åº¦åˆ†æ
            content = case['content']
            sentences = _split_sentences(content)
            tfidf_keywords = _extract_tfidf_keywords(content, top_k=15)
            
            print(f"\nTF-IDFå…³é”®è¯: {', '.join(tfidf_keywords[:10])}")
            
            # åˆ†æä¿¡æ¯ç†µå’Œå›°æƒ‘åº¦
            entropy_scores = []
            fluency_scores = []
            for s in sentences[:10]:
                entropy_scores.append(_calculate_sentence_entropy(s))
                fluency_scores.append(_calculate_sentence_perplexity(s))
            
            avg_entropy = sum(entropy_scores) / len(entropy_scores) if entropy_scores else 0
            avg_fluency = sum(fluency_scores) / len(fluency_scores) if fluency_scores else 0
            
            print(f"å¹³å‡ä¿¡æ¯ç†µï¼ˆå‰10å¥ï¼‰: {avg_entropy:.3f}")
            print(f"å¹³å‡æµç•…åº¦ï¼ˆå‰10å¥ï¼‰: {avg_fluency:.3f}")
            
            # è¯Šæ–­
            print(f"\nğŸ¯ é—®é¢˜è¯Šæ–­:")
            if avg_entropy < 0.4:
                print(f"  âš ï¸ ä¿¡æ¯ç†µè¿‡ä½ ({avg_entropy:.3f}) â†’ å¯èƒ½æ˜¯é‡å¤æ€§/æ¨¡æ¿åŒ–æ–‡æœ¬")
            if avg_fluency < 0.4:
                print(f"  âš ï¸ æµç•…åº¦è¿‡ä½ ({avg_fluency:.3f}) â†’ å¯èƒ½æ˜¯ä»£ç /æ—¥å¿—/ç»“æ„åŒ–æ•°æ®")
            if case['keyword_coverage'] < 0.1:
                print(f"  âŒ å…³é”®è¯è¦†ç›–ç‡æä½ â†’ TF-IDFå…³é”®è¯ä¸æ‘˜è¦ä¸åŒ¹é…")
            
            # æ£€æŸ¥æ–‡æœ¬ç±»å‹
            is_code = any(line.strip().startswith(('def ', 'class ', 'import ', '#', '//')) 
                         for line in content.split('\n')[:20])
            is_log = any(keyword in content[:500] for keyword in ['Traceback', 'Error:', 'File "', 'line '])
            
            if is_code:
                print(f"  ğŸ“ æ–‡æœ¬ç±»å‹: ä»£ç ç±»è®°å¿† â†’ éœ€é’ˆå¯¹ä»£ç ä¼˜åŒ–è¯„åˆ†ç­–ç•¥")
            if is_log:
                print(f"  ğŸ“‹ æ–‡æœ¬ç±»å‹: æ—¥å¿—ç±»è®°å¿† â†’ éœ€æé«˜é”™è¯¯å…³é”®è¯æƒé‡")
    
    print(f"\n" + "=" * 100)
    print(f"âœ… è¯Šæ–­å®Œæˆ")
    print(f"=" * 100)

if __name__ == "__main__":
    main()
