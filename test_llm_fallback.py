#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•LLMå›é€€æœºåˆ¶ä¸å¤šæœåŠ¡å•†é…ç½®
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_api_keys_config():
    """æµ‹è¯•APIå¯†é’¥é…ç½®"""
    print("=" * 60)
    print("1. æµ‹è¯•APIå¯†é’¥é…ç½®")
    print("=" * 60)
    
    try:
        from config.system_config import api_key_manager, API_ENDPOINTS
        
        # åˆ—å‡ºé…ç½®çš„å¯†é’¥
        keys_dict = api_key_manager.list_keys()
        print(f"\nå·²é…ç½®çš„APIå¯†é’¥: {list(keys_dict.keys())}")
        
        # åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„ç«¯ç‚¹
        print(f"\næ”¯æŒçš„LLMç«¯ç‚¹: {list(API_ENDPOINTS.keys())}")
        
        # æ£€æŸ¥æ¯ä¸ªç«¯ç‚¹æ˜¯å¦æœ‰å¯†é’¥
        print("\nå¯†é’¥çŠ¶æ€æ£€æŸ¥:")
        for provider in API_ENDPOINTS.keys():
            key = api_key_manager.get_key(provider)
            status = "âœ… å·²é…ç½®" if key else "âŒ æœªé…ç½®"
            print(f"  {provider}: {status}")
            
    except Exception as e:
        print(f"âŒ é…ç½®æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_single_llm_call():
    """æµ‹è¯•å•ä¸ªLLMè°ƒç”¨"""
    print("\n" + "=" * 60)
    print("2. æµ‹è¯•å•ä¸ªLLMè°ƒç”¨")
    print("=" * 60)
    
    try:
        from src.llm_client_enhanced import LLMClientEnhanced
        from config.system_config import LLM_CONFIG
        
        default_provider = LLM_CONFIG.get("default_provider", "qianwen")
        print(f"\né»˜è®¤æœåŠ¡å•†: {default_provider}")
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        print(f"\næ­£åœ¨åˆ›å»º {default_provider} å®¢æˆ·ç«¯...")
        client = LLMClientEnhanced(provider=default_provider)
        print(f"âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        
        # å‘èµ·æµ‹è¯•è¯·æ±‚
        print(f"\nå‘èµ·æµ‹è¯•è¯·æ±‚...")
        messages = [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·å›å¤'åœ¨çº¿'ä¸¤ä¸ªå­—å³å¯"}
        ]
        
        response = client.chat_completion(messages)
        
        if response:
            print(f"âœ… LLMè°ƒç”¨æˆåŠŸ")
            print(f"å“åº”å†…å®¹: {response[:100]}")
        else:
            print(f"âŒ LLMè¿”å›ç©ºç»“æœï¼ˆNoneï¼‰")
            
    except ValueError as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥ï¼ˆå¯†é’¥æœªé…ç½®ï¼‰: {e}")
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_available_providers():
    """æµ‹è¯•å¯ç”¨æœåŠ¡å•†æ£€æµ‹"""
    print("\n" + "=" * 60)
    print("3. æµ‹è¯•å¯ç”¨æœåŠ¡å•†æ£€æµ‹")
    print("=" * 60)
    
    try:
        from src.llm_client_enhanced import LLMClientEnhanced
        
        # å°è¯•è·å–å¯ç”¨æœåŠ¡å•†åˆ—è¡¨
        try:
            # ç”¨qianwenåˆå§‹åŒ–ï¼ˆå› ä¸ºå®ƒå·²é…ç½®ï¼‰
            client = LLMClientEnhanced(provider="qianwen")
            available = client.get_available_providers()
            print(f"\nå¯ç”¨çš„LLMæœåŠ¡å•†: {available}")
            
            if len(available) < 2:
                print(f"\nâš ï¸ è­¦å‘Šï¼šåªæœ‰ {len(available)} ä¸ªæœåŠ¡å•†å¯ç”¨ï¼Œæ— æ³•å®ç°å›é€€æœºåˆ¶ï¼")
                print(f"å»ºè®®ï¼šè‡³å°‘é…ç½®ä¸¤ä¸ªLLMæœåŠ¡å•†çš„APIå¯†é’¥")
        except Exception as e:
            print(f"âŒ æ— æ³•æ£€æµ‹å¯ç”¨æœåŠ¡å•†: {e}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def test_fallback_scenario():
    """æµ‹è¯•å›é€€åœºæ™¯ï¼ˆæ¨¡æ‹Ÿç¬¬ä¸€ä¸ªLLMå¤±è´¥ï¼‰"""
    print("\n" + "=" * 60)
    print("4. æµ‹è¯•å›é€€åœºæ™¯ï¼ˆæ¨¡æ‹Ÿï¼‰")
    print("=" * 60)
    
    print("\nå½“å‰ä»£ç åˆ†æ:")
    print("  - LLMClientEnhancedåªèƒ½åˆå§‹åŒ–ä¸€ä¸ªprovider")
    print("  - _make_requestå¤±è´¥æ—¶è¿”å›Noneï¼Œæ— åˆ‡æ¢é€»è¾‘")
    print("  - chat_apiçš„_generate_llm_responseæœªå¤„ç†fallback")
    print("\nç»“è®º: âŒ å½“å‰æ²¡æœ‰å®ç°åŒLLMå›é€€æœºåˆ¶ï¼")

if __name__ == "__main__":
    print("\nğŸ” LLMå›é€€æœºåˆ¶è¯Šæ–­å·¥å…·\n")
    
    test_api_keys_config()
    test_single_llm_call()
    test_available_providers()
    test_fallback_scenario()
    
    print("\n" + "=" * 60)
    print("è¯Šæ–­å®Œæˆ")
    print("=" * 60)
