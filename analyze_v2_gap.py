#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""v2.0è´¨é‡å·®è·è¯Šæ–­ï¼šæ‰¾å‡ºä¸ºä»€ä¹ˆä¿¡æ¯ç†µ+å›°æƒ‘åº¦ä¹Ÿæ²¡åˆ°90%"""

import sys
import json
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.mesh_database_interface import MeshDatabaseInterface
from tools.induction_engine import summarize_topic

def analyze_failed_case():
    """åˆ†æå¤±è´¥æ¡ˆä¾‹ï¼šå…³é”®è¯é‡å åº¦ä½çš„æ ¹æœ¬åŸå› """
    
    print("=" * 80)
    print("v2.0è´¨é‡å·®è·è¯Šæ–­ï¼šä¸ºä»€ä¹ˆä¿¡æ¯ç†µ+å›°æƒ‘åº¦ä¹Ÿæ²¡åˆ°90%ï¼Ÿ")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    db = MeshDatabaseInterface()
    all_memories = db.vector_db.get_all_memories()
    memories = sorted(all_memories, key=lambda m: m.get('timestamp', ''), reverse=True)[:50]
    
    print(f"\nğŸ“Š è·å–åˆ° {len(memories)} æ¡è®°å¿†ç”¨äºåˆ†æ\n")
    
    # æ‰¾åˆ°é—®é¢˜è®°å¿†
    target_id = 'mem_-3257990327454786600'
    target_memory = None
    for mem in memories:
        if mem['id'] == target_id:
            target_memory = mem
            break
    
    if not target_memory:
        print(f"âš ï¸ æœªæ‰¾åˆ°ç›®æ ‡è®°å¿† {target_id}")
        return
    
    print(f"ğŸ¯ æ‰¾åˆ°é—®é¢˜è®°å¿†: {target_id}")
    print(f"åŸæ–‡é•¿åº¦: {len(target_memory['content'])} å­—ç¬¦")
    print(f"\nåŸæ–‡å†…å®¹ï¼ˆå‰800å­—ç¬¦ï¼‰:")
    print("-" * 80)
    print(target_memory['content'][:800])
    print("-" * 80)
    
    # è°ƒç”¨å½’çº³å¼•æ“
    result = summarize_topic(target_memory['content'], max_sentences=3, max_chars=280)
    
    print(f"\nğŸ“ å½’çº³å¼•æ“v2.0è¾“å‡º:")
    print(f"æ‘˜è¦: {result['topic_summary']}")
    print(f"æ‘˜è¦é•¿åº¦: {len(result['topic_summary'])} å­—ç¬¦")
    print(f"å‹ç¼©ç‡: {result['stats']['compression_ratio']:.2%}")
    print(f"å…³é”®è¯è¦†ç›–ç‡: {result['stats']['keyword_coverage']:.2%}")
    print(f"TF-IDFå…³é”®è¯: {result['tfidf_keywords']}")
    
    # è¯Šæ–­é—®é¢˜
    print(f"\nğŸ” è¯Šæ–­åˆ†æ:")
    
    # 1. æ£€æŸ¥åŸæ–‡ç‰¹å¾
    content = target_memory['content']
    lines = content.split('\n')
    print(f"\nã€åŸæ–‡ç‰¹å¾ã€‘")
    print(f"  - æ€»è¡Œæ•°: {len(lines)}")
    print(f"  - å¹³å‡è¡Œé•¿: {len(content) / len(lines):.1f} å­—ç¬¦")
    print(f"  - æ˜¯å¦åŒ…å«ä»£ç : {'æ˜¯' if any(line.strip().startswith(('#', 'def', 'class', 'import')) for line in lines) else 'å¦'}")
    print(f"  - æ˜¯å¦åŒ…å«åˆ—è¡¨: {'æ˜¯' if any(line.strip().startswith(('-', '*', '1.', '2.')) for line in lines) else 'å¦'}")
    
    # 2. æ£€æŸ¥æ‘˜è¦å¥å­çš„è¯„åˆ†
    from tools.induction_engine import _split_sentences, _score_sentence, _extract_tfidf_keywords
    
    sentences = _split_sentences(content)
    tfidf_keywords = _extract_tfidf_keywords(content, top_k=15)
    
    print(f"\nã€å¥å­è¯„åˆ†åˆ†æã€‘ï¼ˆå‰10å¥ï¼‰")
    scored = []
    for i, s in enumerate(sentences[:10]):
        score = _score_sentence(s, i, len(sentences), len(content), tfidf_keywords)
        scored.append((score, i, s[:60]))
        print(f"  [{i}] å¾—åˆ†: {score:.2f} | {s[:60]}...")
    
    scored_all = [(_score_sentence(s, i, len(sentences), len(content), tfidf_keywords), i, s) 
                  for i, s in enumerate(sentences)]
    scored_all.sort(key=lambda x: -x[0])
    
    print(f"\nã€æœ€é«˜å¾—åˆ†å¥å­TOP 5ã€‘")
    for rank, (score, idx, s) in enumerate(scored_all[:5], 1):
        print(f"  {rank}. [ä½ç½®{idx}] å¾—åˆ†: {score:.2f}")
        print(f"     {s[:100]}...")
    
    # 3. å…³é”®è¯è¦†ç›–åˆ†æ
    print(f"\nã€å…³é”®è¯è¦†ç›–åˆ†æã€‘")
    print(f"  - TF-IDFå…³é”®è¯: {', '.join(tfidf_keywords[:10])}")
    
    summary_words = set(result['topic_summary'].lower().split())
    matched_keywords = [kw for kw in tfidf_keywords if kw.lower() in result['topic_summary'].lower()]
    print(f"  - æ‘˜è¦ä¸­åŒ…å«çš„å…³é”®è¯: {', '.join(matched_keywords)}")
    print(f"  - è¦†ç›–ç‡: {len(matched_keywords)}/{len(tfidf_keywords)} = {len(matched_keywords)/len(tfidf_keywords)*100:.1f}%")
    
    # 4. è¯Šæ–­ç»“è®º
    print(f"\nğŸ¯ è¯Šæ–­ç»“è®º:")
    
    if result['stats']['keyword_coverage'] < 0.15:
        print(f"  âŒ é—®é¢˜1: å…³é”®è¯è¦†ç›–ç‡è¿‡ä½ ({result['stats']['keyword_coverage']:.2%} < 15%)")
        print(f"     â†’ å¯èƒ½åŸå› : TF-IDFå…³é”®è¯ä¸é«˜åˆ†å¥å­ä¸åŒ¹é…")
    
    if result['stats']['compression_ratio'] > 0.5:
        print(f"  âš ï¸ é—®é¢˜2: å‹ç¼©ç‡è¿‡é«˜ ({result['stats']['compression_ratio']:.2%} > 50%)")
        print(f"     â†’ å¯èƒ½åŸå› : æ‘˜è¦è¿‡é•¿ï¼ŒæœªæŠ“ä½æ ¸å¿ƒ")
    
    # 5. ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ä»£ç /æ—¥å¿—ç±»è®°å¿†
    if any(line.strip().startswith(('Traceback', 'Error', 'File "')) for line in lines):
        print(f"  1. è¿™æ˜¯é”™è¯¯æ—¥å¿—ç±»è®°å¿†ï¼Œå»ºè®®ï¼š")
        print(f"     - æé«˜é”™è¯¯å…³é”®è¯æƒé‡ï¼ˆError, Traceback, Exceptionï¼‰")
        print(f"     - ä¼˜å…ˆæå–é”™è¯¯ä½ç½®å’Œé”™è¯¯ä¿¡æ¯")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´ä¿¡æ¯ç†µ/å›°æƒ‘åº¦æƒé‡
    entropy_scores = []
    fluency_scores = []
    from tools.induction_engine import _calculate_sentence_entropy, _calculate_sentence_perplexity
    
    for s in sentences[:20]:
        entropy_scores.append(_calculate_sentence_entropy(s))
        fluency_scores.append(_calculate_sentence_perplexity(s))
    
    avg_entropy = sum(entropy_scores) / len(entropy_scores) if entropy_scores else 0
    avg_fluency = sum(fluency_scores) / len(fluency_scores) if fluency_scores else 0
    
    print(f"  2. ä¿¡æ¯ç†µ/å›°æƒ‘åº¦ç»Ÿè®¡ï¼ˆå‰20å¥ï¼‰:")
    print(f"     - å¹³å‡ä¿¡æ¯ç†µ: {avg_entropy:.3f}")
    print(f"     - å¹³å‡æµç•…åº¦: {avg_fluency:.3f}")
    
    if avg_entropy < 0.5:
        print(f"     â†’ ä¿¡æ¯ç†µæ™®éåä½ï¼Œå¯èƒ½æ˜¯é‡å¤æ€§æ–‡æœ¬")
    if avg_fluency < 0.5:
        print(f"     â†’ æµç•…åº¦åä½ï¼Œå¯èƒ½æ˜¯ä»£ç æˆ–æ—¥å¿—æ–‡æœ¬")
    
    print("\n" + "=" * 80)

if __name__ == "__main__":
    analyze_failed_case()
