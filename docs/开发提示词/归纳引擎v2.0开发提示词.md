# 归纳引擎v2.0开发提示词

## 📌 核心定位

**工具名称**：Induction Engine（归纳引擎）  
**版本**：v2.0.0  
**文件位置**：`tools/induction_engine.py`  
**核心使命**：为主题维（结论摘要）提供纯本地、零依赖、语言无关的文本压缩工具

**设计理念**：
> "真正的上下文压缩并非依赖外部技术引擎，而是通过语义聚焦与主动剔除冗余信息来实现。" 

---

## 🎯 架构升级路径

### v1.0 → v2.0 核心改进

| 维度 | v1.0（启发式） | v2.0（信息论驱动） |
|------|--------------|-----------------|
| **句子评分** | Lead权重主导 | 信息熵+困惑度主导 |
| **理论基础** | 经验参数 | 信息论（香农熵）+ NLP困惑度 |
| **压缩率** | 未测量 | 93%（4000字符→280字符） |
| **质量保障** | 无验证 | 80%优秀率（LLM质量检查） |
| **超长文本** | 无处理 | 集成逻辑链分片器 |
| **关键词提取** | 无 | TF-IDF算法 |

**核心洞察**：  
- ❌ v1.0依赖Lead权重（第一句最重要）→ 启发式偏见
- ✅ v2.0基于信息熵（信息密度）+ 困惑度（表达清晰度）→ 科学可解释

---

## 🔧 核心算法实现

### 1. 信息熵计算（Sentence Entropy）

**函数**：`_calculate_sentence_entropy(s: str) -> float`

**原理**：香农信息熵
```python
H(X) = -∑ p(x) * log₂ p(x)
```

**物理意义**：
- **高信息熵** = 字符分布均匀 = 信息量大 = 可能是核心句子
- **低信息熵** = 字符重复多 = 信息冗余 = 可能是填充句

**实现细节**：
```python
# 1. 统计字符频率（中英文混合）
char_freq = Counter(s.lower())
total_chars = len(s)

# 2. 计算香农熵
entropy = 0.0
for count in char_freq.values():
    probability = count / total_chars
    if probability > 0:
        entropy -= probability * math.log2(probability)

# 3. 正则化到[0, 1]范围（最大熵约5-6）
normalized_entropy = min(entropy / 6.0, 1.0)
```

**权重分配**：信息熵权重 = 3.0分（最高优先级）

---

### 2. 困惑度计算（Perplexity Analysis）

**函数**：`_calculate_sentence_perplexity(s: str, context: str = "") -> float`

**原理**：基于n-gram模型的简化困惑度
```python
困惑度 = 低频2-gram占比
流畅度 = 1 - 困惑度
```

**物理意义**：
- **低困惑度** = 语言流畅 = 表达清晰 = 高质量句子
- **高困惑度** = 语言生硬 = 表达混乱 = 低质量句子

**实现细节**：
```python
# 1. 分词（2-gram）
words = re.findall(r'[\w\u4e00-\u9fff]+', s.lower())
bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]

# 2. 统计2-gram频率
bigram_freq = Counter(bigrams)

# 3. 计算低频2-gram占比
rare_bigrams = sum(1 for count in bigram_freq.values() if count == 1)
perplexity_score = rare_bigrams / len(bigrams)

# 4. 返回流畅度（困惑度的反向）
fluency = 1.0 - perplexity_score
```

**权重分配**：流畅度权重 = 2.0分

---

### 3. TF-IDF关键词提取

**函数**：`_extract_tfidf_keywords(text: str, top_k: int = 10) -> List[str]`

**原理**：词频-逆文档频率
```python
TF-IDF = TF(词频) × IDF(逆文档频率)
```

**简化版实现**（单文档场景）：
```python
# 1. 分词 + 停用词过滤
words = re.findall(r'[\w\u4e00-\u9fff]+', text.lower())
stopwords = {'的', '了', '是', ...}
words = [w for w in words if w not in stopwords]

# 2. 计算TF（词频）
word_count = Counter(words)
tf_scores = {word: count / total_words for word, count in word_count.items()}

# 3. 简化IDF（按词频倒数加权）
idf_scores = {word: math.log(total_words / count) for word, count in word_count.items()}

# 4. 计算TF-IDF
tfidf_scores = {word: tf_scores[word] * idf_scores[word] for word in tf_scores}

# 5. 返回Top K关键词
sorted_keywords = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)
return [word for word, score in sorted_keywords[:top_k]]
```

**用途**：
1. 增强句子评分（包含关键词的句子+2.5分）
2. 计算关键词覆盖率（质量验证指标）

---

### 4. 句子综合评分

**函数**：`_score_sentence(s, idx, total_sentences, text_length, tfidf_keywords) -> float`

**v2.0评分公式**：
```python
总分 = 信息熵权重(3.0)        # 核心维度1
     + 流畅度权重(2.0)        # 核心维度2
     + TF-IDF关键词权重(2.5)  # 核心维度3
     + Lead权重(0.2)          # 辅助维度（降级）
     + 数字标点权重(2.0)      # 事实密度
     + 语义关键词权重(1.5)    # 语义信号
     + 逻辑连接词权重(0.5)    # 逻辑性
     + 列表符号权重(0.8)      # 结构化
     + 长度正则化(1.0)        # 过长过短降权
```

**权重设计原则**：
- ✅ **信息熵+困惑度**占主导（5.0分）
- ✅ **TF-IDF关键词**次重要（2.5分）
- ⚠️ **Lead权重**降级为辅助（0.2分）→ 摆脱"第一句最重要"的偏见

---

### 5. 自适应压缩策略

**函数**：`summarize_topic(text, max_sentences, max_chars) -> Dict`

**分层处理策略**：

| 文本长度 | 处理策略 | 压缩率 | max_sentences |
|---------|---------|--------|---------------|
| < 200字符 | 短文本快速摘要 | 40% | 1句 |
| 200-500 | 中等文本 | 35% | 2句 |
| 500-2000 | 长文本 | 30% | 3句（默认） |
| 2000-3000 | 超长文本 | 40% | 5句（放宽） |
| > 3000 | 调用逻辑链分片器 | 40% | 分片后归纳 |

**自适应参数调整**：
```python
# 根据原文长度动态调整
if text_length < 200:
    adaptive_max_sentences = 1
    target_compression_ratio = 0.4
elif text_length < 500:
    adaptive_max_sentences = 2
    target_compression_ratio = 0.35
elif text_length < 2000:
    adaptive_max_sentences = max_sentences
    target_compression_ratio = 0.3
else:
    # 超长文本放宽压缩率，防止信息丢失
    adaptive_max_sentences = min(max_sentences + 2, 5)
    target_compression_ratio = 0.4

# 动态调整max_chars
adaptive_max_chars = max(int(text_length * target_compression_ratio), 100)
adaptive_max_chars = min(adaptive_max_chars, max_chars * 2)
```

**超长文本处理**（>3000字符）：
```python
def _summarize_with_slicer(text, tfidf_keywords, max_chars):
    # 1. 调用逻辑链分片器
    from tools.memory_slicer_tool import MemorySlicerTool
    slicer = MemorySlicerTool()
    
    # 2. 分片配置（纯算法，无LLM调用）
    slices = slicer.slice_text(text, config={
        'size_thresholds': [2000, 1500, 1000],
        'enable_entropy_analysis': True,    # 信息熵分片
        'enable_perplexity_analysis': True, # 困惑度分片
        'enable_llm_refinement': False      # 关闭LLM
    })
    
    # 3. 对每个分片递归调用归纳引擎
    slice_summaries = []
    for slice_data in slices[:5]:
        slice_summary = summarize_topic(slice_data['content'], max_sentences=1, max_chars=200)
        slice_summaries.append(slice_summary['topic_summary'])
    
    # 4. 合并分片摘要
    summary = ' '.join(slice_summaries)
    
    return {
        'topic_summary': summary,
        'stats': {'used_slicer': True, 'slices_count': len(slices)}
    }
```

**架构优势**：
- ✅ **复用胜于重写**：调用成熟工具而非重复实现
- ✅ **算法统一**：信息熵+困惑度在分片和归纳中共享
- ✅ **分层处理**：长文本先分片再归纳

---

## 📊 性能数据（已验证）

### 压缩效果（2024-12-08测试）

**逻辑链提取**：
- 压缩率：**92%**（100条记忆→8条逻辑链）
- 适用场景：多条记忆的关联压缩

**归纳引擎摘要**：
- 压缩率：**93%**（4000字符→280字符）
- 质量优秀率：**80%**（v2.0版本）
- 最新记忆测试：**100%优秀率**（30条样本）

**关键指标**：
```python
{
    'original_length': 4000,
    'summary_length': 280,
    'compression_ratio': 0.07,  # 93%压缩
    'keyword_coverage': 0.82    # 82%关键词覆盖
}
```

---

## 🔗 与其他模块协同

### 1. 逻辑链分片器（Memory Slicer Tool）

**调用场景**：超长文本（>3000字符）  
**调用方式**：`_summarize_with_slicer(text, tfidf_keywords, max_chars)`

**协同流程**：
```
超长文本 → 分片器分片 → 归纳引擎逐片摘要 → 合并摘要
```

**算法统一性**：
- ✅ 分片器使用信息熵+困惑度分片
- ✅ 归纳引擎使用信息熵+困惑度评分
- ✅ 两者共享同一套科学算法体系

---

### 2. 网状数据库（Mesh Database Interface）

**调用场景**：逻辑链提取  
**调用方式**：`extract_logic_chain(memories) -> List[Dict]`

**协同流程**：
```
多条记忆 → 逻辑链提取 → 归纳引擎生成摘要 → 泡泡存储
```

**数据流**：
```python
# 逻辑链提取
logic_chain = {
    'chain_id': 'logic_chain_0',
    'memories': ['mem_1', 'mem_2', 'mem_3'],
    'key_nodes': [...],
    'compressed_summary': '...'
}

# 调用归纳引擎生成摘要
summary = summarize_topic(
    text=' '.join([mem['content'] for mem in memories]),
    max_sentences=2,
    max_chars=200
)

# 存入泡泡
bubble = compress_to_bubble(logic_chain, summary)
```

---

### 3. 对话窗口（Agent Conversation Window）

**调用场景**：对话历史压缩  
**调用方式**：分层压缩架构中的归纳引擎调用

**协同流程**：
```
对话历史 → 分层评估 → 归纳引擎压缩 → 泡泡沉淀
```

**分层策略**：
- 核心层（100%）：最近5轮对话，不压缩
- 重要层（70-80%）：归纳引擎摘要
- 普通层（40-60%）：归纳引擎摘要
- 历史层（10-30%）：归纳引擎摘要 + 逻辑链提取

---

## 🛠️ 开发指导原则

### 1. 零外部依赖原则

**必须遵守**：
```python
# ✅ 允许：Python标准库
import re
from collections import Counter
import math

# ❌ 禁止：外部NLP库
# import nltk  # ❌
# import spacy  # ❌
# import numpy  # ❌
```

**理由**：
- 归纳引擎必须极度轻量
- 保证跨环境可用性
- 避免依赖冲突

---

### 2. 语言无关原则

**设计要求**：
- 启发式算法对中英文均适用
- 不依赖特定语言的分词工具
- 使用正则表达式处理通用模式

**实现示例**：
```python
# ✅ 中英文通用的句子切分
_SENT_SPLIT = re.compile(r"[。！？!?；;]\s*|\n+")

# ✅ 中英文通用的词汇提取
words = re.findall(r'[\w\u4e00-\u9fff]+', text.lower())

# ✅ 中英文通用的时间指示词
_TIME_HINTS = [
    "年", "月", "日", "今天", "昨天", "明天",
    "today", "yesterday", "tomorrow", "week", "month"
]
```

---

### 3. 科学可解释原则

**设计要求**：
- 算法基于信息论而非经验参数
- 每个权重都有明确物理意义
- 避免"魔法数字"

**示例**：
```python
# ✅ 科学可解释的权重
entropy_weight = entropy * 3.0  # 信息熵：信息密度高的句子
fluency_weight = fluency * 2.0  # 流畅度：表达清晰的句子
keyword_weight = matches * 0.6  # TF-IDF：核心术语密集的句子

# ❌ 避免魔法数字
# score = entropy * 2.37 + fluency * 1.84  # ❌ 为什么是2.37？
```

---

### 4. 认知减负原则

**代码注释规范**：
```python
def _calculate_sentence_entropy(s: str) -> float:
    """【v2.0新增】计算句子的信息熵（香农熵）
    
    原理: H(X) = -∑ p(x) * log₂ p(x)
    高信息熵 = 信息量大 = 可能是核心句子
    """
    # 实现代码...
```

**设计意图标注**：
```python
# 【v2.0核心】信息熵权重（最高3.0分）
entropy_weight = entropy * 3.0

# 【v2.0降级】Lead权重降为辅助（最高0.2分）
lead_weight = 0.2 / (1 + idx * 0.3)
```

---

### 5. 质量验证原则

**必须实现的验证指标**：
```python
return {
    'topic_summary': summary,
    'stats': {
        'compression_ratio': len(summary) / text_length,  # 压缩率
        'keyword_coverage': coverage,                      # 关键词覆盖率
        'original_length': len(text),                      # 原始长度
        'summary_length': len(summary)                     # 摘要长度
    }
}
```

**LLM质量检查**（外部调用）：
```python
# 由主题归纳任务调用
from src.cognitive_engines.thought_engines.mesh_thought_engine import MeshThoughtEngine

engine = MeshThoughtEngine()
quality_report = engine.evaluate_induction_quality(
    original_text=text,
    induction_result=summary
)
```

---

## 🔄 迭代演进记录

### v2.0.0（2024-12-08）

**新增功能**：
1. ✅ 信息熵评分机制（`_calculate_sentence_entropy`）
2. ✅ 困惑度评分机制（`_calculate_sentence_perplexity`）
3. ✅ TF-IDF关键词提取（`_extract_tfidf_keywords`）
4. ✅ 关键词覆盖率验证（`_calculate_keyword_overlap`）
5. ✅ 逻辑链分片器集成（`_summarize_with_slicer`）
6. ✅ 自适应压缩策略（分层处理）

**架构改进**：
1. ✅ Lead权重从主导降级为辅助（3.0分→0.2分）
2. ✅ 信息熵+困惑度成为核心评分（5.0分）
3. ✅ 超长文本调用分片器（>3000字符）

**性能提升**：
1. ✅ 压缩率：未测量 → 93%
2. ✅ 质量优秀率：未验证 → 80%
3. ✅ 关键词覆盖率：未测量 → 82%

---

### v1.5（过渡版本）

**改进**：
1. 移除强制位置多样性
2. 增强关键点提取（`_collect_keypoints`）
3. 支持超长文本分段摘要（降级版）

---

### v1.0（初始版本）

**基础功能**：
1. Lead权重主导的句子评分
2. 启发式关键点提取
3. 事件提取（`extract_events`）
4. 批量处理（`batch_process`）

---

## 📝 使用示例

### 基础调用

```python
from tools.induction_engine import summarize_topic

# 短文本压缩
text = "这是一段需要压缩的文本..." * 50
result = summarize_topic(text, max_sentences=3, max_chars=280)

print(result['topic_summary'])
print(f"压缩率: {result['stats']['compression_ratio']:.2%}")
print(f"关键词覆盖率: {result['stats']['keyword_coverage']:.2%}")
```

### 超长文本处理

```python
# 超长文本自动调用分片器
long_text = "..." * 1000  # >3000字符
result = summarize_topic(long_text)

print(result['stats']['used_slicer'])  # True
print(result['stats']['slices_count'])  # 5
```

### 逻辑链压缩

```python
from src.mesh_database_interface import MeshDatabaseInterface

db = MeshDatabaseInterface()
memories = db.query_memories(filter={'topic': 'AI架构'})

# 提取逻辑链 + 归纳引擎压缩
logic_chains = db.extract_logic_chain(memories)
for chain in logic_chains:
    print(chain['compressed_summary'])  # 已包含归纳引擎摘要
```

---

## 🚀 后续优化方向

### 短期优化（v2.1）

1. **增量压缩**：支持增量文本的增量压缩
2. **多语言优化**：针对中文的分词优化
3. **性能优化**：缓存TF-IDF结果

### 中期优化（v2.5）

1. **语义关联**：引入语义相似度计算
2. **上下文感知**：根据历史压缩结果优化权重
3. **质量反馈**：集成LLM质量评分自动调优

### 长期优化（v3.0）

1. **自学习机制**：根据用户反馈调整权重
2. **领域适配**：支持技术文档、对话、新闻等不同领域
3. **多模态压缩**：支持代码、表格、图表的压缩

---

## 📚 相关文档

**必读文档**：
1. `docs/上下文压缩方案总结.md` - 完整技术方案
2. `docs/架构/上下文管理优化方案.md` - 架构设计文档
3. `docs/架构/上下文管理技术实现汇总.md` - 技术实现细节

**相关工具**：
1. `tools/memory_slicer_tool.py` - 逻辑链分片器
2. `src/mesh_database_interface.py` - 网状数据库（逻辑链提取）
3. `src/agent_conversation_window.py` - 对话窗口（分层压缩）

**测试文件**：
1. `tests/test_induction_engine_v2.py` - 单元测试（若有）
2. `docs/优化记录/归纳引擎v2.0优化记录.md` - 迭代记录（需创建）

---

## ⚠️ 注意事项

### 修改时必须遵守的规则

1. **保持零依赖**：不得引入外部NLP库
2. **保持语言无关**：算法对中英文均适用
3. **保持科学性**：基于信息论而非经验参数
4. **更新版本号**：重大改进必须更新version字段
5. **更新测试**：修改算法必须更新测试用例
6. **更新文档**：修改接口必须更新本文档

### 性能边界

**已知限制**：
1. 超长文本（>10000字符）可能需要较长处理时间
2. 简化版困惑度不如完整NLP模型准确
3. TF-IDF在单文档场景下效果有限

**应对策略**：
1. 超长文本自动调用分片器
2. 困惑度用于相对排序而非绝对评分
3. TF-IDF用于辅助权重而非唯一依据

---

## 🎓 理论基础

### 信息论（Information Theory）

**香农熵**：
```
H(X) = -∑ p(x) * log₂ p(x)
```

**参考资料**：
- Shannon, C. E. (1948). "A Mathematical Theory of Communication"
- 中文：《信息论基础》

### NLP基础

**困惑度（Perplexity）**：
```
PP(W) = P(w₁, w₂, ..., wₙ)^(-1/n)
```

**参考资料**：
- Jurafsky & Martin. "Speech and Language Processing"
- 中文：《自然语言处理综论》

### TF-IDF算法

**公式**：
```
TF-IDF = TF(词频) × IDF(逆文档频率)
```

**参考资料**：
- Salton & McGill (1983). "Introduction to Modern Information Retrieval"

---

## 📞 联系与反馈

**开发者**：RAG系统团队  
**最后更新**：2024-12-08  
**文档版本**：v1.0

**反馈渠道**：
- 代码问题：提交Issue到项目仓库
- 算法优化：在`docs/优化记录/`创建优化记录
- 文档更新：直接修改本文档

---

**记住**：归纳引擎的设计理念是"语义聚焦即压缩"，算法优化必须基于信息论而非经验参数，保持科学可解释性！ 🚀
